# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_metrics.ipynb.

# %% auto 0
__all__ = ['Metrics']

# %% ../nbs/09_metrics.ipynb 2
import numpy as np
import torch
from rouge import Rouge
import evaluate
from fastcore.utils import *

# %% ../nbs/09_metrics.ipynb 4
class Metrics:
    def __init__(self, names: list):
        self.metrics_names = names
        def validate_names(names):
            if len(names) == 0:
                return True
            try:
                evaluate.combine(names)
                return True
            except Exception as e:
                print(e)
                return False
        is_valid = validate_names(self.metrics_names)
        if not is_valid:
            raise ValueError(f"Invalid metric names, the available metrics are {evaluate.list_evaluation_modules('metric')}")

# %% ../nbs/09_metrics.ipynb 5
@patch
def prepare_targets_llm(self: Metrics, y_true, y_pred, tokenizer= None):

    if hasattr(tokenizer, "pad_token_id"):
        padding_mask = (y_true != tokenizer.pad_token_id)  # Shape: (batch_size, seq_length)
        padding_mask_flat = padding_mask.view(-1)  # Flatten the mask
        # Apply the mask
        y_pred = y_pred[padding_mask_flat]
        y_true = y_true[padding_mask_flat]

    y_true = y_true.cpu() if isinstance(y_true, torch.Tensor) else y_true
    y_pred = y_pred.cpu() if isinstance(y_pred, torch.Tensor) else y_pred
    return y_true, y_pred

# %% ../nbs/09_metrics.ipynb 6
@patch
def compute(self: Metrics, y_true, y_pred, tokenizer= None,  **kwargs):
    self.metrics = evaluate.combine(self.metrics_names)
    if tokenizer:
        y_true, y_pred = self.prepare_targets_llm(y_true, y_pred, tokenizer)
    
    return self.metrics.compute(predictions= y_pred, references= y_true, **kwargs)
