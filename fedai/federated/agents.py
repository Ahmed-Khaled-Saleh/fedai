"""The core abstraction for different FL Agents/Clients."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_federated.agents.ipynb.

# %% auto 0
__all__ = ['AgentRole', 'Agent', 'FLAgent', 'Fedu', 'DMTL', 'pFedMe', 'PerAvgAgent', 'PeftAgent', 'FedSophiaAgent', 'AgentMira']

# %% ../../nbs/02_federated.agents.ipynb 3
from fastcore.utils import *
from fastcore.all import *
import os
import networkx as nx
import pickle
import json
from collections import defaultdict,OrderedDict
from copy import deepcopy
import random
from enum import Enum
import torch
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from peft import *
from community import community_louvain
from ..utils import *
from ..client_selector import *
from ..optimizers import *
from ..data.core import LLMDataCollator
from tqdm import tqdm
import numpy as np
import pandas as pd
from loguru import logger
from ..utils import *
from ..metrics import *
from ..losses import *
from transformers import AutoTokenizer
from omegaconf.dictconfig import DictConfig
import numpy as np
import math
np.random.seed(42)
torch.manual_seed(42)

# %% ../../nbs/02_federated.agents.ipynb 4
class AgentRole(Enum):
    SERVER = 1
    CLIENT = 2
    MARL = 3

# %% ../../nbs/02_federated.agents.ipynb 7
class Agent:
    def __init__(self,
                 id,
                 cfg,
                 state= None,
                 role= AgentRole.CLIENT):
        
        self.id = id # each agent has a unique id
        self.cfg = cfg # contains all the configurations needed for the agent/trainer.
        self.state = state # A dictionary containing the state of the agent
        self.role = role # either a client or a server

# %% ../../nbs/02_federated.agents.ipynb 8
@patch
def init_agent(self: Agent):
    # Initialize the state of the agent. In FL Agent, this means making any adjustments to the model/optimizer/state_dict/...etc
    raise NotImplementedError

# %% ../../nbs/02_federated.agents.ipynb 9
@patch
def communicate(self: Agent, msg):
    raise NotImplementedError

# %% ../../nbs/02_federated.agents.ipynb 10
@patch
def update_state(self: Agent):
    raise NotImplementedError

# %% ../../nbs/02_federated.agents.ipynb 11
@patch
def save_state(self: Agent):
    # save the state of the agent to a file on disk (id, model, optimizer, loss_fn).
    raise NotImplementedError

# %% ../../nbs/02_federated.agents.ipynb 12
@patch
def clear_model(self: Agent):
    self.model = None

# %% ../../nbs/02_federated.agents.ipynb 17
class FLAgent(Agent):
    # A Federated Learning Agent implementing `FedAVG`.
    def __init__(self,
                 id, # the id of the agent
                 cfg, # the configuration of the agent.
                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.
                 role= AgentRole.CLIENT, # the role of the agent (client or server)
                 block= None # The data block (local data of the FL Agent).
                 ):  
                 
        super().__init__(id, cfg, state, role)

        if self.role == AgentRole.CLIENT:
            self.train_ds, self.test_ds = block[0], block[1]
            
            for key, value in self.state.items():
                setattr(self, key, value)

            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
            self.train_loader = prepare_dl(self.cfg, self.train_ds)  # noqa: F405
            self.test_loader = prepare_dl(self.cfg, self.test_ds) # noqa: F405

            self.training_metrics = Metrics(list(self.cfg.training_metrics))  # noqa: F405
            self.test_metrics = Metrics(list(self.cfg.test_metrics))  # noqa: F405

            self.data_key, self.label_key = 'x', 'y'

# %% ../../nbs/02_federated.agents.ipynb 18
@patch
def server_init(self: FLAgent, client_fn, client_selector, client_cls, loss_fn, writer):
    self.client_fn = client_fn
    self.client_selector = client_selector
    self.client_cls = client_cls
    self.loss_fn = loss_fn
    self.writer = writer
    self.latest_round = {}
    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

# %% ../../nbs/02_federated.agents.ipynb 21
@patch
def runFL(self: FLAgent):
    res =  []
    all_ids = self.client_selector.select()
    
    for t in range(1, self.cfg.n_rounds + 1):
        lst_active_ids = all_ids[t-1]
        len_clients_ds = {}
        
        for id in lst_active_ids:
            client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)
            len_clients_ds[id] = len(client.train_ds)
            
            self.communicate(client) 
            client.fit()

            client.communicate(self) 
            self.latest_round[id] = t 

        self.aggregate(lst_active_ids, t, len_clients_ds)
        
        train_res, test_res = self.evaluate(t)
        train_df, test_df = self.writer.write(lst_active_ids, train_res, test_res, t) 
        res.append((train_df, test_df))
        
    self.writer.save(res)
    self.writer.finish()

    return res

# %% ../../nbs/02_federated.agents.ipynb 24
@patch
def __str__(self: FLAgent) -> str:
    return f'''{self.__class__.__name__}: {self.__class__.__name__}
    Index : {self.id}
    Model: {self.model.__class__.__name__}
    Criterion: {self.criterion.__class__.__name__}
    Optimizer: {self.optimizer.__class__.__name__}'''


# %% ../../nbs/02_federated.agents.ipynb 25
@patch
def clear_model(self: FLAgent):
    self.model = None if hasattr(self, 'model') else None

# %% ../../nbs/02_federated.agents.ipynb 26
@patch
def update_parameters(self:FLAgent, new_params):
    with torch.no_grad():
        for param , new_param in zip(self.model.parameters(), new_params):
            param.copy_(new_param.data)


# %% ../../nbs/02_federated.agents.ipynb 27
@patch
def get_batch(self: FLAgent, batch):
    return {k: v.to(self.device) for k, v in batch.items()}

# %% ../../nbs/02_federated.agents.ipynb 29
@patch
def _forward(self: FLAgent, batch):
    X, y = batch['x'], batch['y']
    outputs = self.model(X)
    loss = self.criterion(outputs, y)
    return loss, outputs

# %% ../../nbs/02_federated.agents.ipynb 30
@patch
def _closure(self: FLAgent, batch: dict) -> tuple:
    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined

    try:
        loss, logits = self._forward(batch)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        y_pred = probs.argmax(dim=-1)
        y_true = batch[self.label_key]

        if hasattr(self, "training_metrics") and self.cfg.training_metrics:
            if hasattr(self, "tokenizer"):
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)
            else:
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)

    except Exception as e:
        print(f"Error in _closure: {e}")
        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values

    return loss, metrics


# %% ../../nbs/02_federated.agents.ipynb 31
@patch
def _run_batch(self: FLAgent, batch: dict) -> tuple:
    self.optimizer.zero_grad()
    loss, metrics = self._closure(batch)

    if loss.item() == 0.0:
        return loss, metrics
    
    loss.backward()
    
    if self.cfg.model.grad_norm_clip:
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)

    self.optimizer.step()

    return loss, metrics

# %% ../../nbs/02_federated.agents.ipynb 32
@patch
def _run_epoch(self: FLAgent):

    for i, batch in enumerate(self.train_loader):
        batch = self.get_batch(batch)
        self._run_batch(batch)

# %% ../../nbs/02_federated.agents.ipynb 33
@patch
def fit(self: FLAgent) -> dict:
    
    self.model = self.model.to(self.device)
    self.model.train()
    for _ in range(self.cfg.local_epochs):
        self._run_epoch()


# %% ../../nbs/02_federated.agents.ipynb 35
@patch
def train_test_stats(self: FLAgent, batch: dict) -> tuple:
    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined

    try:
        X, y = batch['x'], batch['y']
        logits = self.model(X)
        loss = self.criterion(logits, y)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        y_pred = probs.argmax(dim=-1)
        y_true = batch[self.label_key]

        if hasattr(self, "training_metrics") and self.cfg.training_metrics:
            if hasattr(self, "tokenizer"):
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)
            else:
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)

    except Exception as e:
        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values

    return loss, metrics


# %% ../../nbs/02_federated.agents.ipynb 36
@patch
def evaluate_local(self: FLAgent, loader= 'train') -> dict:
    total_loss = 0
    lst_metrics = []

    self.model = self.model.to(self.device)
    self.model.eval()
    num_eval = 0
    data_loader = self.train_loader if loader == 'train' else self.test_loader
    
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            batch = self.get_batch(batch)
            loss, metrics = self.train_test_stats(batch)                 
            if not math.isnan(loss.item()):
                total_loss += loss.item()  
                num_eval += len(batch[self.data_key])  # Ensure num_eval is updated
                lst_metrics.append(metrics)           
    
    avg_loss = total_loss / num_eval if num_eval > 0 else 0.0
    logger.info(f"Average {loader} Loss is : {avg_loss}")
    
    if lst_metrics:
        total_metrics = {k: sum(m.get(k, 0) for m in lst_metrics) / len(lst_metrics) for k in self.cfg.test_metrics}
    else:
        total_metrics = {k: 0.0 for k in self.cfg.test_metrics}

    return {"loss": avg_loss, "metrics": total_metrics}


# %% ../../nbs/02_federated.agents.ipynb 37
@patch
def evaluate(self: FLAgent, t):
    lst_train_res = []
    lst_test_res = []
    for id in range(self.cfg.num_clients):
        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)
        
        res_train = client.evaluate_local(loader= 'train')
        lst_train_res.append(res_train)

        res_test = client.evaluate_local(loader= 'test')
        lst_test_res.append(res_test)
    return lst_train_res, lst_test_res    


# %% ../../nbs/02_federated.agents.ipynb 40
@patch
def save_state(self: FLAgent, state_dict):  # noqa: F811
    # save the model to self.cfg.save_dir/comm_round/f"local_output_{id}"/state.pth
    
    state_path = os.path.join(self.cfg.save_dir, str(self.t), f"local_output_{self.id}", "state.pth")
    if not os.path.exists(os.path.dirname(state_path)):
        os.makedirs(os.path.dirname(state_path))

    state_dict['model'] = self.model.state_dict()
    state_dict['optimizer'] = self.optimizer.state_dict()

    torch.save(state_dict, state_path)

    if self.role == AgentRole.CLIENT:
        save_space(self)


# %% ../../nbs/02_federated.agents.ipynb 41
@patch
def communicate(self: Agent, another_agent: Agent):  # noqa: F811
    if self.role == AgentRole.CLIENT:
        self.save_state(self.state)

# %% ../../nbs/02_federated.agents.ipynb 44
@patch
def aggregate(self: FLAgent, lst_active_ids, comm_round, len_clients_ds):
        
    m_t = sum(len_clients_ds.values())
    with torch.no_grad():
        for i, id in enumerate(lst_active_ids):
            state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
            
            state = torch.load(state_path, weights_only=False)
            client_state_dict = state['model']

            if i == 0:
                global_model = {
                    key: torch.zeros_like(value) 
                    for key, value in client_state_dict.items()
                }

            n_k = len_clients_ds[id]
            weight =  n_k / m_t 

        
            for key in client_state_dict.keys():
                global_model[key].add_(weight * client_state_dict[key])


        server_state = {
            'model': global_model,
        }

        server_state_path = os.path.join(self.cfg.save_dir, str(comm_round), "global_model", "state.pth")
        
        if not os.path.exists(os.path.dirname(server_state_path)):
            os.makedirs(os.path.dirname(server_state_path), exist_ok=True)
            
        torch.save(server_state, server_state_path)

    

# %% ../../nbs/02_federated.agents.ipynb 47
class Fedu(FLAgent):
    def __init__(self, 
                 id,
                 cfg,
                 state= None,
                 role= AgentRole.CLIENT,
                 block= None):
        
        super().__init__(id, cfg, state, role, block)

        b = np.random.uniform(0,1,size=(self.cfg.num_clients, self.cfg.num_clients))
        b_symm = (b + b.T)/2
        b_symm[b_symm < 0.25] = 0
        self.alk_connection = b_symm

# %% ../../nbs/02_federated.agents.ipynb 49
@patch
def aggregate(self: Fedu, lst_active_ids, comm_round, len_clients_ds):

    global_lr = float(self.cfg.optimizer.lr) * float(self.cfg.local_epochs)
    reg_param = self.cfg.lambda_
    
    with torch.no_grad():
        for i, id in enumerate(lst_active_ids):
            state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
            
            state = torch.load(state_path, weights_only= False)
            client_state_dict = state['model']

            client_diff = {
                key: torch.zeros_like(value) 
                for key, value in client_state_dict.items()
            }
            
            for j, other_id in enumerate(lst_active_ids):
                if i == j:
                    continue
                other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{other_id}", "state.pth")
                
                other_state = torch.load(other_state_path, weights_only= False)
                other_state_dict = other_state['model']

                weight = self.alk_connection[int(id)][int(other_id)]
                for key in client_state_dict.keys():
                    client_diff[key].add_(weight * (client_state_dict[key] - other_state_dict[key]))

            for key in client_state_dict:
                client_state_dict[key].sub_(global_lr * reg_param * client_diff[key])

            clinet_state = {
                'model': client_state_dict,
            }

            agg_client_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"aggregated_model_{id}", "state.pth")
            
            if not os.path.exists(os.path.dirname(agg_client_state_path)):
                os.makedirs(os.path.dirname(agg_client_state_path))

            torch.save(clinet_state, agg_client_state_path)

# %% ../../nbs/02_federated.agents.ipynb 51
class DMTL(FLAgent):
    def __init__(self, 
                 id,
                 cfg,
                 state= None,
                 role= AgentRole.CLIENT,
                 block= None):
        
        super().__init__(id, cfg, state, role, block)
        
        if self.role == AgentRole.CLIENT:
            self.anchorloss = AnchorLoss(self.cfg.random_seed, self.cfg.data.num_classes, self.cfg.model.hidden_dim, self.t, self.h_c).to(self.device)
            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))

# %% ../../nbs/02_federated.agents.ipynb 52
@patch
def server_init(self: DMTL, client_fn, client_selector, client_cls, loss_fn, writer):
    FLAgent.server_init(self, client_fn, client_selector, client_cls, loss_fn, writer)
    self.classes = self.cfg.data.classes
    self.idx_to_cls = {i: self.classes[i] for i in range(len(self.classes))}

# %% ../../nbs/02_federated.agents.ipynb 54
@patch
def _forward(self: DMTL, batch):
    X, y = batch['x'], batch['y']
    y_copied = deepcopy(y)
    labels = y.type(torch.LongTensor).to(self.device)
    ys = labels.float()
    
    h = self.model.encoder(X)
    outputs = self.model.classifier(h)    
    
    loss_anchor = self.anchorloss(h, ys, Lambda = self.cfg.lambda_anchor)
    loss = self.criterion(outputs, y_copied)
    
    return loss + loss_anchor, h, outputs, labels

# %% ../../nbs/02_federated.agents.ipynb 55
@patch
def _closure(self: DMTL, batch: dict) -> tuple:
    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined
    h, labels = None, None
    try:
        loss, h, logits, labels = self._forward(batch)

        probs = torch.nn.functional.softmax(logits, dim=-1)
        y_pred = probs.argmax(dim=-1)
        y_true = batch[self.label_key]

        if hasattr(self, "training_metrics") and self.cfg.training_metrics:
            if hasattr(self, "tokenizer"):
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)
            else:
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)

    except Exception as e:
        print(e)
        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics, h, labels  # Return safe values

    return loss, metrics, h, labels


# %% ../../nbs/02_federated.agents.ipynb 56
@patch
def _run_batch(self: DMTL, batch: dict) -> tuple:
    batch_mean_anchor = torch.zeros(self.cfg.data.num_classes, self.cfg.model.hidden_dim).to(self.device)
    self.optimizer.zero_grad()

    loss, metrics, h, labels = self._closure(batch)
    if loss.item() == 0.0 or h is None:
        return loss, metrics, batch_mean_anchor
    
    for i in set(labels.tolist()):
        batch_mean_anchor[i] += torch.mean(h[labels==i],dim=0)
   
    loss.backward()
    
    if self.cfg.model.grad_norm_clip:
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)

    self.optimizer.step()

    return loss, metrics, batch_mean_anchor

# %% ../../nbs/02_federated.agents.ipynb 57
@patch
def _run_epoch(self: DMTL):

    epoch_mean_anchor = torch.zeros(self.cfg.data.num_classes, self.cfg.model.hidden_dim).to(self.device)
    with torch.no_grad():
        epoch_mean_anchor.copy_(self.anchorloss.anchor)

    for batch_idx, batch in enumerate(self.train_loader):
        batch = self.get_batch(batch)
        _, _, batch_mean_anchor = self._run_batch(batch)

        for i in self.label_set:
            #compute batch mean anchor according to batch label
            batch_mean_anchor[i] = batch_mean_anchor[i]/(batch_idx+1)

            #compute epoch mean anchor according to batch mean anchor
            lambda_momentum = self.cfg.momentum_anchor #pow(2, -(epoch+1))
            # epoch_mean_anchor[i] = lambda_momentum * epoch_mean_anchor[i] + (1-lambda_momentum)*batch_mean_anchor[i]
            epoch_mean_anchor[i].mul_(lambda_momentum).add_((1 - lambda_momentum) * batch_mean_anchor[i])

        

    # self.anchorloss.anchor =  torch.nn.Parameter(epoch_mean_anchor, requires_grad=False)
    with torch.no_grad():
        self.anchorloss.anchor.copy_(epoch_mean_anchor)


# %% ../../nbs/02_federated.agents.ipynb 60
@patch
def save_state(self: DMTL, state_dict):  # noqa: F811
    # save the model to self.cfg.save_dir/comm_round/f"local_output_{id}"/state.pth
    
    
    state_dict['model'] = self.model.state_dict()
    state_dict['optimizer'] = self.optimizer.state_dict()
    state_dict['h'] = self.anchorloss.anchor.detach().clone() # (num_classes, hidden_size)
    state_dict['label_set'] = self.label_set
    
    # pick a random data point from the train_ds and save it to the state_dict
    # data_point = self.train_ds[np.random.randint(0, len(self.train_ds))]
    # data_point = self.get_batch(data_point)
    # data = data_point['x']
    # batched_data_point = data.view(1, 3, 32, 32) # (B, C, H, W)
    # state_dict['h'] = self.model.encoder(batched_data_point)

    state_path = os.path.join(self.cfg.save_dir, str(self.t), f"local_output_{self.id}", "state.pth")
    if not os.path.exists(os.path.dirname(state_path)):
        os.makedirs(os.path.dirname(state_path))

    torch.save(state_dict, state_path)

    if self.role == AgentRole.CLIENT:
        save_space(self)


# %% ../../nbs/02_federated.agents.ipynb 64
@patch
def model_similarity(self: DMTL, model1, model2):
    total_l1_norm = 0.0
    
    for key in model1.keys():
        param1 = model1[key]
        param2 = model2[key]
        
        total_l1_norm += torch.norm(param1 - param2, p=1).item()
    
    return total_l1_norm

# %% ../../nbs/02_federated.agents.ipynb 66
@patch
def h_similarity(self: DMTL, h1, h2, label_set, label_set2):
    h1 = h1.reshape(self.cfg.data.num_classes, self.cfg.model.hidden_dim)
    h2 = h2.reshape(self.cfg.data.num_classes, self.cfg.model.hidden_dim)
    
    h1_norm = F.normalize(h1, p=2, dim=1)  # (3, 512)
    h2_norm = F.normalize(h2, p=2, dim=1)  # (3, 512)
    
    cos_sim_matrix = torch.mm(h1_norm, h2_norm.T)  # (10, 10)
    max_similarity = cos_sim_matrix.max(dim=1).values.mean()# This gives a higher similarity score if each class in h1 has at least one good match in h2.
    data = cos_sim_matrix.cpu().numpy()

    # index only data for the label_set
    data = data[label_set][:, label_set2]

    cols1 = [self.idx_to_cls[i] for i in label_set]
    cols2 = [self.idx_to_cls[i] for i in label_set2]

    df = pd.DataFrame(data, columns= cols1, index= cols2)
    return df, max_similarity.item()

# %% ../../nbs/02_federated.agents.ipynb 68
@patch
def sym_nromalization(self: DMTL, A):
    "normalize the adjacency matrix while ensuring symmetry"

    np.fill_diagonal(A, 0)
    A = (A + A.T) / 2  # Ensure symmetry
    # Compute the degree matrix (row sums)
    row_sums = A.sum(axis=1)
    # Avoid division by zero
    row_sums[row_sums == 0] = 1
    # Compute D^(-1/2)
    D_inv_sqrt = np.diag(1.0 / np.sqrt(row_sums))
    # Symmetric normalization
    A_normalized = D_inv_sqrt @ A @ D_inv_sqrt

    return A_normalized

# %% ../../nbs/02_federated.agents.ipynb 70
@patch
def build_graph(self: DMTL, lst_active_ids, comm_round):

    num_active = len(lst_active_ids)
    graph = np.random.rand(num_active, num_active)
    graph = graph / graph.sum(axis=1)[:, None]

    clients_sim_dict = {}
    visited = {}
    for i, id in enumerate(lst_active_ids):
        state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
        state = torch.load(state_path, weights_only= False)
        model1 = state['model']
        h1 = state['h']
        label_set = state['label_set']

        for j, other_id in enumerate(lst_active_ids):
            if i == j or (id, other_id) in visited:
                continue
            other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{other_id}", "state.pth")
            other_state = torch.load(other_state_path, weights_only= False)
            model2 = other_state['model']
            h2 = other_state['h']
            label_set2 = other_state['label_set']

            w_sim = self.model_similarity(model1, model2)
            h_sim_df, h_sim = self.h_similarity(h1, h2, label_set, label_set2)
            clients_sim_dict[(id, other_id)] = h_sim_df

            graph[i][j] = - (self.cfg.alpha) * w_sim + (1-self.cfg.alpha) * h_sim
            graph[j][i] = round(graph[i][j], ndigits=3)
            graph[i][j] = graph[j][i]

            visited[(id, other_id)] = True
            visited[(other_id, id)] = True

    graph = self.sym_nromalization(graph)

    edges = []
    for i in range(num_active):
        for j in range(num_active):
            if i != j:
                edges.append((i, j, graph[i][j]))
                
    G = nx.Graph()
    G.add_weighted_edges_from(edges)

    for node, label in zip(list(range(num_active)), lst_active_ids):
        G.nodes[node]['label'] = label
    
    df_path = os.path.join(self.cfg.save_dir, str(comm_round), f"h_sim_df_{str(comm_round)}.pth")
    if not os.path.exists(os.path.dirname(df_path)):
        os.makedirs(os.path.dirname(df_path))
    torch.save(clients_sim_dict, df_path)

    return G, graph

# %% ../../nbs/02_federated.agents.ipynb 72
@patch
def get_coalitions(self: DMTL, G):
    correct_clients_indices = nx.get_node_attributes(G, 'label')
    partitions = community_louvain.best_partition(G)
    communities = defaultdict(list)
    for client, community in partitions.items():
        communities[community].append(client)
    communities = dict(communities)

    for community, clients in communities.items():
        communities[community] = [correct_clients_indices[client] for client in clients]

    return communities


# %% ../../nbs/02_federated.agents.ipynb 81
@patch
def compute_weighted_akl(self: DMTL, graph):
    pass

# %% ../../nbs/02_federated.agents.ipynb 84
@patch
def aggregate(self: DMTL, lst_active_ids, comm_round, len_clients_ds):

    self.graph, self.akl_connection = self.build_graph(lst_active_ids, comm_round)
    graph_path = os.path.join(self.cfg.save_dir, str(comm_round), f"graph_{str(comm_round)}.gpickle")
    with open(graph_path, "wb") as f:
        pickle.dump(self.graph, f, pickle.HIGHEST_PROTOCOL)

    self.coalitions = self.get_coalitions(self.graph)
    coalitions_path = os.path.join(self.cfg.save_dir, str(comm_round), "coalitions.pth")
    torch.save(self.coalitions, coalitions_path)

    global_lr = float(self.cfg.optimizer.lr) * float(self.cfg.local_epochs)
    reg_param = self.cfg.lambda_
    
    with torch.no_grad():
        coalitions_reprs = {}
        for col_ind, lst_clients in self.coalitions.items():

            m_t = sum(len_clients_ds[id] for id in lst_clients)
            for i, id in enumerate(lst_clients):
                if not id in lst_active_ids:
                    continue
                state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
                state = torch.load(state_path, weights_only= False)
                client_h = state['h']

                if i == 0:
                    col_repr = torch.zeros_like(client_h)

                n_k = len_clients_ds[id]
                weight =  n_k / m_t 

                col_repr.add_(weight * client_h)
            coalitions_reprs[col_ind] = col_repr
            

        for col_ind, lst_clients in self.coalitions.items():
            for i, id in enumerate(lst_clients):
                if not id in lst_active_ids:
                    continue
                state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
                
                state = torch.load(state_path, weights_only= False)
                client_model = state['model']

                client_diff = {
                    key: torch.zeros_like(value) 
                    for key, value in client_model.items() if key.startswith("fc2") or key.startswith("dropout")
                }

                for j, other_id in enumerate(lst_clients):
                    if i == j:
                        continue
                    other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{other_id}", "state.pth")
                    
                    other_state = torch.load(other_state_path, weights_only= False)
                    other_client_model = other_state['model']

                    a_kl = self.akl_connection[i, j]
                    for key in client_model.keys():
                        if key.startswith("fc2") or key.startswith("dropout"):
                            client_diff[key].add_(a_kl * (client_model[key] - other_client_model[key]))

                for key in client_model.keys():
                    if key.startswith("fc2") or key.startswith("dropout"):
                        client_model[key].sub_(global_lr * reg_param * client_diff[key])

                clinet_state = {
                    'model': client_model,
                    'h': state['h'],
                    'h_c': coalitions_reprs[col_ind],
                }

                agg_client_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"aggregated_model_{id}", "state.pth")
                
                if not os.path.exists(os.path.dirname(agg_client_state_path)):
                    os.makedirs(os.path.dirname(agg_client_state_path))

                torch.save(clinet_state, agg_client_state_path)

# %% ../../nbs/02_federated.agents.ipynb 87
@patch
def extra_computation(self: DMTL, lst_active_ids, comm_round):
    
    for id in lst_active_ids:
        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, comm_round, self.loss_fn, to_read_from= 'aggregated_model_')
        
        client.model.train()
        for param in client.model.classifier.parameters():
            param.requires_grad = False

        client.model = client.model.to(client.device)
        client.h_c = client.h_c.to(client.device)

        optimizer = get_cls("torch.nn", self.cfg.optimizer2)(client.model.encoder.parameters(), lr=self.cfg.lr2)
        
        client.train_loader = torch.utils.data.DataLoader(client.train_ds, batch_size=1, shuffle=True)
        for i, batch in enumerate(client.train_loader):
            batch = client.get_batch(batch)
            X = batch['x']
            optimizer.zero_grad()
            h_prime = client.model.encoder(X)
            loss = client.alignment_criterion()(h_prime, client.h_c)
            loss.backward()
            optimizer.step()
            with torch.no_grad():
                client.h_c.mul_(self.cfg.beta1).add_(h_prime, alpha=1 - self.cfg.beta1)

        
        state = {
            'model': client.model.state_dict(),
            'h_c': client.h_c,
            'h': client.h
        }

        state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_aligned_{id}", "state.pth")
        if not os.path.exists(os.path.dirname(state_path)):
            os.makedirs(os.path.dirname(state_path))

        torch.save(state, state_path)

        for param in client.model.classifier.parameters():
            param.requires_grad = True

# %% ../../nbs/02_federated.agents.ipynb 89
class pFedMe(FLAgent):
    def __init__(self, 
                 id,
                 cfg,
                 state= None,
                 role= AgentRole.CLIENT,
                 block= None):
        
        super().__init__(id, cfg, state, role, block)
        
        if self.role == AgentRole.CLIENT:
            self.local_model = deepcopy(self.model)
            self.optimizer = pFedMeOptimizer(self.model.to(self.device).parameters(), lr=self.cfg.personal_lr, lambda_=self.cfg.lambda_)
            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))
        

# %% ../../nbs/02_federated.agents.ipynb 91
@patch
def save_state(self: pFedMe, state_dict):  # noqa: F811    
    state_path = os.path.join(self.cfg.save_dir, str(self.t), f"local_output_{self.id}", "state.pth")
    
    if not os.path.exists(os.path.dirname(state_path)):
        os.makedirs(os.path.dirname(state_path))

    self.pers_model = deepcopy(self.model).to(self.device)
    with torch.no_grad():
        for p_model, p_updated in zip(self.pers_model.parameters(), self.persionalized_model_bar):
            p_model.copy_(p_updated)

    client_state = {
        **state_dict,
        "model": self.model.state_dict(),
        'optimizer': self.optimizer.state_dict(),
        'pers_model': self.pers_model.state_dict(),
    }

    torch.save(client_state, state_path)

    if self.role == AgentRole.CLIENT:
        save_space(self)


# %% ../../nbs/02_federated.agents.ipynb 92
@patch
def communicate(self: pFedMe, another_agent: Agent):  # noqa: F811
    if self.role == AgentRole.CLIENT:
        self.save_state(self.state)

# %% ../../nbs/02_federated.agents.ipynb 94
@patch
def _run_batch(self: pFedMe, batch: dict) -> tuple:
    
    # find an approximate theta
    for j in range(self.cfg.K):
        self.optimizer.zero_grad()
        loss, metrics = self._closure(batch)

        if loss.item() == 0.0:
            return loss, metrics
        
        loss.backward()
        self.persionalized_model_bar, _ = self.optimizer.step(self.local_model)

    # update local weight after finding aproximate theta
    with torch.no_grad():
        for localweight, per_param in zip(self.local_model.parameters(), self.persionalized_model_bar):
            localweight.sub_(self.cfg.lambda_* self.cfg.optimizer.lr * (localweight - per_param))


    if self.cfg.model.grad_norm_clip:
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)
    

    return loss, metrics

# %% ../../nbs/02_federated.agents.ipynb 95
@patch
def _run_epoch(self: pFedMe):

    for i, batch in enumerate(self.train_loader):
        batch = self.get_batch(batch)
        self._run_batch(batch)

# %% ../../nbs/02_federated.agents.ipynb 96
@patch
def fit(self: pFedMe) -> dict:
    
    self.model = self.model.to(self.device)
    self.local_model = self.local_model.to(self.device)
    self.model.train()
    self.local_model.train()
    for _ in range(self.cfg.local_epochs):
        self._run_epoch()

    with torch.no_grad():
        for model_param, local_param in zip(self.model.parameters(), self.local_model.parameters()):
            model_param.copy_(local_param)

# %% ../../nbs/02_federated.agents.ipynb 98
@patch
def train_test_stats(self: pFedMe, batch: dict) -> tuple:
    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined

    try:
        X, y = batch['x'], batch['y']
        logits = self.pers_model(X)
        loss = self.criterion(logits, y)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        y_pred = probs.argmax(dim=-1)
        y_true = batch[self.label_key]

        if hasattr(self, "training_metrics") and self.cfg.training_metrics:
            if hasattr(self, "tokenizer"):
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)
            else:
                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)

    except Exception as e:
        print(f"Error in _closure Eval_test_stats: {e}")
        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values

    return loss, metrics


# %% ../../nbs/02_federated.agents.ipynb 99
@patch
def evaluate_local(self: pFedMe, loader= 'train') -> dict:
    total_loss = 0
    lst_metrics = []

    # self.per_model = deepcopy(self.model)
    # self.per_model = self.per_model.to(self.device)

    # with torch.no_grad():
    #     for param, per_param in zip(self.per_model.parameters(), self.persionalized_model_bar.to(self.device).parameters()):
    #         param.copy_(per_param)
    
    self.pers_model.eval()
    self.pers_model = self.pers_model.to(self.device)

    num_eval = 0
    data_loader = self.train_loader if loader == 'train' else self.test_loader
    
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            batch = self.get_batch(batch)
            loss, metrics = self.train_test_stats(batch)                 
            if not math.isnan(loss.item()):
                total_loss += loss.item()  
                num_eval += len(batch[self.data_key])  # Ensure num_eval is updated
                lst_metrics.append(metrics)           
    
    avg_loss = total_loss / num_eval if num_eval > 0 else 0.0
    logger.info(f"Average {loader} Loss is : {avg_loss}")
    
    if lst_metrics:
        total_metrics = {k: sum(m.get(k, 0) for m in lst_metrics) / len(lst_metrics) for k in self.cfg.test_metrics}
    else:
        total_metrics = {k: 0.0 for k in self.cfg.test_metrics}

    return {"loss": avg_loss, "metrics": total_metrics}


# %% ../../nbs/02_federated.agents.ipynb 101
@patch
def evaluate(self: pFedMe, t):
    self.cfg.agg ="mtl"
    lst_train_res = []
    lst_test_res = []
    for id in range(self.cfg.num_clients):

        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)
        
        res_train = client.evaluate_local(loader= 'train')
        lst_train_res.append(res_train)

        res_test = client.evaluate_local(loader= 'test')
        lst_test_res.append(res_test)
        
    self.cfg.agg = "one_model"
    return lst_train_res, lst_test_res    


# %% ../../nbs/02_federated.agents.ipynb 103
@patch
def aggregate(self: pFedMe, lst_active_ids, comm_round, len_clients_ds):

    m_t = sum(len_clients_ds.values())
    with torch.no_grad():

        if comm_round > 1:
            prev_server_state_path = os.path.join(self.cfg.save_dir, str(comm_round - 1), "global_model", "state.pth")
            prev_server_state = torch.load(prev_server_state_path, weights_only=False)
            prev_global_model = prev_server_state['model']

        else:
            id = lst_active_ids[0]
            prev_server_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
            prev_server_state = torch.load(prev_server_state_path, weights_only=False)
            prev_global_model = prev_server_state['w0'].to(self.device).state_dict()


        for i, id in enumerate(lst_active_ids):
            state_path = os.path.join(self.cfg.save_dir, str(comm_round), f"local_output_{id}", "state.pth")
            state = torch.load(state_path, weights_only=False)
            client_state_dict = state['model']

            if i == 0:
                global_model = {
                    key: torch.zeros_like(value) 
                    for key, value in client_state_dict.items()
                }

            n_k = len_clients_ds[id]
            weight =  n_k / m_t 

            for key in client_state_dict.keys():
                global_model[key].add_(weight * client_state_dict[key])

        for key in global_model.keys():
            global_model[key].copy_((1-self.cfg.beta)*global_model[key] + self.cfg.beta*prev_global_model[key])

        server_state = {
            'model': global_model,
        }

        server_state_path = os.path.join(self.cfg.save_dir, str(comm_round), "global_model", "state.pth")
        
        if not os.path.exists(os.path.dirname(server_state_path)):
            os.makedirs(os.path.dirname(server_state_path), exist_ok=True)
            
        torch.save(server_state, server_state_path)

    

# %% ../../nbs/02_federated.agents.ipynb 106
class PerAvgAgent(FLAgent):
    def __init__(self, 
                 id,
                 cfg,
                 state= None,
                 role= AgentRole.CLIENT,
                 block= None):
        
        super().__init__(id, cfg, state, role, block)
        
        if self.role == AgentRole.CLIENT:
            self.local_model = deepcopy(self.model)
            self.optimizer = PerFedAvgOpt(self.model.parameters(), lr=self.learning_rate)
            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))
            self.iter_trainloader = iter(self.train_loader)
            self.iter_testloader = iter(self.test_loader)

# %% ../../nbs/02_federated.agents.ipynb 107
@patch
def get_next_batch(self: PerAvgAgent, train= True) -> dict:
    
    loader_type = self.train_loader if train else self.test_loader
    to_iter = self.iter_trainloader if train else self.iter_testloader

    if(int(self.cfg.data.batch_size) == 0):
        for batch in loader_type:
            X = batch['x']
            y = batch['y']
            return (X.to(self.device), y.to(self.device))
        
    else:
        try:

            batch = next(to_iter)
            X = batch['x']
            y = batch['y']
            
        except StopIteration:

            if train:
                self.iter_trainloader = iter(self.train_loader)
                to_iter = self.iter_trainloader
            else:
                self.iter_testloader = iter(self.test_loader)
                to_iter = self.iter_testloader

            batch = next(to_iter)
            X = batch['x']
            y = batch['y']

            
        return (X.to(self.device), y.to(self.device))


# %% ../../nbs/02_federated.agents.ipynb 109
@patch
def _run_batch(self: PerAvgAgent) -> tuple:
    
    X, y = self.get_next_batch()
    self.optimizer.zero_grad()
    output = self.model(X)
    loss = self.loss(output, y)
    loss.backward()
    self.optimizer.step()

    X, y = self.get_next_batch()
    self.optimizer.zero_grad()
    output = self.model(X)
    loss = self.loss(output, y)
    loss.backward()
    self.optimizer.step(beta= self.cfg.beta)

    with torch.no_grad():
        for localweight, model_param in zip(self.local_model.parameters(), self.model):
            localweight.copy_(model_param)


    if self.cfg.model.grad_norm_clip:
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)
    

    return loss, 

# %% ../../nbs/02_federated.agents.ipynb 110
@patch
def _run_epoch(self: PerAvgAgent):

    for i in range(self.num_minibatch):
        self._run_batch()

# %% ../../nbs/02_federated.agents.ipynb 111
@patch
def fit(self: PerAvgAgent) -> None:
    
    self.num_minibatch = int(len(self.train_ds) / int(self.cfg.data.batch_size))

    self.model = self.model.to(self.device)
    self.local_model = self.local_model.to(self.device)

    self.model.train()
    self.local_model.train()

    for _ in range(self.cfg.local_epochs):
        self._run_epoch()

    

# %% ../../nbs/02_federated.agents.ipynb 112
@patch
def train_one_step(self: PerAvgAgent) -> None:
    self.model.train()
    self.model = self.model.to(self.device)

    #step 1
    X, y = self.get_next_batch(train= False)
    self.optimizer.zero_grad()
    output = self.model(X)
    loss = self.loss(output, y)
    loss.backward()
    self.optimizer.step()
    
    #step 2
    X, y = self.get_next_batch(Train= True)
    self.optimizer.zero_grad()
    output = self.model(X)
    loss = self.loss(output, y)
    loss.backward()
    self.optimizer.step(beta=self.cfg.beta)

# %% ../../nbs/02_federated.agents.ipynb 114
@patch
def evaluate(self: PerAvgAgent, t):
    self.cfg.agg ="mtl"
    lst_train_res = []
    lst_test_res = []
    for id in range(self.cfg.num_clients):
        
        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)
        client.train_one_step()
        
        res_train = client.evaluate_local(loader= 'train')
        lst_train_res.append(res_train)

        res_test = client.evaluate_local(loader= 'test')
        lst_test_res.append(res_test)
        
    self.cfg.agg = "one_model"
    return lst_train_res, lst_test_res    


# %% ../../nbs/02_federated.agents.ipynb 121
class PeftAgent(FLAgent):
    def __init__(self,
                 cfg,
                 block,
                 id,
                 state= None,
                 role= "client",
                 **adapter_settings):
        super().__init__(cfg, block, id, state, role)


# %% ../../nbs/02_federated.agents.ipynb 122
@patch
def peftify(self: PeftAgent):
    # extract only the adapter's parameters from the model and store them in a dictionary
    self.params_dict_old = deepcopy(
        OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if
                    "default" in name))
    
    self.params_dict_new = deepcopy(self.params_dict_old)
    
    self.model.state_dict = (
        lambda instance, *_, **__: get_peft_model_state_dict(  # noqa: F405
            instance, self.params_dict_new, "default"
        )
    ).__get__(self.model, type(self.model))

# %% ../../nbs/02_federated.agents.ipynb 123
@patch 
def init_agent(self: PeftAgent):  # noqa: F811
    self.peftify()


# %% ../../nbs/02_federated.agents.ipynb 124
@patch
def save_state_(self: PeftAgent, epoch, local_dataset_len_dict, previously_selected_clients_set):  # noqa: F811
    # save the new adapter weights to disk
    self.save_state(epoch)

    local_dataset_len_dict[self.id] = len(self.block)
    older_adapter_weight = get_peft_model_state_dict(self.model, self.params_dict_old, "default")  # noqa: F405
    set_peft_model_state_dict(self.model, older_adapter_weight, "default")  # noqa: F405
    previously_selected_clients_set = previously_selected_clients_set | set({self.id})
    last_client_id = self.id

    return self.model, local_dataset_len_dict, previously_selected_clients_set, last_client_id

# %% ../../nbs/02_federated.agents.ipynb 126
class FedSophiaAgent(FLAgent):
    def __init__(self,
                 id, # the id of the agent
                 cfg, # the configuration of the agent.
                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.
                 role= AgentRole.CLIENT, # the role of the agent (client or server)
                 block= None):
        super().__init__(id, cfg, state, role, block)


# %% ../../nbs/02_federated.agents.ipynb 127
@patch
def train(self: FedSophiaAgent):
    trainer = self.trainer(self) 
    client_history = trainer.fit() 
    return client_history

# %% ../../nbs/02_federated.agents.ipynb 138
class AgentMira(FLAgent):
    def __init__(self,
                 data_dict: dict,
                 model: torch.nn.Module,
                 criterion,
                 optimizer: torch.optim.Optimizer,
                 id: int,
                 gen_data_dict: dict,
                 tokenizer: AutoTokenizer,
                 collat_fn: LLMDataCollator,
                 cfg: DictConfig) -> None:
            
        super().__init__(data_dict, model, criterion, optimizer, id)
        
        self.train_ds_genr = gen_data_dict['train']
        self.test_ds_genr = gen_data_dict['test']
        self.tokenizer = tokenizer
        self.collat_fn = collat_fn
        self.cfg = cfg 
