{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_HOME = '~/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "class Endpoints(Enum):\n",
    "    DOLLY15k = \"https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_HOME = os.path.join(os.path.expanduser(\"~\"), \"fedai\", \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint = Endpoints[\"DOLLY15k\"].value\n",
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b4ba51bf054782868d2e5c7876a768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac691c4b07d4fb6b1a7a0c9775275bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27014fe262d439c83c2d53c52bac84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(r, name):\n",
    "    '''saves a jsonl object into a file'''\n",
    "    base_dir = os.path.splitext(os.path.basename(f\"{name}\"))[0]\n",
    "    save_dir = os.path.join(DATASET_HOME, base_dir, f\"{name}.jsonl\")\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    with open(save_dir, \"w\") as f:\n",
    "        for line in r.text.splitlines():\n",
    "            json_obj = json.loads(line)\n",
    "            f.write(json.dumps(json_obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/ahmed/fedai/datasets/DOLLY15k/DOLLY15k.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         save_jsonl(r, name)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDOLLY15k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, please check the name or the locations. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m                    The available datasets are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEndpoints\u001b[38;5;241m.\u001b[39m__members__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43msave_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m, in \u001b[0;36msave_jsonl\u001b[0;34m(r, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_dir):\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[1;32m     10\u001b[0m         json_obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/fedai/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/ahmed/fedai/datasets/DOLLY15k/DOLLY15k.jsonl'"
     ]
    }
   ],
   "source": [
    "def get_dataset(name):\n",
    "    endpoint = Endpoints[name].value\n",
    "    \n",
    "    r = requests.get(endpoint)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        raise Exception(f\"Failed to get dataset {name}, please check the name or the locations. \\\n",
    "                        The available datasets are {Endpoints.__members__}\")\n",
    "    \n",
    "    else:\n",
    "        save_jsonl(r, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ahmed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "import copy\n",
    "\n",
    "from enum import Enum\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_jsonl(file_path,\n",
    "               instruction='instruction',\n",
    "               input='input',\n",
    "               output='output',\n",
    "               category='category'):\n",
    "    # Format of each line:\n",
    "    # {'instruction': ..., 'input': ..., 'output':...}\n",
    "    list_data_dict = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            new_item = dict(\n",
    "                instruction=item[instruction] if instruction in item else None,\n",
    "                input=item[input] if input in item else None,\n",
    "                output=item[output] if output in item else None,\n",
    "                category=item[category] if category in item else None)\n",
    "            item = new_item\n",
    "            list_data_dict.append(item)\n",
    "    return list_data_dict\n",
    "\n",
    "\n",
    "class DefaultToken(Enum):\n",
    "    PAD_TOKEN = \"[PAD]\"\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    BOS_TOKEN = \"<s>\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, \"\n",
    "        \"paired with an input that provides further context. \"\n",
    "        \"Write a response for the task request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\"\n",
    "        \"\\n{input}\\n\\n### Response:\"),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response for the task request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMDataCollator(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels,\n",
    "            batch_first=True,\n",
    "            padding_value=DefaultToken.IGNORE_INDEX.value)\n",
    "        categories = torch.tensor([instance[\"categories\"] for instance in instances])\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            categories=categories,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "class MTLDataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                 list_data_dict,\n",
    "                 tokenizer,\n",
    "                 prompt_input=PROMPT_DICT[\"prompt_input\"],\n",
    "                 prompt_no_input=PROMPT_DICT[\"prompt_no_input\"], \n",
    "                 generation=False):\n",
    "        \"\"\"\n",
    "            list_data_dict: list of dictionaries with keys 'input', 'output', 'category'\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MTLDataSet, self).__init__()\n",
    "            \n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\"\n",
    "            else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [\n",
    "            f\"{example['output']}{tokenizer.eos_token}\"\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "\n",
    "        data_dict = self.preprocess(sources, targets, tokenizer, generation=generation)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "        categories = [\n",
    "            example['category'] if 'category' in example else None\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        self.tasks = categories\n",
    "        df = pd.DataFrame(categories, columns=[\"category\"])\n",
    "        self.categories = list(pd.Categorical(df[\"category\"]).codes)\n",
    "\n",
    "    def _tokenize_fn(self, strings, tokenizer):\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "            ) for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [\n",
    "            tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "        ]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "            for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "    def preprocess(self, sources, targets, tokenizer, generation):\n",
    "        if generation:\n",
    "            sources_tokenized, labels_tokenized = [\n",
    "                self._tokenize_fn(strings, tokenizer)\n",
    "                for strings in (sources, targets)\n",
    "            ]\n",
    "            input_ids = self._tokenize_fn(sources, tokenizer)[\"input_ids\"]\n",
    "            labels = self._tokenize_fn(targets, tokenizer)[\"input_ids\"]\n",
    "        else:\n",
    "            examples = [s + t for s, t in zip(sources, targets)]\n",
    "            examples_tokenized, sources_tokenized = [\n",
    "                self._tokenize_fn(strings, tokenizer)\n",
    "                for strings in (examples, sources)\n",
    "            ]\n",
    "            input_ids = examples_tokenized[\"input_ids\"]\n",
    "            labels = copy.deepcopy(input_ids)\n",
    "            for label, source_len in zip(labels,\n",
    "                                        sources_tokenized[\"input_ids_lens\"]):\n",
    "                label[:source_len] = DefaultToken.IGNORE_INDEX.value\n",
    "        return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return dict(input_ids=self.input_ids[i],\n",
    "                    labels=self.labels[i],\n",
    "                    categories=self.categories[i],\n",
    "                    tasks=self.tasks[i])\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def split_by_category(list_data_dict):\n",
    "        groupdict = dict()\n",
    "        for example in list_data_dict:\n",
    "            if example['category'] not in groupdict:\n",
    "                groupdict[example['category']] = []\n",
    "            groupdict[example['category']].append(example)\n",
    "        return groupdict\n",
    "\n",
    "\n",
    "def train_eval_split(dataset):\n",
    "    \n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.8 * dataset_size)  # 80% for training\n",
    "    val_size = dataset_size - train_size  # Remaining 20% for validation\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "import random\n",
    "\n",
    "def split_tasks_among_clients(tasks_dict, num_clients=10):\n",
    "    result = {task: [[] for _ in range(num_clients)] for task in tasks_dict}\n",
    "    \n",
    "    for task, samples in tasks_dict.items():\n",
    "        # Shuffle the samples to ensure randomness\n",
    "        shuffled_samples = samples.copy()\n",
    "        random.shuffle(shuffled_samples)\n",
    "        \n",
    "        # Distribute samples among clients\n",
    "        for i, sample in enumerate(shuffled_samples):\n",
    "            client_index = random.randint(0, num_clients - 1)\n",
    "            result[task][client_index].append(sample)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_dolly(args, tokenizer):\n",
    "    data_collator = LLMDataCollator(tokenizer=tokenizer)\n",
    "    json_name = 'databricks-dolly-15k.jsonl'\n",
    "    list_data_dict =  load_jsonl(os.path.join('data', json_name), \n",
    "                            instruction='instruction',\n",
    "                            input='context',\n",
    "                            output='response',\n",
    "                            category='category')\n",
    "    \n",
    "    grouped_data = split_by_category(list_data_dict)\n",
    "    result = split_tasks_among_clients(grouped_data)\n",
    "\n",
    "    lst_train_ds = []\n",
    "    lst_eval_set = []\n",
    "    lst_train_ds_genr = []\n",
    "    lst_eval_set_genr = []\n",
    "\n",
    "    for task in result:\n",
    "        for client_data in result[task]:\n",
    "            dataset = MTLDataSet(client_data, tokenizer, generation=False)\n",
    "            train_loader, val_loader = train_eval_split(dataset)\n",
    "            lst_train_ds.append(train_loader)\n",
    "            lst_eval_set.append(val_loader)\n",
    "\n",
    "            dataset_genr = MTLDataSet(client_data, tokenizer, generation=True)\n",
    "            train_ds_genr, val_loader_genr = train_eval_split(dataset_genr)\n",
    "            lst_train_ds_genr.append(train_ds_genr)\n",
    "            lst_eval_set_genr.append(val_loader_genr)\n",
    "\n",
    "    return (lst_train_ds, lst_eval_set, tokenizer, data_collator), (lst_train_ds_genr, lst_eval_set_genr) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
