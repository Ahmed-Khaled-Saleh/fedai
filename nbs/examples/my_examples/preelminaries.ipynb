{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Load CIFAR10 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fedai.vision.VisionBlock import VisionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_cifa_data():\n",
    "#     transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#     trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "#     testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "#     trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset.data),shuffle=False)\n",
    "#     testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset.data),shuffle=False)\n",
    "\n",
    "#     for _, train_data in enumerate(trainloader,0):\n",
    "#         trainset.data, trainset.targets = train_data\n",
    "#     for _, train_data in enumerate(testloader,0):\n",
    "#         testset.data, testset.targets = train_data\n",
    "\n",
    "#     random.seed(1)\n",
    "#     np.random.seed(1)\n",
    "#     NUM_USERS = 20 # should be muitiple of 10\n",
    "#     NUM_LABELS = 3\n",
    "#     # Setup directory for train/test data\n",
    "#     train_path = './data/train/cifa_train_100.json'\n",
    "#     test_path = './data/test/cifa_test_100.json'\n",
    "#     dir_path = os.path.dirname(train_path)\n",
    "#     if not os.path.exists(dir_path):\n",
    "#         os.makedirs(dir_path)\n",
    "#     dir_path = os.path.dirname(test_path)\n",
    "#     if not os.path.exists(dir_path):\n",
    "#         os.makedirs(dir_path)\n",
    "\n",
    "#     cifa_data_image = []\n",
    "#     cifa_data_label = []\n",
    "\n",
    "#     cifa_data_image.extend(trainset.data.cpu().detach().numpy())\n",
    "#     cifa_data_image.extend(testset.data.cpu().detach().numpy())\n",
    "#     cifa_data_label.extend(trainset.targets.cpu().detach().numpy())\n",
    "#     cifa_data_label.extend(testset.targets.cpu().detach().numpy())\n",
    "#     cifa_data_image = np.array(cifa_data_image)\n",
    "#     cifa_data_label = np.array(cifa_data_label)\n",
    "\n",
    "#     cifa_data = []\n",
    "#     for i in trange(10):\n",
    "#         idx = cifa_data_label==i\n",
    "#         cifa_data.append(cifa_data_image[idx])\n",
    "\n",
    "\n",
    "#     print(\"\\nNumb samples of each label:\\n\", [len(v) for v in cifa_data])\n",
    "#     users_lables = []\n",
    "\n",
    "#     ###### CREATE USER DATA SPLIT #######\n",
    "#     # Assign 100 samples to each user\n",
    "#     X = [[] for _ in range(NUM_USERS)]\n",
    "#     y = [[] for _ in range(NUM_USERS)]\n",
    "#     idx = np.zeros(10, dtype=np.int64)\n",
    "#     for user in range(NUM_USERS):\n",
    "#         for j in range(NUM_LABELS):  # 3 labels for each users\n",
    "#             #l = (2*user+j)%10\n",
    "#             l = (user + j) % 10\n",
    "#             print(\"L:\", l)\n",
    "#             X[user] += cifa_data[l][idx[l]:idx[l]+10].tolist()\n",
    "#             y[user] += (l*np.ones(10)).tolist()\n",
    "#             idx[l] += 10\n",
    "\n",
    "#     print(\"IDX1:\", idx)  # counting samples for each labels\n",
    "\n",
    "#     # Assign remaining sample by power law\n",
    "#     user = 0\n",
    "#     props = np.random.lognormal(\n",
    "#         0, 2., (10, NUM_USERS, NUM_LABELS))  # last 5 is 5 labels\n",
    "#     props = np.array([[[len(v)-NUM_USERS]] for v in cifa_data]) * \\\n",
    "#         props/np.sum(props, (1, 2), keepdims=True)\n",
    "#     # print(\"here:\",props/np.sum(props,(1,2), keepdims=True))\n",
    "#     #props = np.array([[[len(v)-100]] for v in mnist_data]) * \\\n",
    "#     #    props/np.sum(props, (1, 2), keepdims=True)\n",
    "#     #idx = 1000*np.ones(10, dtype=np.int64)\n",
    "#     # print(\"here2:\",props)\n",
    "#     for user in trange(NUM_USERS):\n",
    "#         for j in range(NUM_LABELS):  # 4 labels for each users\n",
    "#             # l = (2*user+j)%10\n",
    "#             l = (user + j) % 10\n",
    "#             num_samples = int(props[l, user//int(NUM_USERS/10), j])\n",
    "#             numran1 = random.randint(300, 600)\n",
    "#             num_samples = (num_samples)  + numran1 #+ 200\n",
    "#             if(NUM_USERS <= 20): \n",
    "#                 num_samples = num_samples * 2\n",
    "#             if idx[l] + num_samples < len(cifa_data[l]):\n",
    "#                 X[user] += cifa_data[l][idx[l]:idx[l]+num_samples].tolist()\n",
    "#                 y[user] += (l*np.ones(num_samples)).tolist()\n",
    "#                 idx[l] += num_samples\n",
    "#                 print(\"check len os user:\", user, j,\n",
    "#                     \"len data\", len(X[user]), num_samples)\n",
    "\n",
    "#     print(\"IDX2:\", idx) # counting samples for each labels\n",
    "\n",
    "#     # Create data structure\n",
    "#     train_data = {'users': [], 'user_data':{}, 'num_samples':[]}\n",
    "#     test_data = {'users': [], 'user_data':{}, 'num_samples':[]}\n",
    "\n",
    "#     # Setup 5 users\n",
    "#     # for i in trange(5, ncols=120):\n",
    "#     for i in range(NUM_USERS):\n",
    "#         uname = i\n",
    "#         combined = list(zip(X[i], y[i]))\n",
    "#         random.shuffle(combined)\n",
    "#         X[i][:], y[i][:] = zip(*combined)\n",
    "\n",
    "#         num_samples = len(X[i])\n",
    "#         train_len = int(0.75*num_samples)\n",
    "#         test_len = num_samples - train_len\n",
    "\n",
    "#         #X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], train_size=0.75, stratify=y[i])\\\n",
    "        \n",
    "#         test_data['users'].append(uname)\n",
    "#         test_data[\"user_data\"][uname] =  {'x': X[i][:test_len], 'y': y[i][:test_len]} \n",
    "#         test_data['num_samples'].append(test_len)\n",
    "\n",
    "#         train_data[\"user_data\"][uname] =  {'x': X[i][test_len:], 'y': y[i][test_len:]}\n",
    "#         train_data['users'].append(uname)\n",
    "#         train_data['num_samples'].append(train_len)\n",
    "        \n",
    "\n",
    "#     return train_data['users'], _ , train_data['user_data'], test_data['user_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 29.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numb samples of each label:\n",
      " [6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]\n",
      "L: 0\n",
      "L: 1\n",
      "L: 2\n",
      "L: 1\n",
      "L: 2\n",
      "L: 3\n",
      "L: 2\n",
      "L: 3\n",
      "L: 4\n",
      "L: 3\n",
      "L: 4\n",
      "L: 5\n",
      "L: 4\n",
      "L: 5\n",
      "L: 6\n",
      "L: 5\n",
      "L: 6\n",
      "L: 7\n",
      "L: 6\n",
      "L: 7\n",
      "L: 8\n",
      "L: 7\n",
      "L: 8\n",
      "L: 9\n",
      "L: 8\n",
      "L: 9\n",
      "L: 0\n",
      "L: 9\n",
      "L: 0\n",
      "L: 1\n",
      "L: 0\n",
      "L: 1\n",
      "L: 2\n",
      "L: 1\n",
      "L: 2\n",
      "L: 3\n",
      "L: 2\n",
      "L: 3\n",
      "L: 4\n",
      "L: 3\n",
      "L: 4\n",
      "L: 5\n",
      "L: 4\n",
      "L: 5\n",
      "L: 6\n",
      "L: 5\n",
      "L: 6\n",
      "L: 7\n",
      "L: 6\n",
      "L: 7\n",
      "L: 8\n",
      "L: 7\n",
      "L: 8\n",
      "L: 9\n",
      "L: 8\n",
      "L: 9\n",
      "L: 0\n",
      "L: 9\n",
      "L: 0\n",
      "L: 1\n",
      "IDX1: [60 60 60 60 60 60 60 60 60 60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 0 0 len data 1796 1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:21,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 0 1 len data 3584 1788\n",
      "check len os user: 0 2 len data 4536 952\n",
      "check len os user: 1 0 len data 900 870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:14,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 1 1 len data 1624 724\n",
      "check len os user: 1 2 len data 2794 1170\n",
      "check len os user: 2 0 len data 2246 2216\n",
      "check len os user: 2 1 len data 3346 1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:03<00:17,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 2 2 len data 4494 1148\n",
      "check len os user: 3 0 len data 1162 1132\n",
      "check len os user: 3 1 len data 1864 702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:03<00:12,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 3 2 len data 3418 1554\n",
      "check len os user: 4 0 len data 800 770\n",
      "check len os user: 4 1 len data 1802 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:04<00:12,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 4 2 len data 2936 1134\n",
      "check len os user: 5 0 len data 650 620\n",
      "check len os user: 5 1 len data 1706 1056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:05<00:12,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 5 2 len data 2612 906\n",
      "check len os user: 6 0 len data 918 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:05<00:09,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 6 1 len data 1990 1072\n",
      "check len os user: 6 2 len data 3062 1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:05<00:06,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 7 0 len data 672 642\n",
      "check len os user: 7 1 len data 1328 656\n",
      "check len os user: 7 2 len data 1954 626\n",
      "check len os user: 8 0 len data 1202 1172\n",
      "check len os user: 8 1 len data 1810 608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:07<00:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 8 2 len data 3186 1376\n",
      "check len os user: 9 0 len data 878 848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:07<00:07,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 9 1 len data 1928 1050\n",
      "check len os user: 9 2 len data 2648 720\n",
      "check len os user: 10 0 len data 1174 1144\n",
      "check len os user: 10 1 len data 2026 852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:09<00:08,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 10 2 len data 3076 1050\n",
      "check len os user: 11 0 len data 1136 1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:09<00:05,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 11 2 len data 2028 892\n",
      "check len os user: 12 1 len data 1060 1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:09<00:04,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 12 2 len data 1934 874\n",
      "check len os user: 13 1 len data 938 908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:09<00:02,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check len os user: 13 2 len data 1572 634\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "clients, groups, train_data, test_data = read_cifa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fedai.FLearner import *  # noqa: F403\n",
    "from fedai.learner_utils import *  # noqa: F403\n",
    "from fedai.utils import * # noqa: F403\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def cfg_fn(f):\n",
    "    cfg = load_config(f)  # noqa: F405\n",
    "    cfg = OmegaConf.create(cfg)\n",
    "    return cfg\n",
    "cfg = cfg_fn('../cfg.yaml')\n",
    "cfg.data.name = 'CIFAR10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project_name': 'example_project', 'random_seed': 42, 'num_clients': 20, 'n_rounds': 10, 'm': 0.1, 'local_epochs': 2, 'lr': 0.01, 'agg': 'one_model', 'beta': 0.9, 'lambda_': 0.01, 'server_lr': 0.01, 'save_dir': 'models', 'res_dir': 'results', 'log_dir': 'logs', 'training_metrics': ['accuracy'], 'test_metrics': ['accuracy'], 'data': {'data_dir': 'data', 'modality': ['Vision'], 'batch_size': 10, 'name': 'CIFAR10', 'niid': False, 'balance': False, 'partitioner': 'DirPartitioner', 'alpha': 0.1, 'train_ratio': 0.75, 'num_classes': 2}, 'model': {'name': 'MLP', 'dim_in': 784, 'dim_hidden': 128, 'dim_out': 10, 'vocab_size': 1000, 'embed_size': 128, 'hidden_size': 128, 'num_layers': 1, 'grad_norm_clip': 1.0, 'peft': {'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'target_modules': ['c_attn', 'c_proj']}}, 'optimizer': {'name': 'Adam', 'lr': 0.001, 'weight_decay': 0.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Models -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 205MB/s]\n"
     ]
    }
   ],
   "source": [
    "# deefine resnet without the classifer\n",
    "from torchvision import models\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "model = torch.nn.Sequential(*list(resnet18.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "\n",
    "        # First fully connected block\n",
    "        self.fc1 = nn.Linear(512, 256)  # Input size of 128 (features from CNN output or other input)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Second fully connected block\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc3 = nn.Linear(256, 512)  # Smaller output for feature learning\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImprovedMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedMLP, self).__init__()\n",
    "\n",
    "        # First fully connected block with BatchNorm and Dropout\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization\n",
    "        self.act1 = nn.ReLU()  # Swish activation\n",
    "        self.drop1 = nn.Dropout(0.2)  # Regularization\n",
    "\n",
    "        # Second fully connected block with a residual connection\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # Store original input for residual connection\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = x + residual  # Residual connection\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM_h(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LSTM_h, self).__init__()\n",
    "#         self.lstm = nn.LSTM(512, 512, 1)\n",
    "#         self.fc = nn.Linear(512, 512)\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 512, 1)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = x[-1, :, :]\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observer = ImprovedMLP()\n",
    "# observer = observer.to('cuda')\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # States Dataset -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_index_map = {i: trainset[i][0] for i in range(len(trainset))}  # Maps idx → image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # Define loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(resnet18.parameters(), lr=0.001)\n",
    "\n",
    "# # Directory to store features\n",
    "# os.makedirs(\"features\", exist_ok=True)\n",
    "\n",
    "# # Training loop with feature tracking\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     resnet18.train()\n",
    "    \n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = resnet18(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Extract features at this epoch for each image\n",
    "#     resnet18.eval()\n",
    "#     with torch.no_grad():\n",
    "#         with open(f\"features/epoch_{epoch}.pt\", \"wb\") as f:\n",
    "#             feature_dict = {}  # Store features temporarily\n",
    "#             for img_idx in range(len(trainset)):  # Process each image separately\n",
    "#                 image, _ = trainset[img_idx]  # Load image dynamically\n",
    "#                 image = image.unsqueeze(0).to(\"cuda\")  # Add batch dimension\n",
    "#                 feature = trainset(image).squeeze().cpu()  # Shape: (512,)\n",
    "#                 feature_dict[img_idx] = feature  # Store feature\n",
    "#             torch.save(feature_dict, f)  # Save features for the current epoch\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Features saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a data set of all consecutive hiiden states and make it memry efficient\n",
    "\n",
    "# class FeatureDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, model, data_loader):\n",
    "#         self.model = model\n",
    "#         self.model.eval()\n",
    "#         self.data_loader = data_loader\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data_loader.dataset) - 1\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         with torch.no_grad():\n",
    "#             data, _ = self.data_loader.dataset[idx]\n",
    "#             d2, _ = self.data_loader.dataset[idx+1]\n",
    "#             h1 = self.model(data.to('cuda').unsqueeze(0)).view(-1)\n",
    "#             h2 = self.model(d2.to('cuda').unsqueeze(0)).view(-1)\n",
    "#             return h1, h2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = FeatureDataset(model, trainloader)\n",
    "# dl_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "\n",
    "# ds_test = FeatureDataset(model, testloader)\n",
    "# dl_test = torch.utils.data.DataLoader(ds_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmed-elbakary\u001b[0m (\u001b[33medge-ai-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.login(key=\"0c6ac9fee5239a432529a35c7433d121ec9c9f37\",\n",
    "#            verify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.022\n",
      "Test: [1,   400] loss: 1.029\n",
      "Test: [1,   600] loss: 1.006\n",
      "Test: [1,   800] loss: 1.012\n",
      "Test: [1,  1000] loss: 1.005\n",
      "Test: [1,  1200] loss: 1.018\n",
      "Test: [1,  1400] loss: 1.028\n",
      "Test: [1,  1600] loss: 1.021\n",
      "Test: [1,  1800] loss: 1.026\n",
      "Test: [1,  2000] loss: 1.030\n",
      "Test: [1,  2200] loss: 1.009\n",
      "Test: [1,  2400] loss: 1.030\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.023\n",
      "Test: [1,   400] loss: 1.021\n",
      "Test: [1,   600] loss: 1.011\n",
      "Test: [1,   800] loss: 1.023\n",
      "Test: [1,  1000] loss: 1.021\n",
      "Test: [1,  1200] loss: 1.004\n",
      "Test: [1,  1400] loss: 1.021\n",
      "Test: [1,  1600] loss: 1.024\n",
      "Test: [1,  1800] loss: 1.019\n",
      "Test: [1,  2000] loss: 1.028\n",
      "Test: [1,  2200] loss: 1.019\n",
      "Test: [1,  2400] loss: 1.021\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.023\n",
      "Test: [1,   400] loss: 1.012\n",
      "Test: [1,   600] loss: 1.031\n",
      "Test: [1,   800] loss: 1.012\n",
      "Test: [1,  1000] loss: 1.023\n",
      "Test: [1,  1200] loss: 1.034\n",
      "Test: [1,  1400] loss: 1.005\n",
      "Test: [1,  1600] loss: 1.018\n",
      "Test: [1,  1800] loss: 1.018\n",
      "Test: [1,  2000] loss: 1.038\n",
      "Test: [1,  2200] loss: 1.017\n",
      "Test: [1,  2400] loss: 1.019\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.024\n",
      "Test: [1,   400] loss: 1.012\n",
      "Test: [1,   600] loss: 1.031\n",
      "Test: [1,   800] loss: 1.028\n",
      "Test: [1,  1000] loss: 1.012\n",
      "Test: [1,  1200] loss: 1.033\n",
      "Test: [1,  1400] loss: 1.022\n",
      "Test: [1,  1600] loss: 1.017\n",
      "Test: [1,  1800] loss: 1.014\n",
      "Test: [1,  2000] loss: 1.026\n",
      "Test: [1,  2200] loss: 1.016\n",
      "Test: [1,  2400] loss: 1.028\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.031\n",
      "Test: [1,   400] loss: 1.008\n",
      "Test: [1,   600] loss: 1.028\n",
      "Test: [1,   800] loss: 1.014\n",
      "Test: [1,  1000] loss: 1.013\n",
      "Test: [1,  1200] loss: 1.032\n",
      "Test: [1,  1400] loss: 1.022\n",
      "Test: [1,  1600] loss: 1.021\n",
      "Test: [1,  1800] loss: 1.026\n",
      "Test: [1,  2000] loss: 1.021\n",
      "Test: [1,  2200] loss: 1.034\n",
      "Test: [1,  2400] loss: 1.027\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.034\n",
      "Test: [1,   400] loss: 1.018\n",
      "Test: [1,   600] loss: 1.028\n",
      "Test: [1,   800] loss: 1.021\n",
      "Test: [1,  1000] loss: 1.039\n",
      "Test: [1,  1200] loss: 1.026\n",
      "Test: [1,  1400] loss: 1.014\n",
      "Test: [1,  1600] loss: 1.021\n",
      "Test: [1,  1800] loss: 1.024\n",
      "Test: [1,  2000] loss: 1.005\n",
      "Test: [1,  2200] loss: 1.014\n",
      "Test: [1,  2400] loss: 1.018\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.028\n",
      "Test: [1,   400] loss: 1.024\n",
      "Test: [1,   600] loss: 1.015\n",
      "Test: [1,   800] loss: 1.028\n",
      "Test: [1,  1000] loss: 1.013\n",
      "Test: [1,  1200] loss: 1.025\n",
      "Test: [1,  1400] loss: 1.012\n",
      "Test: [1,  1600] loss: 1.027\n",
      "Test: [1,  1800] loss: 1.024\n",
      "Test: [1,  2000] loss: 1.029\n",
      "Test: [1,  2200] loss: 1.020\n",
      "Test: [1,  2400] loss: 1.019\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.020\n",
      "Test: [1,   400] loss: 1.006\n",
      "Test: [1,   600] loss: 1.015\n",
      "Test: [1,   800] loss: 1.026\n",
      "Test: [1,  1000] loss: 1.005\n",
      "Test: [1,  1200] loss: 1.021\n",
      "Test: [1,  1400] loss: 1.030\n",
      "Test: [1,  1600] loss: 1.031\n",
      "Test: [1,  1800] loss: 1.021\n",
      "Test: [1,  2000] loss: 1.027\n",
      "Test: [1,  2200] loss: 1.028\n",
      "Test: [1,  2400] loss: 1.022\n",
      "here***here***here***here***\n",
      "Test: [1,   200] loss: 1.037\n",
      "Test: [1,   400] loss: 1.019\n",
      "Test: [1,   600] loss: 1.014\n",
      "Test: [1,   800] loss: 1.035\n",
      "Test: [1,  1000] loss: 1.008\n",
      "Test: [1,  1200] loss: 1.022\n",
      "Test: [1,  1400] loss: 1.020\n",
      "Test: [1,  1600] loss: 1.019\n",
      "Test: [1,  1800] loss: 1.012\n",
      "Test: [1,  2000] loss: 1.027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-6e8331a40639>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mrunning_loss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-56a65ee6d610>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # train the model\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(observer.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# n_epochs = 2\n",
    "\n",
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"observer\",\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"architecture\": \"MLP\",\n",
    "#     \"dataset\": \"CIFAR10\", # first 2000 videos of the dataset\n",
    "#     \"epochs\": n_epochs,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(dl_train):\n",
    "        \n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         h, h_next = data\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = observer(h.to('cuda'))\n",
    "#         loss = criterion(outputs, h_next.to('cuda'))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         print(\"here***\"*4)\n",
    "#         if i % 200 == 199:    # print every 2000 mini-batches\n",
    "#             print('Train: [%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 200))\n",
    "#             avg_train_loss = running_loss / 200\n",
    "#             wandb.log({\"Train Loss\": avg_train_loss})\n",
    "#             running_loss = 0.0\n",
    "\n",
    "        \n",
    "#         # compute the loss for the test set\n",
    "#         running_loss_test = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for j, data in enumerate(dl_test):\n",
    "#                 # get the inputs; data is a list of [inputs, labels]\n",
    "#                 h, h_next = data\n",
    "            \n",
    "#                 # forward + backward + optimize\n",
    "#                 outputs = observer(h.to('cuda'))\n",
    "#                 loss = criterion(outputs, h_next.to('cuda'))\n",
    "            \n",
    "#                 # print statistics\n",
    "#                 running_loss_test += loss.item()\n",
    "#                 if j % 200 == 199:    # print every 2000 mini-batches\n",
    "#                     print('Test: [%d, %5d] loss: %.3f' %\n",
    "#                           (epoch + 1, j + 1, running_loss_test / 200))\n",
    "#                     avg_test_loss = running_loss_test / 200\n",
    "#                     wandb.log({\"Test Loss\": avg_test_loss})\n",
    "#                     ## FIXME\n",
    "#                     # https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "#                     running_loss_test = 0.0\n",
    "    \n",
    "\n",
    "# wandb.finish()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# import nbdev # type: ignore\n",
    "# nbdev.nbdev_export() # type: ignore  # noqa: E702\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wolframlanguage14",
   "language": "Wolfram Language",
   "name": "wolframlanguage14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
