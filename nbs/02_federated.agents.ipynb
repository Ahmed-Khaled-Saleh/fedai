{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "> The core abstraction for different FL Agents/Clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp federated.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "from fastcore.all import *\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict,OrderedDict\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import *\n",
    "from community import community_louvain\n",
    "from fedai.utils import *\n",
    "from fedai.client_selector import *\n",
    "from fedai.optimizers import *\n",
    "from fedai.data.core import LLMDataCollator\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from fedai.utils import *\n",
    "from fedai.metrics import *\n",
    "from transformers import AutoTokenizer\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "import numpy as np\n",
    "import math\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentRole(Enum):\n",
    "    SERVER = 1\n",
    "    CLIENT = 2\n",
    "    MARL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Agent\n",
    "\n",
    "An agent is an entity that has a state and exist in an environment. In the case of Federated learning (FL), the agent's state is defined as its own model, data, criterion, optimizer. FL Focuses on distributed model training across multiple clients (agents), each with its local data. Clients **collaborate** to improve a global or shared model while keeping their data private. Communication is often periodic (e.g., every few training rounds). On the other hand, Multi-agent RL systems (MARL) Involves multiple agents interacting with an environment to learn policies for specific tasks (e.g., navigation, resource allocation). Each agent has a state also, but the state represntation might differ slightly from that of an FL agent. The data is often not preloaded as in FL rather, it's collected from the environemnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT):\n",
    "        \n",
    "        self.id = id # each agent has a unique id\n",
    "        self.cfg = cfg # contains all the configurations needed for the agent/trainer.\n",
    "        self.state = state # A dictionary containing the state of the agent\n",
    "        self.role = role # either a client or a server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def init_agent(self: Agent):\n",
    "    # Initialize the state of the agent. In FL Agent, this means making any adjustments to the model/optimizer/state_dict/...etc\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, msg):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update_state(self: Agent):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: Agent):\n",
    "    # save the state of the agent to a file on disk (id, model, optimizer, loss_fn).\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: Agent):\n",
    "    self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARLAgent(Agent):\n",
    "    def _sense(self, state):\n",
    "        # sense the environment\n",
    "        self.state = state\n",
    "\n",
    "    def _decide(self):\n",
    "    # Compute the next action(s) based on the current state and observations.\n",
    "        pass\n",
    "\n",
    "    def _act(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Agent (FedAVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FLAgent(Agent):\n",
    "    # A Federated Learning Agent implementing `FedAVG`.\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None # The data block (local data of the FL Agent).\n",
    "                 ):  \n",
    "                 \n",
    "        super().__init__(id, cfg, state, role)\n",
    "\n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.train_ds, self.test_ds = block[0], block[1]\n",
    "            \n",
    "            for key, value in self.state.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "            self.train_loader = prepare_dl(self.cfg, self.train_ds)  # noqa: F405\n",
    "            self.test_loader = prepare_dl(self.cfg, self.test_ds) # noqa: F405\n",
    "\n",
    "            self.training_metrics = Metrics(list(self.cfg.training_metrics))  # noqa: F405\n",
    "            self.test_metrics = Metrics(list(self.cfg.test_metrics))  # noqa: F405\n",
    "\n",
    "            self.data_key, self.label_key = 'x', 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def server_init(self: FLAgent, client_fn, client_selector, client_cls, loss_fn, writer):\n",
    "    self.client_fn = client_fn\n",
    "    self.client_selector = client_selector\n",
    "    self.client_cls = client_cls\n",
    "    self.loss_fn = loss_fn\n",
    "    self.writer = writer\n",
    "    self.latest_round = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data blocks are already on the disk, and since RL agents don't have a preloaded data blocks, we don't include the data in the FL agent's state. Another ratioanle behind this decision is that, state should contain dynamic objects that change over the interaction of the agents and data blocks are static in the case of FL agents, unless you are doing FL-RL Agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every client abstraction, whether it a base or any other type of federated client, it will initalize the training locally with a set of steps. This might include things like extracting the peft model out of the base model (in the case of LLMs clients). Also, it will terminate the local training with some steps, like saving the model state dictionary and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def runFL(self: FLAgent):\n",
    "    res =  []\n",
    "    all_ids = self.client_selector.select()\n",
    "    \n",
    "    for t in range(1, self.cfg.n_rounds + 1):\n",
    "        lst_active_ids = all_ids[t]\n",
    "        len_clients_ds = []\n",
    "        \n",
    "        train_res, test_res = self.evaluate(t)\n",
    "        train_df, test_df = self.writer.write(lst_active_ids, train_res, test_res, t) \n",
    "        res.append((train_df, test_df))\n",
    "        \n",
    "        for id in lst_active_ids:\n",
    "            client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn)\n",
    "            len_clients_ds.append(len(client.train_ds))\n",
    "            \n",
    "            self.communicate(client) \n",
    "            client.fit()\n",
    "\n",
    "            client.communicate(self) \n",
    "            self.latest_round[id] = t \n",
    "\n",
    "        self.aggregate(lst_active_ids, t, len_clients_ds)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    self.writer.save(res)\n",
    "    self.writer.finish()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate(self: FLAgent, t):\n",
    "    lst_train_res = []\n",
    "    lst_test_res = []\n",
    "    for id in range(self.cfg.num_clients):\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn)\n",
    "        \n",
    "        res_train = client.evaluate_local(loader= 'train')\n",
    "        lst_train_res.append(res_train)\n",
    "\n",
    "        res_test = client.evaluate_local(loader= 'test')\n",
    "        lst_test_res.append(res_test)\n",
    "    return lst_train_res, lst_test_res    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adjust the string reprsntation of the client abstraction to make it more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self: FLAgent) -> str:\n",
    "    return f'''{self.__class__.__name__}: {self.__class__.__name__}\n",
    "    Index : {self.id}\n",
    "    Model: {self.model.__class__.__name__}\n",
    "    Criterion: {self.criterion.__class__.__name__}\n",
    "    Optimizer: {self.optimizer.__class__.__name__}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: FLAgent):\n",
    "    self.model = None if hasattr(self, 'model') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_batch(self: FLAgent, batch):\n",
    "    return {k: v.to(self.device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: FLAgent, batch):\n",
    "    X, y = batch['x'], batch['y']\n",
    "    outputs = self.model(X)\n",
    "    loss = self.criterion(outputs, y)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: FLAgent, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "\n",
    "    try:\n",
    "        loss, logits = self._forward(batch)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: FLAgent, batch: dict) -> tuple:\n",
    "    self.model.zero_grad()\n",
    "    loss, metrics = self._closure(batch)\n",
    "\n",
    "    if loss.item() == 0.0:\n",
    "        return loss, metrics\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: FLAgent):\n",
    "\n",
    "    for i, batch in enumerate(self.train_loader):\n",
    "        batch = self.get_batch(batch)\n",
    "        self._run_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def fit(self: FLAgent) -> dict:\n",
    "    \n",
    "    self.model = self.model.to(self.device)\n",
    "    self.model.train()\n",
    "    for _ in range(self.cfg.local_epochs):\n",
    "        self._run_epoch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate_local(self: FLAgent, loader= 'train') -> dict:\n",
    "    total_loss = 0\n",
    "    lst_metrics = []\n",
    "\n",
    "    self.model = self.model.to(self.device)\n",
    "    self.model.eval()\n",
    "    num_eval = 0\n",
    "    data_loader = self.train_loader if loader == 'train' else self.test_loader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            batch = self.get_batch(batch)\n",
    "            loss, metrics = self._closure(batch)                 \n",
    "\n",
    "            if not math.isnan(loss.item()):\n",
    "                total_loss += loss.item()  \n",
    "                num_eval += len(batch[self.data_key])  # Ensure num_eval is updated\n",
    "                lst_metrics.append(metrics)           \n",
    "    \n",
    "    avg_loss = total_loss / num_eval if num_eval > 0 else 0.0\n",
    "\n",
    "    if lst_metrics:\n",
    "        total_metrics = {k: sum(m.get(k, 0) for m in lst_metrics) / len(lst_metrics) for k in self.cfg.test_metrics}\n",
    "    else:\n",
    "        total_metrics = {k: 0.0 for k in self.cfg.test_metrics}\n",
    "\n",
    "    return {\"loss\": avg_loss, \"metrics\": total_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: FLAgent, state_dict):  # noqa: F811\n",
    "    # save the model to self.cfg.save_dir/comm_round/f\"local_output_{id}\"/state.pth\n",
    "    \n",
    "    state_path = os.path.join(self.cfg.save_dir, \n",
    "                              str(self.t),\n",
    "                              f\"local_output_{self.id}\",\n",
    "                              \"state.pth\")\n",
    "    if not os.path.exists(os.path.dirname(state_path)):\n",
    "        os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "    state_dict['model'] = self.model.state_dict()\n",
    "    state_dict['optimizer'] = self.optimizer.state_dict()\n",
    "\n",
    "    torch.save(state_dict, state_path)\n",
    "\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        save_space(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To do: implement the communication process in **Protobuf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication\n",
    "\n",
    "Communication refers to the process of downloading and uploading models from the server and to the client. Since we are safeguarding against memory issues, we use sequential client processing and disk checkpointing as our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, another_agent: Agent):  # noqa: F811\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        self.save_state(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggegation for `fedavg` is defined as:\n",
    "\n",
    "$$m_t \\leftarrow \\sum_{k \\in S_t} n_k$$\n",
    "$$W_{global}^{(t + 1)} \\leftarrow \\sum_{k \\in S_t} \\frac{n_k}{m_t} w_k^{(t + 1)}$$\n",
    "\n",
    "where $S_t$ is the set of active clients at communication round $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: FLAgent, lst_active_ids, comm_round, len_clients_ds):\n",
    "        \n",
    "    m_t = sum(len_clients_ds)\n",
    "    with torch.no_grad():\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{id}\",\n",
    "                                    \"state.pth\")\n",
    "            \n",
    "            state = torch.load(state_path, weights_only=False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            if i == 0:\n",
    "                global_model = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_state_dict.items()\n",
    "                }\n",
    "\n",
    "            n_k = len_clients_ds[i]\n",
    "            weight =  n_k / m_t \n",
    "\n",
    "        \n",
    "            for key in client_state_dict.keys():\n",
    "                global_model[key].add_(weight * client_state_dict[key])\n",
    "\n",
    "\n",
    "        server_state = {\n",
    "            'model': global_model,\n",
    "        }\n",
    "\n",
    "        server_state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    \"global_model\",\n",
    "                                    \"state.pth\")\n",
    "        os.makedirs(os.path.dirname(server_state_path), exist_ok=True)\n",
    "\n",
    "        torch.save(server_state, server_state_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fedu Agent (MTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This client uses **Multi-Task-Learning** scheme to personalize the models in a non-iid setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Fedu(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "\n",
    "        b = np.random.uniform(0,1,size=(self.cfg.num_clients, self.cfg.num_clients))\n",
    "        b_symm = (b + b.T)/2\n",
    "        b_symm[b_symm < 0.25] = 0\n",
    "        self.alk_connection = b_symm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Algorithm learns a model per client, and add a new aggregation scheme defined as\n",
    "\n",
    "$$ W_{k}^{(t)} = W_{k, R}^{(t)} - \\lambda \\eta_2 \\sum_{\\ell \\in N_k} a_{k, \\ell} (W_{k, R}^{(t)} - W_{\\ell, R}^{(t)})$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter and $\\eta_2$ is defined as $$\\eta_2 = \\eta_1 * R $$\n",
    "\n",
    "such that $\\eta_1$ is the local learning rate, and $R$ is the number of lcoal iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: Fedu, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    global_lr = float(self.cfg.lr) * float(self.cfg.local_epochs)\n",
    "    reg_param = self.cfg.lambda_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{id}\",\n",
    "                                    \"state.pth\")\n",
    "            \n",
    "            state = torch.load(state_path, weights_only= False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            client_diff = {\n",
    "                key: torch.zeros_like(value) \n",
    "                for key, value in client_state_dict.items()\n",
    "            }\n",
    "            \n",
    "            for j, other_id in enumerate(lst_active_ids):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                other_state_path = os.path.join(self.cfg.save_dir,\n",
    "                                                str(comm_round),\n",
    "                                                f\"local_output_{other_id}\",\n",
    "                                                \"state.pth\")\n",
    "                \n",
    "                other_state = torch.load(other_state_path, weights_only= False)\n",
    "                other_state_dict = other_state['model']\n",
    "\n",
    "                weight = self.alk_connection[int(id)][int(other_id)]\n",
    "                for key in client_state_dict.keys():\n",
    "                    client_diff[key].add_(weight * (client_state_dict[key] - other_state_dict[key]))\n",
    "\n",
    "            for key in client_state_dict:\n",
    "                client_state_dict[key].sub_(global_lr * reg_param * client_diff[key])\n",
    "\n",
    "            clinet_state = {\n",
    "                'model': client_state_dict,\n",
    "            }\n",
    "\n",
    "            agg_client_state_path = os.path.join(self.cfg.save_dir,\n",
    "                                                 str(comm_round),\n",
    "                                                 f\"aggregated_model_{id}\",\n",
    "                                                 \"state.pth\")\n",
    "            \n",
    "            if not os.path.exists(os.path.dirname(agg_client_state_path)):\n",
    "                os.makedirs(os.path.dirname(agg_client_state_path))\n",
    "\n",
    "            torch.save(clinet_state, agg_client_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMTL-G (MTL)\n",
    "Distributed Multi-Task Learning over Graphs: A game-theoretic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DMTL(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a weihted graph where the weights are defined as:\n",
    "\n",
    "$\\begin{align}\n",
    "\\operatorname{Sim} = \\sum_{i \\in S} \\alpha \\cdot \\operatorname{Sim}_{\\text {head }}(C_j)+(1-\\alpha) \\cdot \\operatorname{Sim}_{\\mathrm{repr}}(C_j)\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\operatorname{Sim}_{\\text {head }}(C_j)=\\frac{1}{|C_j|^2} \\sum_{k, \\ell \\in C_j} \\left\\|\\bm{w}_k-\\bm{w}_\\ell\\right\\|, \\nonumber \\\\ \\operatorname{Sim}_{\\text {repr }}(C_j)=\\frac{1}{|C_j|^2} \\sum_{k, \\ell \\in C_j} \\cos \\left(\\bm{h}_k^{\\text { }}, \\bm{h}_\\ell^{\\text { }}\\right)\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def head_similarity(self: DMTL, model1, model2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def repr_similarity(self: DMTL, model1, model2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def build_graph(self: DMTL, cfg, model1, model2):\n",
    "#     h_sim = self.head_similarity(model1, model2)\n",
    "#     r_sim = self.repr_similarity(model1, model2)\n",
    "#     return (self.cfg.alpha) * h_sim + (1-self.cfg.alpha) * r_sim\n",
    "#     graph = np.random.rand(cfg.num_clients, cfg.num_clients)\n",
    "#     graph = graph / graph.sum(axis=1)[:, None]\n",
    "#     return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a wighted graph, we nedd to form the coalitions (detecting the communities). We do so by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_coalitions(self: DMTL):\n",
    "    return community_louvain.best_partition(self.G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation rule here is two-folds:\n",
    "- Representations are aggregated as:\n",
    "  $$ h_c = \\sum_{k \\in C_{j}} \\frac{\\zeta_k}{\\sum_{k \\in C_{j}} \\zeta_k}h_k$$\n",
    "\n",
    "  where $\\zeta$ is the shapely value.\n",
    "\n",
    "- Classification heads are aggregated as:\n",
    "$$  w_k^{(t+1)} = w_{k, R}^{(t)} - \\lambda \\eta_2 \\sum_{\\ell \\in C_{j}} a_{k, \\ell} (w_{k,R}^{(t)} - w_{\\ell, R}^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_shapley_vals(self: DMTL):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: DMTL, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    global_lr = float(self.cfg.lr) * float(self.cfg.local_epochs)\n",
    "    reg_param = self.cfg.lambda_\n",
    "    self.G = self.build_graph(self.cfg)\n",
    "    partitions = self.get_coalitions()\n",
    "    # shapely_values = self.get_shapley_vals()\n",
    "\n",
    "    for k, v in partitions.items():\n",
    "        for i, id in enumerate(v):\n",
    "            state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{id}\",\n",
    "                                    \"state.pth\")\n",
    "            state = torch.load(state_path, weights_only= False)\n",
    "            client_head = state['model']['head']\n",
    "            client_repr = state['model']['repr']\n",
    "\n",
    "            if i == 0:\n",
    "                client_avg = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_head.items()\n",
    "                }\n",
    "\n",
    "            for j, other_id in enumerate(v):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                other_state_path = os.path.join(self.cfg.save_dir,\n",
    "                                                str(comm_round),\n",
    "                                                f\"local_output_{other_id}\",\n",
    "                                                \"state.pth\")\n",
    "                other_state = torch.load(other_state_path, weights_only= False)\n",
    "                other_head = other_state['model']['head']\n",
    "                other_repr = other_state['model']['repr']\n",
    "\n",
    "\n",
    "\n",
    "        client_diff = {\n",
    "            key: torch.zeros_like(value) \n",
    "            for key, value in client_head.items()\n",
    "        }\n",
    "        \n",
    "        for j, other_id in enumerate(lst_active_ids):\n",
    "            if i == j:\n",
    "                continue\n",
    "            other_state_path = os.path.join(self.cfg.save_dir,\n",
    "                                            str(comm_round),\n",
    "                                            f\"local_output_{other_id}\",\n",
    "                                            \"state.pth\")\n",
    "            \n",
    "            other_state = torch.load(other_state_path, weights_only= False)\n",
    "            other_state_dict = other_state['model']\n",
    "\n",
    "            weight = self.alk_connection[int(id)][int(other_id)] #FIXME\n",
    "            for key in client_state_dict.keys():\n",
    "                client_diff[key].data += weight * (client_state_dict[key].data.clone() - other_state_dict[key].data.clone())\n",
    "\n",
    "    for key in client_state_dict:\n",
    "        client_state_dict[key].data -=  global_lr * reg_param * client_diff[key].data\n",
    "\n",
    "    clinet_state = {\n",
    "        'model': client_state_dict,\n",
    "    }\n",
    "\n",
    "    agg_client_state_path = os.path.join(self.cfg.save_dir, \n",
    "                                  str(comm_round),\n",
    "                                  f\"aggregated_model_{id}\",\n",
    "                                  \"state.pth\")\n",
    "    if not os.path.exists(os.path.dirname(agg_client_state_path)):\n",
    "        os.makedirs(os.path.dirname(agg_client_state_path))\n",
    "\n",
    "    torch.save(clinet_state, agg_client_state_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to align the represntation of the coalition with the local represntation. This process is done as follows:\n",
    "\n",
    "$$\\min_{\\phi_i} \\left\\|\\bm{\\phi}_i(x)- \\bm{h}_c\\right\\|^2 ; \\hspace{0.3cm} \\forall x \\in D_k$$\n",
    "\n",
    "We prevent the coallpse of reprenstations as follows:\n",
    "\n",
    "$$\\bm{h}_c^{(i+1)} = \\beta \\bm{h}_c^{(i)} + (1-\\beta) \\bm{\\phi}_i(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def repr_alignment(self: DMTL):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PeftAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 block,\n",
    "                 id,\n",
    "                 state= None,\n",
    "                 role= \"client\",\n",
    "                 **adapter_settings):\n",
    "        super().__init__(cfg, block, id, state, role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def peftify(self: PeftAgent):\n",
    "    # extract only the adapter's parameters from the model and store them in a dictionary\n",
    "    self.params_dict_old = deepcopy(\n",
    "        OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if\n",
    "                    \"default\" in name))\n",
    "    \n",
    "    self.params_dict_new = deepcopy(self.params_dict_old)\n",
    "    \n",
    "    self.model.state_dict = (\n",
    "        lambda instance, *_, **__: get_peft_model_state_dict(  # noqa: F405\n",
    "            instance, self.params_dict_new, \"default\"\n",
    "        )\n",
    "    ).__get__(self.model, type(self.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch \n",
    "def init_agent(self: PeftAgent):  # noqa: F811\n",
    "    self.peftify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state_(self: PeftAgent, epoch, local_dataset_len_dict, previously_selected_clients_set):  # noqa: F811\n",
    "    # save the new adapter weights to disk\n",
    "    self.save_state(epoch)\n",
    "\n",
    "    local_dataset_len_dict[self.id] = len(self.block)\n",
    "    older_adapter_weight = get_peft_model_state_dict(self.model, self.params_dict_old, \"default\")  # noqa: F405\n",
    "    set_peft_model_state_dict(self.model, older_adapter_weight, \"default\")  # noqa: F405\n",
    "    previously_selected_clients_set = previously_selected_clients_set | set({self.id})\n",
    "    last_client_id = self.id\n",
    "\n",
    "    return self.model, local_dataset_len_dict, previously_selected_clients_set, last_client_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fed-Sophia Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FedSophiaAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None):\n",
    "        super().__init__(id, cfg, state, role, block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train(self: FedSophiaAgent):\n",
    "    trainer = self.trainer(self) \n",
    "    client_history = trainer.fit() \n",
    "    return client_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PADG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PadgAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None):\n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "\n",
    "        if role == AgentRole.SERVER:\n",
    "            self.connections = torch.from_numpy(generate_graph(self.cfg.num_clients))  # noqa: F405\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def apply_constraints(self: PadgAgent, \n",
    "                      graph, # (np.ndarray): The input matrix.\n",
    "                      symmetrize=True, # (bool): If True, makes the matrix symmetric.\n",
    "                      normalize=True, # (bool): If True, normalizes the matrix symmetrically.\n",
    "                      threshold= 0, # (float or None): If provided, sets values below this threshold to 0.\n",
    "                      diag_fill= 0): # (float or None): If provided, fills the diagonal with this value.\n",
    "    \n",
    "\n",
    "    # Symmetrize the matrix\n",
    "    if symmetrize:\n",
    "        graph = (graph + graph.T) / 2\n",
    "\n",
    "    # Apply threshold to ensure non-negativity\n",
    "    if threshold is not None:\n",
    "        graph = torch.where(graph > threshold, graph, 0)\n",
    "\n",
    "    # Normalize the matrix symmetrically\n",
    "    if normalize:\n",
    "        row_sums = graph.sum(axis=1, keepdims=True)\n",
    "        col_sums = graph.sum(axis=0, keepdims=True)\n",
    "        norm_factor = torch.sqrt(row_sums @ col_sums)  # Symmetric normalization factor\n",
    "        graph = torch.divide(graph, norm_factor, where=norm_factor != 0)\n",
    "\n",
    "    # Fill the diagonal\n",
    "    if diag_fill is not None:\n",
    "        torch.fill_diagonal(graph, diag_fill)\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedai.models import *  # noqa: F403\n",
    "model = MLP(dim_in= 784, dim_hidden= 64, dim_out= 10)  # noqa: F405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "rand_in = torch.rand(1, 1, 28, 28)\n",
    "rand_in2 = torch.rand(1, 1, 28, 28)\n",
    "out1 = model(rand_in)\n",
    "out2 = model(rand_in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3843, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the KL divergence between the two outputs\n",
    "dist1 = F.softmax(out1, dim= -1)\n",
    "dist2 = F.softmax(out2, dim= -1)\n",
    "F.kl_div(dist1, dist2, reduction= 'batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def compute_probs(self: PadgAgent,\n",
    "                  batch_size=32, # batch_size (int): Batch size for evaluation.\n",
    "                  return_log_probs=True): # return_log_probs (bool): If True, return log-probabilities; otherwise, return probabilities.\n",
    "    \n",
    "    # Computes probabilities or log-probabilities across the entire dataset for a given model.\n",
    "    # Ensure model is in evaluation mode\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    self.model.to(device)\n",
    "    self.model.eval()\n",
    "    \n",
    "    # Create DataLoader for the dataset\n",
    "    dataloader = DataLoader(self.train_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_probs = []  # To store probabilities or log-probabilities for all batches\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in dataloader:  # Assuming dataset returns (inputs, labels)\n",
    "            inputs = batch['x'].to(device)  # Move to model's device\n",
    "            \n",
    "            logits = self.model(inputs)\n",
    "            \n",
    "            if return_log_probs:\n",
    "                # Convert logits to log-probabilities\n",
    "                batch_log_probs = F.log_softmax(logits, dim=-1)\n",
    "                all_probs.append(batch_log_probs)\n",
    "            else:\n",
    "                # Convert logits to probabilities\n",
    "                batch_probs = F.softmax(logits, dim=-1)\n",
    "                all_probs.append(batch_probs)\n",
    "    \n",
    "    self.model.to('cpu')\n",
    "    # Concatenate all batch probabilities/log-probabilities\n",
    "    return torch.cat(all_probs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(self: PadgAgent,\n",
    "                 ):\n",
    "    # compute the graident f R(w, A) w.r.t w\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: PadgAgent, lst_active_ids, comm_round, len_clients_ds, one_model= False):\n",
    "    \n",
    "    visited = []\n",
    "    for i, id in enumerate(lst_active_ids):\n",
    "\n",
    "        neighbour_ids = torch.where(self.connections[id] != float(0))[0]\n",
    "\n",
    "        model_path = os.path.join(self.cfg.save_dir, \n",
    "                                   str(comm_round),\n",
    "                                   f\"local_output_{id}\",\n",
    "                                   \"pytorch_model.pth\")\n",
    "        client_state_dict = torch.load(model_path, map_location='cpu', weights_only= False)\n",
    "        self.model.load_state_dict(client_state_dict)\n",
    "        \n",
    "        neighbours_sum = {\n",
    "            key: torch.zeros_like(value) \n",
    "            for key, value in client_state_dict.items()\n",
    "        }\n",
    "            \n",
    "        probs_1 = self.compute_probs(batch_size=32, return_log_probs=True)\n",
    "        \n",
    "        for other_id in neighbour_ids:\n",
    "\n",
    "            if (other_id, id) in visited:\n",
    "                continue\n",
    "            other_model_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{other_id}\",\n",
    "                                    \"pytorch_model.pth\")\n",
    "            \n",
    "            other_client_state_dict = torch.load(other_model_path, map_location='cpu', weights_only= False)\n",
    "            self.model.load_state_dict(other_client_state_dict)\n",
    "            \n",
    "            probs_2 = self.compute_probs(batch_size=32, return_log_probs=False)\n",
    "\n",
    "            kl_div = F.kl_div(probs_1, probs_2, reduction= 'batchmean').to('cpu')\n",
    "            self.connections[id][other_id] -= self.cfg.server_lr * self.cfg.lambda_ * kl_div\n",
    "\n",
    "            # apply constraints to the KL divergence\n",
    "            self.connections[id][other_id] = self.apply_constraints(self.connections[id][other_id])\n",
    "            self.connections[id][other_id] = self.connections[other_id][id]\n",
    "\n",
    "            visited.append((id, other_id))\n",
    "            visited.append((other_id, id))\n",
    "            \n",
    "        for other_id in neighbour_ids:\n",
    "            other_model_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{other_id}\",\n",
    "                                    \"pytorch_model.pth\")\n",
    "            other_client_state_dict = torch.load(other_model_path, map_location='cpu', weights_only= False)\n",
    "\n",
    "            weight = self.connections[id][other_id]\n",
    "            for key in other_client_state_dict.keys():\n",
    "                neighbours_sum[key].data += weight * other_client_state_dict[key].data\n",
    "\n",
    "        # for key in neighbours_sum.keys():\n",
    "            # neighbours_sum[key].data /= len(neighbour_ids)\n",
    "\n",
    "        for key in client_state_dict.keys():\n",
    "            client_state_dict[key].data = self.cfg.beta * client_state_dict[key].data + (1 - self.cfg.beta) * neighbours_sum[key].data\n",
    "\n",
    "    \n",
    "        # save the updated model to the disk\n",
    "        self.save_state(client_state_dict, comm_round + 1, id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira clients have more parameters. Since it's a client for LLM in principle, we need to feed the generation dataset (the dataset of text ids at the end layer not the logits). Also, a tokenizer and a collate function that will be used for the generation and the data loader construction processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentMira(FLAgent):\n",
    "    def __init__(self,\n",
    "                 data_dict: dict,\n",
    "                 model: torch.nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 id: int,\n",
    "                 gen_data_dict: dict,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 collat_fn: LLMDataCollator,\n",
    "                 cfg: DictConfig) -> None:\n",
    "            \n",
    "        super().__init__(data_dict, model, criterion, optimizer, id)\n",
    "        \n",
    "        self.train_ds_genr = gen_data_dict['train']\n",
    "        self.test_ds_genr = gen_data_dict['test']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collat_fn = collat_fn\n",
    "        self.cfg = cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to save space, we will replace the original model with only the trainable peft model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mira Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following:\n",
    "- Define a Mira client.\n",
    "- inspect the `init_local_train` and `terminate_local_train` methods and their effect on the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# base_model = deepcopy(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# config = LoraConfig(\n",
    "#     r=8,# arbitrary numbr but usually 8, 16, 32, 64, 128\n",
    "#     target_modules=['c_attn'],\n",
    "#     lora_alpha=8,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "\n",
    "# peft_model = get_peft_model(gpt2, config)\n",
    "# mira  = AgentMira(DataDict, peft_model, criterion, optimizer, 0, train_dataset, test_dataset, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inpect the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
