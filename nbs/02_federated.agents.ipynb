{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "> The core abstraction for different FL Agents/Clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp federated.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "from fastcore.all import *\n",
    "import os\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict,OrderedDict\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import *\n",
    "from community import community_louvain\n",
    "from fedai.utils import *\n",
    "from fedai.client_selector import *\n",
    "from fedai.optimizers import *\n",
    "from fedai.data.core import LLMDataCollator\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from fedai.utils import *\n",
    "from fedai.metrics import *\n",
    "from fedai.losses import *\n",
    "from transformers import AutoTokenizer\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "import numpy as np\n",
    "import math\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentRole(Enum):\n",
    "    SERVER = 1\n",
    "    CLIENT = 2\n",
    "    MARL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Agent\n",
    "\n",
    "An agent is an entity that has a state and exist in an environment. In the case of Federated learning (FL), the agent's state is defined as its own model, data, criterion, optimizer. FL Focuses on distributed model training across multiple clients (agents), each with its local data. Clients **collaborate** to improve a global or shared model while keeping their data private. Communication is often periodic (e.g., every few training rounds). On the other hand, Multi-agent RL systems (MARL) Involves multiple agents interacting with an environment to learn policies for specific tasks (e.g., navigation, resource allocation). Each agent has a state also, but the state represntation might differ slightly from that of an FL agent. The data is often not preloaded as in FL rather, it's collected from the environemnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT):\n",
    "        \n",
    "        self.id = id # each agent has a unique id\n",
    "        self.cfg = cfg # contains all the configurations needed for the agent/trainer.\n",
    "        self.state = state # A dictionary containing the state of the agent\n",
    "        self.role = role # either a client or a server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def init_agent(self: Agent):\n",
    "    # Initialize the state of the agent. In FL Agent, this means making any adjustments to the model/optimizer/state_dict/...etc\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, msg):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update_state(self: Agent):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: Agent):\n",
    "    # save the state of the agent to a file on disk (id, model, optimizer, loss_fn).\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: Agent):\n",
    "    self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARLAgent(Agent):\n",
    "    def _sense(self, state):\n",
    "        # sense the environment\n",
    "        self.state = state\n",
    "\n",
    "    def _decide(self):\n",
    "    # Compute the next action(s) based on the current state and observations.\n",
    "        pass\n",
    "\n",
    "    def _act(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Agent (FedAVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FLAgent(Agent):\n",
    "    # A Federated Learning Agent implementing `FedAVG`.\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None # The data block (local data of the FL Agent).\n",
    "                 ):  \n",
    "                 \n",
    "        super().__init__(id, cfg, state, role)\n",
    "\n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.train_ds, self.test_ds = block[0], block[1]\n",
    "            \n",
    "            for key, value in self.state.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "            self.train_loader = prepare_dl(self.cfg, self.train_ds)  # noqa: F405\n",
    "            self.test_loader = prepare_dl(self.cfg, self.test_ds) # noqa: F405\n",
    "\n",
    "            self.training_metrics = Metrics(list(self.cfg.training_metrics))  # noqa: F405\n",
    "            self.test_metrics = Metrics(list(self.cfg.test_metrics))  # noqa: F405\n",
    "\n",
    "            self.data_key, self.label_key = 'x', 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def server_init(self: FLAgent, client_fn, client_selector, client_cls, loss_fn, writer):\n",
    "    self.client_fn = client_fn\n",
    "    self.client_selector = client_selector\n",
    "    self.client_cls = client_cls\n",
    "    self.loss_fn = loss_fn\n",
    "    self.writer = writer\n",
    "    self.latest_round = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data blocks are already on the disk, and since RL agents don't have a preloaded data blocks, we don't include the data in the FL agent's state. Another ratioanle behind this decision is that, state should contain dynamic objects that change over the interaction of the agents and data blocks are static in the case of FL agents, unless you are doing FL-RL Agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every client abstraction, whether it a base or any other type of federated client, it will initalize the training locally with a set of steps. This might include things like extracting the peft model out of the base model (in the case of LLMs clients). Also, it will terminate the local training with some steps, like saving the model state dictionary and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def runFL(self: FLAgent):\n",
    "    res =  []\n",
    "    all_ids = self.client_selector.select()\n",
    "    \n",
    "    for t in range(1, self.cfg.n_rounds + 1):\n",
    "        lst_active_ids = all_ids[t]\n",
    "        len_clients_ds = []\n",
    "        \n",
    "        train_res, test_res = self.evaluate(t)\n",
    "        train_df, test_df = self.writer.write(lst_active_ids, train_res, test_res, t) \n",
    "        res.append((train_df, test_df))\n",
    "        \n",
    "        for id in lst_active_ids:\n",
    "            client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, to_read_from= 'global_model')\n",
    "            len_clients_ds.append(len(client.train_ds))\n",
    "            \n",
    "            self.communicate(client) \n",
    "            client.fit()\n",
    "\n",
    "            client.communicate(self) \n",
    "            self.latest_round[id] = t \n",
    "\n",
    "        self.aggregate(lst_active_ids, t, len_clients_ds)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    self.writer.save(res)\n",
    "    self.writer.finish()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate(self: FLAgent, t):\n",
    "    lst_train_res = []\n",
    "    lst_test_res = []\n",
    "    for id in range(self.cfg.num_clients):\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, to_read_from= 'global_model')\n",
    "        \n",
    "        res_train = client.evaluate_local(loader= 'train')\n",
    "        lst_train_res.append(res_train)\n",
    "\n",
    "        res_test = client.evaluate_local(loader= 'test')\n",
    "        lst_test_res.append(res_test)\n",
    "    return lst_train_res, lst_test_res    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adjust the string reprsntation of the client abstraction to make it more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self: FLAgent) -> str:\n",
    "    return f'''{self.__class__.__name__}: {self.__class__.__name__}\n",
    "    Index : {self.id}\n",
    "    Model: {self.model.__class__.__name__}\n",
    "    Criterion: {self.criterion.__class__.__name__}\n",
    "    Optimizer: {self.optimizer.__class__.__name__}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: FLAgent):\n",
    "    self.model = None if hasattr(self, 'model') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_batch(self: FLAgent, batch):\n",
    "    return {k: v.to(self.device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: FLAgent, batch):\n",
    "    X, y = batch['x'], batch['y']\n",
    "    outputs = self.model(X)\n",
    "    loss = self.criterion(outputs, y)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: FLAgent, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "\n",
    "    try:\n",
    "        loss, logits = self._forward(batch)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: FLAgent, batch: dict) -> tuple:\n",
    "    self.optimizer.zero_grad()\n",
    "    loss, metrics = self._closure(batch)\n",
    "\n",
    "    if loss.item() == 0.0:\n",
    "        return loss, metrics\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: FLAgent):\n",
    "\n",
    "    for i, batch in enumerate(self.train_loader):\n",
    "        batch = self.get_batch(batch)\n",
    "        self._run_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def fit(self: FLAgent) -> dict:\n",
    "    \n",
    "    self.model = self.model.to(self.device)\n",
    "    self.model.train()\n",
    "    for _ in range(self.cfg.local_epochs):\n",
    "        self._run_epoch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate_local(self: FLAgent, loader= 'train') -> dict:\n",
    "    total_loss = 0\n",
    "    lst_metrics = []\n",
    "\n",
    "    self.model = self.model.to(self.device)\n",
    "    self.model.eval()\n",
    "    num_eval = 0\n",
    "    data_loader = self.train_loader if loader == 'train' else self.test_loader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            batch = self.get_batch(batch)\n",
    "            loss, metrics = self._closure(batch)                 \n",
    "\n",
    "            if not math.isnan(loss.item()):\n",
    "                total_loss += loss.item()  \n",
    "                num_eval += len(batch[self.data_key])  # Ensure num_eval is updated\n",
    "                lst_metrics.append(metrics)           \n",
    "    \n",
    "    avg_loss = total_loss / num_eval if num_eval > 0 else 0.0\n",
    "\n",
    "    if lst_metrics:\n",
    "        total_metrics = {k: sum(m.get(k, 0) for m in lst_metrics) / len(lst_metrics) for k in self.cfg.test_metrics}\n",
    "    else:\n",
    "        total_metrics = {k: 0.0 for k in self.cfg.test_metrics}\n",
    "\n",
    "    return {\"loss\": avg_loss, \"metrics\": total_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: FLAgent, state_dict):  # noqa: F811\n",
    "    # save the model to self.cfg.save_dir/comm_round/f\"local_output_{id}\"/state.pth\n",
    "    \n",
    "    state_path = os.path.join(self.cfg.save_dir, \n",
    "                              str(self.t),\n",
    "                              f\"local_output_{self.id}\",\n",
    "                              \"state.pth\")\n",
    "    if not os.path.exists(os.path.dirname(state_path)):\n",
    "        os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "    state_dict['model'] = self.model.state_dict()\n",
    "    state_dict['optimizer'] = self.optimizer.state_dict()\n",
    "\n",
    "    torch.save(state_dict, state_path)\n",
    "\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        save_space(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To do: implement the communication process in **Protobuf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication\n",
    "\n",
    "Communication refers to the process of downloading and uploading models from the server and to the client. Since we are safeguarding against memory issues, we use sequential client processing and disk checkpointing as our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, another_agent: Agent):  # noqa: F811\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        self.save_state(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggegation for `fedavg` is defined as:\n",
    "\n",
    "$$m_t \\leftarrow \\sum_{k \\in S_t} n_k$$\n",
    "$$W_{global}^{(t + 1)} \\leftarrow \\sum_{k \\in S_t} \\frac{n_k}{m_t} w_k^{(t + 1)}$$\n",
    "\n",
    "where $S_t$ is the set of active clients at communication round $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: FLAgent, lst_active_ids, comm_round, len_clients_ds):\n",
    "        \n",
    "    m_t = sum(len_clients_ds)\n",
    "    with torch.no_grad():\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{id}\",\n",
    "                                    \"state.pth\")\n",
    "            \n",
    "            state = torch.load(state_path, weights_only=False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            if i == 0:\n",
    "                global_model = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_state_dict.items()\n",
    "                }\n",
    "\n",
    "            n_k = len_clients_ds[i]\n",
    "            weight =  n_k / m_t \n",
    "\n",
    "        \n",
    "            for key in client_state_dict.keys():\n",
    "                global_model[key].add_(weight * client_state_dict[key])\n",
    "\n",
    "\n",
    "        server_state = {\n",
    "            'model': global_model,\n",
    "        }\n",
    "\n",
    "        server_state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    \"global_model\",\n",
    "                                    \"state.pth\")\n",
    "        \n",
    "        if not os.path.exists(os.path.dirname(server_state_path)):\n",
    "            os.makedirs(os.path.dirname(server_state_path), exist_ok=True)\n",
    "        torch.save(server_state, server_state_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fedu Agent (MTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This client uses **Multi-Task-Learning** scheme to personalize the models in a non-iid setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Fedu(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "\n",
    "        b = np.random.uniform(0,1,size=(self.cfg.num_clients, self.cfg.num_clients))\n",
    "        b_symm = (b + b.T)/2\n",
    "        b_symm[b_symm < 0.25] = 0\n",
    "        self.alk_connection = b_symm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Algorithm learns a model per client, and add a new aggregation scheme defined as\n",
    "\n",
    "$$ W_{k}^{(t)} = W_{k, R}^{(t)} - \\lambda \\eta_2 \\sum_{\\ell \\in N_k} a_{k, \\ell} (W_{k, R}^{(t)} - W_{\\ell, R}^{(t)})$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter and $\\eta_2$ is defined as $$\\eta_2 = \\eta_1 * R $$\n",
    "\n",
    "such that $\\eta_1$ is the local learning rate, and $R$ is the number of lcoal iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: Fedu, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    global_lr = float(self.cfg.lr) * float(self.cfg.local_epochs)\n",
    "    reg_param = self.cfg.lambda_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{id}\",\n",
    "                                    \"state.pth\")\n",
    "            \n",
    "            state = torch.load(state_path, weights_only= False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            client_diff = {\n",
    "                key: torch.zeros_like(value) \n",
    "                for key, value in client_state_dict.items()\n",
    "            }\n",
    "            \n",
    "            for j, other_id in enumerate(lst_active_ids):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                other_state_path = os.path.join(self.cfg.save_dir,\n",
    "                                                str(comm_round),\n",
    "                                                f\"local_output_{other_id}\",\n",
    "                                                \"state.pth\")\n",
    "                \n",
    "                other_state = torch.load(other_state_path, weights_only= False)\n",
    "                other_state_dict = other_state['model']\n",
    "\n",
    "                weight = self.alk_connection[int(id)][int(other_id)]\n",
    "                for key in client_state_dict.keys():\n",
    "                    client_diff[key].add_(weight * (client_state_dict[key] - other_state_dict[key]))\n",
    "\n",
    "            for key in client_state_dict:\n",
    "                client_state_dict[key].sub_(global_lr * reg_param * client_diff[key])\n",
    "\n",
    "            clinet_state = {\n",
    "                'model': client_state_dict,\n",
    "            }\n",
    "\n",
    "            agg_client_state_path = os.path.join(self.cfg.save_dir,\n",
    "                                                 str(comm_round),\n",
    "                                                 f\"aggregated_model_{id}\",\n",
    "                                                 \"state.pth\")\n",
    "            \n",
    "            if not os.path.exists(os.path.dirname(agg_client_state_path)):\n",
    "                os.makedirs(os.path.dirname(agg_client_state_path))\n",
    "\n",
    "            torch.save(clinet_state, agg_client_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMTL-G (MTL)\n",
    "Distributed Multi-Task Learning over Graphs: A game-theoretic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DMTL(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "        \n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.anchorloss = AnchorLoss(self.cfg.data.num_classes, self.cfg.model.hidden_size).to(self.device)\n",
    "            self.anchorloss.anchor = self.h_c if self.t > 1 else self.anchorloss.anchor\n",
    "            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "from omegaconf import OmegaConf \n",
    "cfg = OmegaConf.load('./examples/cfg.yaml')\n",
    "cfg.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def server_init(self: DMTL, client_fn, client_selector, client_cls, loss_fn, writer):\n",
    "    FLAgent.server_init(self, client_fn, client_selector, client_cls, loss_fn, writer)\n",
    "    self.classes = self.cfg.data.classes\n",
    "    self.idx_to_cls = {i: self.classes[i] for i in range(len(self.classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def runFL(self: DMTL):\n",
    "    res =  []\n",
    "    all_ids = self.client_selector.select()\n",
    "    \n",
    "    for t in range(1, self.cfg.n_rounds + 1):\n",
    "        lst_active_ids = all_ids[t]\n",
    "        len_clients_ds = {}\n",
    "        \n",
    "        for id in lst_active_ids:\n",
    "            client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, to_read_from= 'aggregated_model_')\n",
    "            len_clients_ds[id] = len(client.train_ds)\n",
    "            \n",
    "            self.communicate(client) \n",
    "            client.fit()\n",
    "\n",
    "            client.communicate(self) \n",
    "            self.latest_round[id] = t \n",
    "\n",
    "        self.aggregate(lst_active_ids, t, len_clients_ds)\n",
    "        # self.extra_computation(lst_active_ids, t)\n",
    "        \n",
    "        train_res, test_res = self.evaluate(t)\n",
    "        train_df, test_df = self.writer.write(lst_active_ids, train_res, test_res, t) \n",
    "        res.append((train_df, test_df))\n",
    "        \n",
    "    self.writer.save(res)\n",
    "    self.writer.finish()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: DMTL, batch):\n",
    "    from IPython.core.debugger import Pdb\n",
    "    ipdb = Pdb(); ipdb.set_trace()\n",
    "    X, y = batch['x'], batch['y']\n",
    "    y_copied = deepcopy(y)\n",
    "    labels = y.type(torch.LongTensor).to(self.device)\n",
    "    ys = labels.float()\n",
    "    \n",
    "    h = self.model.encoder(X)\n",
    "    outputs = self.model.classifier(h)    \n",
    "    \n",
    "    loss_anchor = self.anchorloss(h, ys, Lambda = self.cfg.lambda_anchor)\n",
    "    loss = self.criterion(outputs, y_copied)\n",
    "    return loss + loss_anchor, h, outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: DMTL, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "    h, labels = None, None\n",
    "    try:\n",
    "        loss, h, logits, labels = self._forward(batch)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics, h, labels  # Return safe values\n",
    "\n",
    "    return loss, metrics, h, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: DMTL, batch: dict) -> tuple:\n",
    "    batch_mean_anchor = torch.zeros(self.cfg.data.num_classes, self.cfg.model.hidden_size).to(self.device)\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "    loss, metrics, h, labels = self._closure(batch)\n",
    "    if loss.item() == 0.0 or h is None:\n",
    "        return loss, metrics, batch_mean_anchor\n",
    "    \n",
    "    for i in set(labels.tolist()):\n",
    "        batch_mean_anchor[i] += torch.mean(h[labels==i],dim=0)\n",
    "   \n",
    "    loss.backward()\n",
    "    \n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss, metrics, batch_mean_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: DMTL):\n",
    "\n",
    "    epoch_mean_anchor = torch.zeros(self.cfg.data.num_classes, self.cfg.model.hidden_size).to(self.device)\n",
    "    for batch_idx, batch in enumerate(self.train_loader):\n",
    "        batch = self.get_batch(batch)\n",
    "        _, _, batch_mean_anchor = self._run_batch(batch)\n",
    "\n",
    "        for i in self.label_set:\n",
    "            #compute batch mean anchor according to batch label\n",
    "            batch_mean_anchor[i] = batch_mean_anchor[i]/(batch_idx+1)\n",
    "\n",
    "            #compute epoch mean anchor according to batch mean anchor\n",
    "            lambda_momentum = self.cfg.momentum_anchor #pow(2, -(epoch+1))\n",
    "            epoch_mean_anchor[i] = lambda_momentum*epoch_mean_anchor[i] + (1-lambda_momentum)*batch_mean_anchor[i]\n",
    "        \n",
    "\n",
    "    self.anchorloss.anchor =  torch.nn.Parameter(epoch_mean_anchor, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate(self: DMTL, t):\n",
    "    lst_train_res = []\n",
    "    lst_test_res = []\n",
    "    for id in range(self.cfg.num_clients):\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, to_read_from= 'aggregated_model_')\n",
    "        \n",
    "        res_train = client.evaluate_local(loader= 'train')\n",
    "        lst_train_res.append(res_train)\n",
    "\n",
    "        res_test = client.evaluate_local(loader= 'test')\n",
    "        lst_test_res.append(res_test)\n",
    "    return lst_train_res, lst_test_res    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def pick_n_points(self: DMTL, n= 3):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The communication in DMTL is a matter of sending (saving to the disk) two things, the classification head and a randomly picked data represntation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: DMTL, state_dict):  # noqa: F811\n",
    "    # save the model to self.cfg.save_dir/comm_round/f\"local_output_{id}\"/state.pth\n",
    "    \n",
    "    \n",
    "    state_dict['model'] = self.model.state_dict()\n",
    "    state_dict['optimizer'] = self.optimizer.state_dict()\n",
    "    state_dict['h'] = self.anchorloss.anchor # (num_classes, hidden_size)\n",
    "    state_dict['label_set'] = self.label_set\n",
    "    \n",
    "    # pick a random data point from the train_ds and save it to the state_dict\n",
    "    # data_point = self.train_ds[np.random.randint(0, len(self.train_ds))]\n",
    "    # data_point = self.get_batch(data_point)\n",
    "    # data = data_point['x']\n",
    "    # batched_data_point = data.view(1, 3, 32, 32) # (B, C, H, W)\n",
    "    # state_dict['h'] = self.model.encoder(batched_data_point)\n",
    "\n",
    "    state_path = os.path.join(self.cfg.save_dir, str(self.t), f\"local_output_{self.id}\", \"state.pth\")\n",
    "    if not os.path.exists(os.path.dirname(state_path)):\n",
    "        os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "    torch.save(state_dict, state_path)\n",
    "\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        save_space(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the server, we are doing the followin steps:\n",
    "- Form the coalitions.\n",
    "  - First, construct the weighted undirected graph $\\mathcal{g}$.\n",
    "  - Second, pass this graph to the louvian algorithm to get the communities.\n",
    "- aggregate based on the equations stated in the next cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, model similrity is compared using the norm of the difference between the two models as follows:\n",
    "$$\\operatorname{Sim}_{\\text {head }}(\\bm{w}_k,\\bm{w}_\\ell)=\\left\\|\\bm{w}_k-\\bm{w}_\\ell\\right\\|,$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def model_similarity(self: DMTL, model1, model2):\n",
    "    total_l1_norm = 0.0\n",
    "    \n",
    "    for key in model1.keys():\n",
    "        param1 = model1[key]\n",
    "        param2 = model2[key]\n",
    "        \n",
    "        total_l1_norm += torch.norm(param1 - param2, p=1).item()\n",
    "    \n",
    "    return total_l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the embedding closeness using cosine similiairy.\n",
    "\n",
    "\n",
    "$$\\operatorname{Sim}_{\\mathrm{repr}}(k, \\ell) = \\cos(\\Omega) = \\frac{\\mathbf{h}_k \\cdot \\mathbf{h}_{\\ell}}{\\|\\mathbf{h}_k\\| \\|\\mathbf{h}_{\\ell}\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.1199e-02, -4.1328e-02,  2.0406e-03,  ...,  4.2562e-02,\n",
      "          1.6889e-02, -7.1817e-02],\n",
      "        [-7.9444e-03,  4.9498e-02, -9.6207e-02,  ..., -3.8808e-02,\n",
      "         -1.8961e-05,  1.1457e-02],\n",
      "        [-3.8553e-02,  3.4144e-02,  1.2492e-02,  ..., -2.2866e-02,\n",
      "          3.2718e-02, -1.1679e-02]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0327,  0.0048, -0.0271],\n",
       "        [-0.0087, -0.0430, -0.0395],\n",
       "        [ 0.1083, -0.0309,  0.0110]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "h1 = torch.nn.Parameter(F.normalize(torch.randn(3, 512)), requires_grad=True)\n",
    "h2 = torch.nn.Parameter(F.normalize(torch.randn(3, 512)), requires_grad=True)\n",
    "\n",
    "h = print(h1.data)\n",
    "\n",
    "h1_norm = F.normalize(h1, p=2, dim=1)  # (3, 512)\n",
    "h2_norm = F.normalize(h2, p=2, dim=1)  # (3, 512)\n",
    "cos_sim_matrix = torch.mm(h1_norm, h2_norm.T)  # (3, 3)\n",
    "cos_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0689, -0.0668,  0.0598],\n",
       "        [ 0.0468,  0.0179,  0.0262],\n",
       "        [ 0.0489, -0.0006,  0.0229]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "import torch.nn.functional as F\n",
    "# ensure the batch dimension is removed\n",
    "h1 = torch.randn(3, 512)\n",
    "h2 = torch.randn(3, 512)\n",
    "\n",
    "h1_norm = F.normalize(h1, p=2, dim=1)  # (3, 512)\n",
    "h2_norm = F.normalize(h2, p=2, dim=1)  # (3, 512)\n",
    "\n",
    "cos_sim_matrix = torch.mm(h1_norm, h2_norm.T)  # (3, 3)\n",
    "cos_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "car",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "bird",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "cat",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d0fc153d-3052-441f-946b-1202de4053e5",
       "rows": [
        [
         "plane",
         "-0.06885007",
         "-0.06681198",
         "0.05977638"
        ],
        [
         "car",
         "0.046795364",
         "0.01790896",
         "0.026214145"
        ],
        [
         "bird",
         "0.048908867",
         "-0.00064683147",
         "0.022857765"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>bird</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>-0.068850</td>\n",
       "      <td>-0.066812</td>\n",
       "      <td>0.059776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.017909</td>\n",
       "      <td>0.026214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bird</th>\n",
       "      <td>0.048909</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>0.022858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            car      bird       cat\n",
       "plane -0.068850 -0.066812  0.059776\n",
       "car    0.046795  0.017909  0.026214\n",
       "bird   0.048909 -0.000647  0.022858"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "data = cos_sim_matrix.numpy()\n",
    "cols = [1, 2, 3]#list({0, 1, 2})\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "idx_to_cls = {i: classes[i] for i in range(10)}\n",
    "cols = [idx_to_cls[i] for i in cols]\n",
    "cols2 = ['plane', 'car', 'bird']\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns= cols, index= cols2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0096)\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "single_similarity = cos_sim_matrix.mean()\n",
    "print(single_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0518)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "max_similarity = cos_sim_matrix.max(dim=1).values.mean() \n",
    "print(max_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "# visulaize the vcotrs in 2-d using Tsne and make each vector in the same tensor to have the same color\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity= 4)\n",
    "h = torch.cat([h1_norm, h2_norm], dim=0)\n",
    "h_embedded = tsne.fit_transform(h)\n",
    "\n",
    "sns.scatterplot(x=h_embedded[:, 0], y=h_embedded[:, 1], hue=[0, 0, 0, 1, 1, 1])\n",
    "# label the vectors with h11, h12, h13, h21, h22, h23\n",
    "for i, txt in enumerate(['h11', 'h12', 'h13', 'h21', 'h22', 'h23']):\n",
    "    plt.annotate(txt, (h_embedded[i, 0], h_embedded[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def h_similarity(self: DMTL, h1, h2, label_set, label_set2):\n",
    "    h1 = h1.reshape(self.cfg.data.num_classes, self.cfg.model.hidden_size)\n",
    "    h2 = h2.reshape(self.cfg.data.num_classes, self.cfg.model.hidden_size)\n",
    "    \n",
    "    h1_norm = F.normalize(h1, p=2, dim=1)  # (3, 512)\n",
    "    h2_norm = F.normalize(h2, p=2, dim=1)  # (3, 512)\n",
    "    \n",
    "    cos_sim_matrix = torch.mm(h1_norm, h2_norm.T)  # (3, 3)\n",
    "    max_similarity = cos_sim_matrix.max(dim=1).values.mean()# This gives a higher similarity score if each class in h1 has at least one good match in h2.\n",
    "    data = cos_sim_matrix.numpy()\n",
    "\n",
    "    cols1 = [self.idx_to_cls[i] for i in label_set]\n",
    "    cols2 = [self.idx_to_cls[i] for i in label_set2]\n",
    "\n",
    "    df = pd.DataFrame(data, columns= cols1, index= cols2)\n",
    "    return df, max_similarity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph laplacian matrix  is a symmetric matrix. Also, the weights(value of the matrix) must be normalized so that they sum to $1$, or at least close to $1$. We can use an approximate method to ensure that those criteria are met. The resultant matrix is called a Doubly stochastic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def sym_nromalization(self: DMTL, A):\n",
    "    \"normalize the adjacency matrix while ensuring symmetry\"\n",
    "\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = (A + A.T) / 2  # Ensure symmetry\n",
    "    # Compute the degree matrix (row sums)\n",
    "    row_sums = A.sum(axis=1)\n",
    "    # Avoid division by zero\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    # Compute D^(-1/2)\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(row_sums))\n",
    "    # Symmetric normalization\n",
    "    A_normalized = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "\n",
    "    return A_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a similarity graph using both the local represntation $h$ and the classification head $w$ where similairty is defined as:\n",
    "$$\\operatorname{Sim} = \\sum_{i \\in S} \\alpha \\cdot \\operatorname{Sim}_{\\text {head }}(C_j)+(1-\\alpha) \\cdot \\operatorname{Sim}_{\\mathrm{repr}}(C_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_graph(self: DMTL, lst_active_ids, comm_round):\n",
    "\n",
    "    num_active = len(lst_active_ids)\n",
    "    graph = np.random.rand(num_active, num_active)\n",
    "    graph = graph / graph.sum(axis=1)[:, None]\n",
    "\n",
    "    pair_wise_h_df = []\n",
    "    visited = {}\n",
    "    for i, id in enumerate(lst_active_ids):\n",
    "        state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "        state = torch.load(state_path, weights_only= False)\n",
    "        model1 = state['model']\n",
    "        h1 = state['h']\n",
    "        label_set = state['label_set']\n",
    "\n",
    "        for j, other_id in enumerate(lst_active_ids):\n",
    "            if i == j or (id, other_id) in visited:\n",
    "                continue\n",
    "            other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{other_id}\", \"state.pth\")\n",
    "            other_state = torch.load(other_state_path, weights_only= False)\n",
    "            model2 = other_state['model']\n",
    "            h2 = other_state['h']\n",
    "            label_set2 = state['label_set']\n",
    "\n",
    "            w_sim = self.model_similarity(model1, model2)\n",
    "            h_sim_df, h_sim = self.h_similarity(h1, h2, label_set, label_set2)\n",
    "            pair_wise_h_df.append(h_sim_df)\n",
    "\n",
    "            graph[i][j] = (self.cfg.alpha) * w_sim + (1-self.cfg.alpha) * h_sim\n",
    "            graph[i][j] = graph[i][j]\n",
    "\n",
    "            visited[(id, other_id)] = True\n",
    "            visited[(other_id, id)] = True\n",
    "\n",
    "    graph = self.sym_nromalization(graph)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(num_active):\n",
    "        for j in range(num_active):\n",
    "            if i != j:\n",
    "                edges.append((i, j, graph[i][j]))\n",
    "                \n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(edges)\n",
    "\n",
    "    for node, label in zip(list(range(num_active)), lst_active_ids):\n",
    "        G.nodes[node]['label'] = label\n",
    "    \n",
    "    df_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"h_sim_df_{str(comm_round)}.pth\")\n",
    "    if not os.path.exists(os.path.dirname(df_path)):\n",
    "        os.makedirs(os.path.dirname(df_path))\n",
    "    torch.save(pair_wise_h_df, df_path)\n",
    "\n",
    "    return G, graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a wighted graph, we nedd to form the coalitions (detecting the communities). We do so by using the louvain method for graph partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_coalitions(self: DMTL, G):\n",
    "    correct_clients_indices = nx.get_node_attributes(G, 'label')\n",
    "    partitions = community_louvain.best_partition(G)\n",
    "    communities = defaultdict(list)\n",
    "    for client, community in partitions.items():\n",
    "        communities[community].append(client)\n",
    "    communities = dict(communities)\n",
    "\n",
    "    for community, clients in communities.items():\n",
    "        communities[community] = [correct_clients_indices[client] for client in clients]\n",
    "\n",
    "    return communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [19, 16, 15, 5, 4], 1: [12, 14, 7, 3, 6]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "node_labels = {0: 19, 1: 16, 2: 15, 3: 5, 4: 4, 5: 12, 6: 14, 7: 7, 8: 3, 9: 6}\n",
    "communities = {0:[0, 1, 2, 3, 4], 1:[5, 6, 7, 8, 9]}\n",
    "\n",
    "for community, clients in communities.items():\n",
    "        communities[community] = [node_labels[client] for client in clients]\n",
    "communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We compute the shapely value whcih helps distribute the payoff among the clients in the same coalition. The computation is done as:\n",
    "$$\\zeta_k=\\sum_{S \\subseteq \\mathcal{C} \\backslash\\{i\\}} \\frac{|S|!(|\\mathcal{C}|-|S|-1)!}{|\\mathcal{C}|!}(v(S \\cup\\{k\\})-v(S))$$\n",
    "\n",
    "To ensure that the calculation is fast, we use a monte-carlo-based method to approximate those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_shapley_vals(self: DMTL):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation rule here is two-folds:\n",
    "- Representations are aggregated as:\n",
    "  $$ h_c = \\sum_{k \\in C_{j}} \\frac{\\zeta_k}{\\sum_{k \\in C_{j}} \\zeta_k}h_k$$\n",
    "  where $\\zeta$ is the shapely value.\n",
    "  \n",
    "The Classification heads are aggregated as:\n",
    "$$  w_k^{(t+1)} = w_{k, R}^{(t)} - \\lambda \\eta_2 \\sum_{\\ell \\in C_{j}} a_{k, \\ell} (w_{k,R}^{(t)} - w_{\\ell, R}^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: DMTL, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    self.graph, self.akl_connection = self.build_graph(lst_active_ids, comm_round)\n",
    "    graph_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"graph_{str(comm_round)}.gpickle\")\n",
    "    with open(graph_path, \"wb\") as f:\n",
    "        pickle.dump(self.graph, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    self.coalitions = self.get_coalitions(self.graph)\n",
    "    coalitions_path = os.path.join(self.cfg.save_dir, str(comm_round), \"coalitions.pth\")\n",
    "    torch.save(self.coalitions, coalitions_path)\n",
    "\n",
    "    global_lr = float(self.cfg.lr) * float(self.cfg.local_epochs)\n",
    "    reg_param = self.cfg.lambda_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        coalitions_reprs = {}\n",
    "        for col_ind, lst_clients in self.coalitions.items():\n",
    "\n",
    "            m_t = sum(len_clients_ds[id] for id in lst_clients)\n",
    "            for i, id in enumerate(lst_clients):\n",
    "                if not id in lst_active_ids:\n",
    "                    continue\n",
    "                state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "                state = torch.load(state_path, weights_only= False)\n",
    "                client_h = state['h']\n",
    "\n",
    "                if i == 0:\n",
    "                    col_repr = torch.zeros_like(client_h)\n",
    "\n",
    "                n_k = len_clients_ds[id]\n",
    "                weight =  n_k / m_t \n",
    "\n",
    "                col_repr.add_(weight * client_h)\n",
    "            coalitions_reprs[col_ind] = col_repr\n",
    "            \n",
    "\n",
    "        for col_ind, lst_clients in self.coalitions.items():\n",
    "            for i, id in enumerate(lst_clients):\n",
    "                if not id in lst_active_ids:\n",
    "                    continue\n",
    "                state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "                \n",
    "                state = torch.load(state_path, weights_only= False)\n",
    "                client_model = state['model']\n",
    "\n",
    "                client_diff = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_model.items() if key.startswith(\"fc2\") or key.startswith(\"dropout\")\n",
    "                }\n",
    "\n",
    "                for j, other_id in enumerate(lst_clients):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{other_id}\", \"state.pth\")\n",
    "                    \n",
    "                    other_state = torch.load(other_state_path, weights_only= False)\n",
    "                    other_client_model = other_state['model']\n",
    "\n",
    "                    a_kl = self.akl_connection[i, j]\n",
    "                    for key in client_model.keys():\n",
    "                        if key.startswith(\"fc2\") or key.startswith(\"dropout\"):\n",
    "                            client_diff[key].add_(a_kl * (client_model[key] - other_client_model[key]))\n",
    "\n",
    "                for key in client_model.keys():\n",
    "                    if key.startswith(\"fc2\") or key.startswith(\"dropout\"):\n",
    "                        client_model[key].sub_(global_lr * reg_param * client_diff[key])\n",
    "\n",
    "                clinet_state = {\n",
    "                    'model': client_model,\n",
    "                    'h': state['h'],\n",
    "                    'h_c': coalitions_reprs[col_ind],\n",
    "                }\n",
    "\n",
    "                agg_client_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"aggregated_model_{id}\", \"state.pth\")\n",
    "                \n",
    "                if not os.path.exists(os.path.dirname(agg_client_state_path)):\n",
    "                    os.makedirs(os.path.dirname(agg_client_state_path))\n",
    "\n",
    "                torch.save(clinet_state, agg_client_state_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to align the represntation of the coalition with the local represntation. This process is done as follows:\n",
    "\n",
    "$$\\min_{\\phi_i} \\left\\|\\bm{\\phi}_i(x)- \\bm{h}_c\\right\\|^2 ; \\hspace{0.3cm} \\forall x \\in D_k$$\n",
    "\n",
    "We prevent the coallpse of reprenstations as follows:\n",
    "\n",
    "$$\\bm{h}_c^{(i+1)} = \\beta \\bm{h}_c^{(i)} + (1-\\beta) \\bm{\\phi}_i(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def extra_computation(self: DMTL, lst_active_ids, comm_round):\n",
    "    \n",
    "    for id in lst_active_ids:\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, comm_round, self.loss_fn, to_read_from= 'aggregated_model_', extra= True)\n",
    "        \n",
    "        client.model.train()\n",
    "        for param in client.model.classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        client.model = client.model.to(client.device)\n",
    "        client.h_c = client.h_c.to(client.device)\n",
    "\n",
    "        optimizer = get_cls(\"torch.nn\", self.cfg.optimizer2)(client.model.encoder.parameters(), lr=self.cfg.lr2)\n",
    "        \n",
    "        client.train_loader = torch.utils.data.DataLoader(client.train_ds, batch_size=1, shuffle=True)\n",
    "        for i, batch in enumerate(client.train_loader):\n",
    "            batch = client.get_batch(batch)\n",
    "            X = batch['x']\n",
    "            optimizer.zero_grad()\n",
    "            h_prime = client.model.encoder(X)\n",
    "            loss = client.alignment_criterion()(h_prime, client.h_c)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                client.h_c.mul_(self.cfg.beta1).add_(h_prime, alpha=1 - self.cfg.beta1)\n",
    "\n",
    "        \n",
    "        state = {\n",
    "            'model': client.model.state_dict(),\n",
    "            'h_c': client.h_c,\n",
    "            'h': client.h\n",
    "        }\n",
    "\n",
    "        state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_aligned_{id}\", \"state.pth\")\n",
    "        if not os.path.exists(os.path.dirname(state_path)):\n",
    "            os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "        torch.save(state, state_path)\n",
    "\n",
    "        for param in client.model.classifier.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PeftAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 block,\n",
    "                 id,\n",
    "                 state= None,\n",
    "                 role= \"client\",\n",
    "                 **adapter_settings):\n",
    "        super().__init__(cfg, block, id, state, role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def peftify(self: PeftAgent):\n",
    "    # extract only the adapter's parameters from the model and store them in a dictionary\n",
    "    self.params_dict_old = deepcopy(\n",
    "        OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if\n",
    "                    \"default\" in name))\n",
    "    \n",
    "    self.params_dict_new = deepcopy(self.params_dict_old)\n",
    "    \n",
    "    self.model.state_dict = (\n",
    "        lambda instance, *_, **__: get_peft_model_state_dict(  # noqa: F405\n",
    "            instance, self.params_dict_new, \"default\"\n",
    "        )\n",
    "    ).__get__(self.model, type(self.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch \n",
    "def init_agent(self: PeftAgent):  # noqa: F811\n",
    "    self.peftify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state_(self: PeftAgent, epoch, local_dataset_len_dict, previously_selected_clients_set):  # noqa: F811\n",
    "    # save the new adapter weights to disk\n",
    "    self.save_state(epoch)\n",
    "\n",
    "    local_dataset_len_dict[self.id] = len(self.block)\n",
    "    older_adapter_weight = get_peft_model_state_dict(self.model, self.params_dict_old, \"default\")  # noqa: F405\n",
    "    set_peft_model_state_dict(self.model, older_adapter_weight, \"default\")  # noqa: F405\n",
    "    previously_selected_clients_set = previously_selected_clients_set | set({self.id})\n",
    "    last_client_id = self.id\n",
    "\n",
    "    return self.model, local_dataset_len_dict, previously_selected_clients_set, last_client_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fed-Sophia Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FedSophiaAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None):\n",
    "        super().__init__(id, cfg, state, role, block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train(self: FedSophiaAgent):\n",
    "    trainer = self.trainer(self) \n",
    "    client_history = trainer.fit() \n",
    "    return client_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PADG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PadgAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None):\n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "\n",
    "        if role == AgentRole.SERVER:\n",
    "            self.connections = torch.from_numpy(generate_graph(self.cfg.num_clients))  # noqa: F405\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def apply_constraints(self: PadgAgent, \n",
    "                      graph, # (np.ndarray): The input matrix.\n",
    "                      symmetrize=True, # (bool): If True, makes the matrix symmetric.\n",
    "                      normalize=True, # (bool): If True, normalizes the matrix symmetrically.\n",
    "                      threshold= 0, # (float or None): If provided, sets values below this threshold to 0.\n",
    "                      diag_fill= 0): # (float or None): If provided, fills the diagonal with this value.\n",
    "    \n",
    "\n",
    "    # Symmetrize the matrix\n",
    "    if symmetrize:\n",
    "        graph = (graph + graph.T) / 2\n",
    "\n",
    "    # Apply threshold to ensure non-negativity\n",
    "    if threshold is not None:\n",
    "        graph = torch.where(graph > threshold, graph, 0)\n",
    "\n",
    "    # Normalize the matrix symmetrically\n",
    "    if normalize:\n",
    "        row_sums = graph.sum(axis=1, keepdims=True)\n",
    "        col_sums = graph.sum(axis=0, keepdims=True)\n",
    "        norm_factor = torch.sqrt(row_sums @ col_sums)  # Symmetric normalization factor\n",
    "        graph = torch.divide(graph, norm_factor, where=norm_factor != 0)\n",
    "\n",
    "    # Fill the diagonal\n",
    "    if diag_fill is not None:\n",
    "        torch.fill_diagonal(graph, diag_fill)\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedai.models import *  # noqa: F403\n",
    "model = MLP(dim_in= 784, dim_hidden= 64, dim_out= 10)  # noqa: F405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "rand_in = torch.rand(1, 1, 28, 28)\n",
    "rand_in2 = torch.rand(1, 1, 28, 28)\n",
    "out1 = model(rand_in)\n",
    "out2 = model(rand_in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3843, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the KL divergence between the two outputs\n",
    "dist1 = F.softmax(out1, dim= -1)\n",
    "dist2 = F.softmax(out2, dim= -1)\n",
    "F.kl_div(dist1, dist2, reduction= 'batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def compute_probs(self: PadgAgent,\n",
    "                  batch_size=32, # batch_size (int): Batch size for evaluation.\n",
    "                  return_log_probs=True): # return_log_probs (bool): If True, return log-probabilities; otherwise, return probabilities.\n",
    "    \n",
    "    # Computes probabilities or log-probabilities across the entire dataset for a given model.\n",
    "    # Ensure model is in evaluation mode\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    self.model.to(device)\n",
    "    self.model.eval()\n",
    "    \n",
    "    # Create DataLoader for the dataset\n",
    "    dataloader = DataLoader(self.train_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_probs = []  # To store probabilities or log-probabilities for all batches\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in dataloader:  # Assuming dataset returns (inputs, labels)\n",
    "            inputs = batch['x'].to(device)  # Move to model's device\n",
    "            \n",
    "            logits = self.model(inputs)\n",
    "            \n",
    "            if return_log_probs:\n",
    "                # Convert logits to log-probabilities\n",
    "                batch_log_probs = F.log_softmax(logits, dim=-1)\n",
    "                all_probs.append(batch_log_probs)\n",
    "            else:\n",
    "                # Convert logits to probabilities\n",
    "                batch_probs = F.softmax(logits, dim=-1)\n",
    "                all_probs.append(batch_probs)\n",
    "    \n",
    "    self.model.to('cpu')\n",
    "    # Concatenate all batch probabilities/log-probabilities\n",
    "    return torch.cat(all_probs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(self: PadgAgent,\n",
    "                 ):\n",
    "    # compute the graident f R(w, A) w.r.t w\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: PadgAgent, lst_active_ids, comm_round, len_clients_ds, one_model= False):\n",
    "    \n",
    "    visited = []\n",
    "    for i, id in enumerate(lst_active_ids):\n",
    "\n",
    "        neighbour_ids = torch.where(self.connections[id] != float(0))[0]\n",
    "\n",
    "        model_path = os.path.join(self.cfg.save_dir, \n",
    "                                   str(comm_round),\n",
    "                                   f\"local_output_{id}\",\n",
    "                                   \"pytorch_model.pth\")\n",
    "        client_state_dict = torch.load(model_path, map_location='cpu', weights_only= False)\n",
    "        self.model.load_state_dict(client_state_dict)\n",
    "        \n",
    "        neighbours_sum = {\n",
    "            key: torch.zeros_like(value) \n",
    "            for key, value in client_state_dict.items()\n",
    "        }\n",
    "            \n",
    "        probs_1 = self.compute_probs(batch_size=32, return_log_probs=True)\n",
    "        \n",
    "        for other_id in neighbour_ids:\n",
    "\n",
    "            if (other_id, id) in visited:\n",
    "                continue\n",
    "            other_model_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{other_id}\",\n",
    "                                    \"pytorch_model.pth\")\n",
    "            \n",
    "            other_client_state_dict = torch.load(other_model_path, map_location='cpu', weights_only= False)\n",
    "            self.model.load_state_dict(other_client_state_dict)\n",
    "            \n",
    "            probs_2 = self.compute_probs(batch_size=32, return_log_probs=False)\n",
    "\n",
    "            kl_div = F.kl_div(probs_1, probs_2, reduction= 'batchmean').to('cpu')\n",
    "            self.connections[id][other_id] -= self.cfg.server_lr * self.cfg.lambda_ * kl_div\n",
    "\n",
    "            # apply constraints to the KL divergence\n",
    "            self.connections[id][other_id] = self.apply_constraints(self.connections[id][other_id])\n",
    "            self.connections[id][other_id] = self.connections[other_id][id]\n",
    "\n",
    "            visited.append((id, other_id))\n",
    "            visited.append((other_id, id))\n",
    "            \n",
    "        for other_id in neighbour_ids:\n",
    "            other_model_path = os.path.join(self.cfg.save_dir, \n",
    "                                    str(comm_round),\n",
    "                                    f\"local_output_{other_id}\",\n",
    "                                    \"pytorch_model.pth\")\n",
    "            other_client_state_dict = torch.load(other_model_path, map_location='cpu', weights_only= False)\n",
    "\n",
    "            weight = self.connections[id][other_id]\n",
    "            for key in other_client_state_dict.keys():\n",
    "                neighbours_sum[key].data += weight * other_client_state_dict[key].data\n",
    "\n",
    "        # for key in neighbours_sum.keys():\n",
    "            # neighbours_sum[key].data /= len(neighbour_ids)\n",
    "\n",
    "        for key in client_state_dict.keys():\n",
    "            client_state_dict[key].data = self.cfg.beta * client_state_dict[key].data + (1 - self.cfg.beta) * neighbours_sum[key].data\n",
    "\n",
    "    \n",
    "        # save the updated model to the disk\n",
    "        self.save_state(client_state_dict, comm_round + 1, id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira clients have more parameters. Since it's a client for LLM in principle, we need to feed the generation dataset (the dataset of text ids at the end layer not the logits). Also, a tokenizer and a collate function that will be used for the generation and the data loader construction processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentMira(FLAgent):\n",
    "    def __init__(self,\n",
    "                 data_dict: dict,\n",
    "                 model: torch.nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 id: int,\n",
    "                 gen_data_dict: dict,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 collat_fn: LLMDataCollator,\n",
    "                 cfg: DictConfig) -> None:\n",
    "            \n",
    "        super().__init__(data_dict, model, criterion, optimizer, id)\n",
    "        \n",
    "        self.train_ds_genr = gen_data_dict['train']\n",
    "        self.test_ds_genr = gen_data_dict['test']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collat_fn = collat_fn\n",
    "        self.cfg = cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to save space, we will replace the original model with only the trainable peft model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mira Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following:\n",
    "- Define a Mira client.\n",
    "- inspect the `init_local_train` and `terminate_local_train` methods and their effect on the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# base_model = deepcopy(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# config = LoraConfig(\n",
    "#     r=8,# arbitrary numbr but usually 8, 16, 32, 64, 128\n",
    "#     target_modules=['c_attn'],\n",
    "#     lora_alpha=8,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "\n",
    "# peft_model = get_peft_model(gpt2, config)\n",
    "# mira  = AgentMira(DataDict, peft_model, criterion, optimizer, 0, train_dataset, test_dataset, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inpect the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
