{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "> The core abstraction for different FL Agents/Clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp federated.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "from fastcore.all import *\n",
    "import os\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "from collections import defaultdict,OrderedDict\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import *\n",
    "from community import community_louvain\n",
    "from fedai.utils import *\n",
    "from fedai.client_selector import *\n",
    "from fedai.optimizers import *\n",
    "from fedai.data.core import LLMDataCollator\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from fedai.utils import *\n",
    "from fedai.metrics import *\n",
    "from fedai.losses import *\n",
    "from transformers import AutoTokenizer\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "import numpy as np\n",
    "import math\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentRole(Enum):\n",
    "    SERVER = 1\n",
    "    CLIENT = 2\n",
    "    MARL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Agent\n",
    "\n",
    "An agent is an entity that has a state and exist in an environment. In the case of Federated learning (FL), the agent's state is defined as its own model, data, criterion, optimizer. FL Focuses on distributed model training across multiple clients (agents), each with its local data. Clients **collaborate** to improve a global or shared model while keeping their data private. Communication is often periodic (e.g., every few training rounds). On the other hand, Multi-agent RL systems (MARL) Involves multiple agents interacting with an environment to learn policies for specific tasks (e.g., navigation, resource allocation). Each agent has a state also, but the state represntation might differ slightly from that of an FL agent. The data is often not preloaded as in FL rather, it's collected from the environemnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT):\n",
    "        \n",
    "        self.id = id # each agent has a unique id\n",
    "        self.cfg = cfg # contains all the configurations needed for the agent/trainer.\n",
    "        self.state = state # A dictionary containing the state of the agent\n",
    "        self.role = role # either a client or a server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def init_agent(self: Agent):\n",
    "    # Initialize the state of the agent. In FL Agent, this means making any adjustments to the model/optimizer/state_dict/...etc\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, msg):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update_state(self: Agent):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: Agent):\n",
    "    # save the state of the agent to a file on disk (id, model, optimizer, loss_fn).\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: Agent):\n",
    "    self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Agent (FedAVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FLAgent(Agent):\n",
    "    # A Federated Learning Agent implementing `FedAVG`.\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None # The data block (local data of the FL Agent).\n",
    "                 ):  \n",
    "                 \n",
    "        super().__init__(id, cfg, state, role)\n",
    "\n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.train_ds, self.test_ds = block[0], block[1]\n",
    "            \n",
    "            for key, value in self.state.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "            self.train_loader = prepare_dl(self.cfg, self.train_ds)  # noqa: F405\n",
    "            self.test_loader = prepare_dl(self.cfg, self.test_ds) # noqa: F405\n",
    "\n",
    "            self.training_metrics = Metrics(list(self.cfg.training_metrics))  # noqa: F405\n",
    "            self.test_metrics = Metrics(list(self.cfg.test_metrics))  # noqa: F405\n",
    "\n",
    "            self.data_key, self.label_key = 'x', 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def server_init(self: FLAgent, client_fn, client_selector, client_cls, loss_fn, writer):\n",
    "    self.client_fn = client_fn\n",
    "    self.client_selector = client_selector\n",
    "    self.client_cls = client_cls\n",
    "    self.loss_fn = loss_fn\n",
    "    self.writer = writer\n",
    "    self.latest_round = {}\n",
    "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data blocks are already on the disk, and since RL agents don't have a preloaded data blocks, we don't include the data in the FL agent's state. Another ratioanle behind this decision is that, state should contain dynamic objects that change over the interaction of the agents and data blocks are static in the case of FL agents, unless you are doing FL-RL Agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every client abstraction, whether it a base or any other type of federated client, it will initalize the training locally with a set of steps. This might include things like extracting the peft model out of the base model (in the case of LLMs clients). Also, it will terminate the local training with some steps, like saving the model state dictionary and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def runFL(self: FLAgent):\n",
    "    res =  []\n",
    "    all_ids = self.client_selector.select()\n",
    "    \n",
    "    for t in range(1, self.cfg.n_rounds + 1):\n",
    "        lst_active_ids = all_ids[t-1]\n",
    "        len_clients_ds = {}\n",
    "        \n",
    "        for id in lst_active_ids:\n",
    "            client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)\n",
    "            len_clients_ds[id] = len(client.train_ds)\n",
    "            \n",
    "            self.communicate(client) \n",
    "            client.fit()\n",
    "\n",
    "            client.communicate(self) \n",
    "            self.latest_round[id] = t \n",
    "\n",
    "        self.aggregate(lst_active_ids, t, len_clients_ds)\n",
    "        \n",
    "        train_res, test_res = self.evaluate(t)\n",
    "        train_df, test_df = self.writer.write(lst_active_ids, train_res, test_res, t) \n",
    "        res.append((train_df, test_df))\n",
    "        \n",
    "    self.writer.save(res)\n",
    "    self.writer.finish()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adjust the string reprsntation of the client abstraction to make it more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self: FLAgent) -> str:\n",
    "    return f'''{self.__class__.__name__}: {self.__class__.__name__}\n",
    "    Index : {self.id}\n",
    "    Model: {self.model.__class__.__name__}\n",
    "    Criterion: {self.criterion.__class__.__name__}\n",
    "    Optimizer: {self.optimizer.__class__.__name__}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: FLAgent):\n",
    "    self.model = None if hasattr(self, 'model') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update_parameters(self:FLAgent, new_params):\n",
    "    with torch.no_grad():\n",
    "        for param , new_param in zip(self.model.parameters(), new_params):\n",
    "            param.copy_(new_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_batch(self: FLAgent, batch):\n",
    "    return {k: v.to(self.device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: FLAgent, batch):\n",
    "    X, y = batch['x'], batch['y']\n",
    "    outputs = self.model(X)\n",
    "    loss = self.criterion(outputs, y)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: FLAgent, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "\n",
    "    try:\n",
    "        loss, logits = self._forward(batch)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in _closure: {e}\")\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: FLAgent, batch: dict) -> tuple:\n",
    "    self.optimizer.zero_grad()\n",
    "    loss, metrics = self._closure(batch)\n",
    "\n",
    "    if loss.item() == 0.0:\n",
    "        return loss, metrics\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: FLAgent):\n",
    "\n",
    "    for i, batch in enumerate(self.train_loader):\n",
    "        batch = self.get_batch(batch)\n",
    "        self._run_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def fit(self: FLAgent) -> dict:\n",
    "    \n",
    "    self.model = self.model.to(self.device)\n",
    "    self.model.train()\n",
    "    for _ in range(self.cfg.local_epochs):\n",
    "        self._run_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train_test_stats(self: FLAgent, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "\n",
    "    try:\n",
    "        X, y = batch['x'], batch['y']\n",
    "        logits = self.model(X)\n",
    "        loss = self.criterion(logits, y)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate_local(self: FLAgent, loader= 'train') -> dict:\n",
    "    total_loss = 0\n",
    "    lst_metrics = []\n",
    "\n",
    "    self.model = self.model.to(self.device)\n",
    "    self.model.eval()\n",
    "    num_eval = 0\n",
    "    data_loader = self.train_loader if loader == 'train' else self.test_loader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            batch = self.get_batch(batch)\n",
    "            loss, metrics = self.train_test_stats(batch)                 \n",
    "            if not math.isnan(loss.item()):\n",
    "                total_loss += loss.item()  \n",
    "                num_eval += len(batch[self.data_key])  # Ensure num_eval is updated\n",
    "                lst_metrics.append(metrics)           \n",
    "    \n",
    "    avg_loss = total_loss / num_eval if num_eval > 0 else 0.0\n",
    "    logger.info(f\"Average {loader} Loss is : {avg_loss}\")\n",
    "    \n",
    "    if lst_metrics:\n",
    "        total_metrics = {k: sum(m.get(k, 0) for m in lst_metrics) / len(lst_metrics) for k in self.cfg.test_metrics}\n",
    "    else:\n",
    "        total_metrics = {k: 0.0 for k in self.cfg.test_metrics}\n",
    "\n",
    "    return {\"loss\": avg_loss, \"metrics\": total_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate(self: FLAgent, t):\n",
    "    lst_train_res = []\n",
    "    lst_test_res = []\n",
    "    for id in range(self.cfg.num_clients):\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)\n",
    "        \n",
    "        res_train = client.evaluate_local(loader= 'train')\n",
    "        lst_train_res.append(res_train)\n",
    "\n",
    "        res_test = client.evaluate_local(loader= 'test')\n",
    "        lst_test_res.append(res_test)\n",
    "    return lst_train_res, lst_test_res    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To do: implement the communication process in **Protobuf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication\n",
    "\n",
    "Communication refers to the process of downloading and uploading models from the server and to the client. Since we are safeguarding against memory issues, we use sequential client processing and disk checkpointing as our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: FLAgent, state_dict):  # noqa: F811\n",
    "    # save the model to self.cfg.save_dir/comm_round/f\"local_output_{id}\"/state.pth\n",
    "    \n",
    "    state_path = os.path.join(self.cfg.save_dir, str(self.t), f\"local_output_{self.id}\", \"state.pth\")\n",
    "    if not os.path.exists(os.path.dirname(state_path)):\n",
    "        os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "    state_dict['model'] = self.model.state_dict()\n",
    "    state_dict['optimizer'] = self.optimizer.state_dict()\n",
    "\n",
    "    torch.save(state_dict, state_path)\n",
    "\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        save_space(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, another_agent: Agent):  # noqa: F811\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        self.save_state(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggegation for `fedavg` is defined as:\n",
    "\n",
    "$$m_t \\leftarrow \\sum_{k \\in S_t} n_k$$\n",
    "$$W_{global}^{(t + 1)} \\leftarrow \\sum_{k \\in S_t} \\frac{n_k}{m_t} w_k^{(t + 1)}$$\n",
    "\n",
    "where $S_t$ is the set of active clients at communication round $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: FLAgent, lst_active_ids, comm_round, len_clients_ds):\n",
    "        \n",
    "    m_t = sum(len_clients_ds.values())\n",
    "    with torch.no_grad():\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "            \n",
    "            state = torch.load(state_path, weights_only=False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            if i == 0:\n",
    "                global_model = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_state_dict.items()\n",
    "                }\n",
    "\n",
    "            n_k = len_clients_ds[id]\n",
    "            weight =  n_k / m_t \n",
    "\n",
    "        \n",
    "            for key in client_state_dict.keys():\n",
    "                global_model[key].add_(weight * client_state_dict[key])\n",
    "\n",
    "\n",
    "        server_state = {\n",
    "            'model': global_model,\n",
    "        }\n",
    "\n",
    "        server_state_path = os.path.join(self.cfg.save_dir, str(comm_round), \"global_model\", \"state.pth\")\n",
    "        \n",
    "        if not os.path.exists(os.path.dirname(server_state_path)):\n",
    "            os.makedirs(os.path.dirname(server_state_path), exist_ok=True)\n",
    "            \n",
    "        torch.save(server_state, server_state_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fedu Agent (MTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This client uses **Multi-Task-Learning** scheme to personalize the models in a non-iid setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Fedu(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "\n",
    "        b = np.random.uniform(0,1,size=(self.cfg.num_clients, self.cfg.num_clients))\n",
    "        b_symm = (b + b.T)/2\n",
    "        b_symm[b_symm < 0.25] = 0\n",
    "        self.alk_connection = b_symm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Algorithm learns a model per client, and add a new aggregation scheme defined as\n",
    "\n",
    "$$ W_{k}^{(t)} = W_{k, R}^{(t)} - \\lambda \\eta_2 \\sum_{\\ell \\in N_k} a_{k, \\ell} (W_{k, R}^{(t)} - W_{\\ell, R}^{(t)})$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter and $\\eta_2$ is defined as $$\\eta_2 = \\eta_1 * R $$\n",
    "\n",
    "such that $\\eta_1$ is the local learning rate, and $R$ is the number of lcoal iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: Fedu, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    global_lr = float(self.cfg.optimizer.lr) * float(self.cfg.local_epochs)\n",
    "    reg_param = self.cfg.lambda_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "            \n",
    "            state = torch.load(state_path, weights_only= False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            client_diff = {\n",
    "                key: torch.zeros_like(value) \n",
    "                for key, value in client_state_dict.items()\n",
    "            }\n",
    "            \n",
    "            for j, other_id in enumerate(lst_active_ids):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{other_id}\", \"state.pth\")\n",
    "                \n",
    "                other_state = torch.load(other_state_path, weights_only= False)\n",
    "                other_state_dict = other_state['model']\n",
    "\n",
    "                weight = self.alk_connection[int(id)][int(other_id)]\n",
    "                for key in client_state_dict.keys():\n",
    "                    client_diff[key].add_(weight * (client_state_dict[key] - other_state_dict[key]))\n",
    "\n",
    "            for key in client_state_dict:\n",
    "                client_state_dict[key].sub_(global_lr * reg_param * client_diff[key])\n",
    "\n",
    "            clinet_state = {\n",
    "                'model': client_state_dict,\n",
    "            }\n",
    "\n",
    "            agg_client_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"aggregated_model_{id}\", \"state.pth\")\n",
    "            \n",
    "            if not os.path.exists(os.path.dirname(agg_client_state_path)):\n",
    "                os.makedirs(os.path.dirname(agg_client_state_path))\n",
    "\n",
    "            torch.save(clinet_state, agg_client_state_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMTL-Graph (MTL)\n",
    "Distributed Multi-Task Learning over Graphs: A game-theoretic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DMTL(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "        \n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.anchorloss = AnchorLoss(self.cfg.random_seed, self.cfg.data.num_classes, self.cfg.model.hidden_dim, self.t, self.h_c).to(self.device)\n",
    "            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def server_init(self: DMTL, client_fn, client_selector, client_cls, loss_fn, writer):\n",
    "    FLAgent.server_init(self, client_fn, client_selector, client_cls, loss_fn, writer)\n",
    "    self.classes = self.cfg.data.classes\n",
    "    self.idx_to_cls = {i: self.classes[i] for i in range(len(self.classes))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: DMTL, batch):\n",
    "    X, y = batch['x'], batch['y']\n",
    "    y_copied = deepcopy(y)\n",
    "    labels = y.type(torch.LongTensor).to(self.device)\n",
    "    ys = labels.float()\n",
    "    \n",
    "    h = self.model.encoder(X)\n",
    "    outputs = self.model.classifier(h)    \n",
    "    \n",
    "    loss_anchor = self.anchorloss(h, ys, Lambda = self.cfg.lambda_anchor)\n",
    "    loss = self.criterion(outputs, y_copied)\n",
    "    \n",
    "    return loss + loss_anchor, h, outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: DMTL, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "    h, labels = None, None\n",
    "    try:\n",
    "        loss, h, logits, labels = self._forward(batch)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics, h, labels  # Return safe values\n",
    "\n",
    "    return loss, metrics, h, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: DMTL, batch: dict) -> tuple:\n",
    "    batch_mean_anchor = torch.zeros(self.cfg.data.num_classes, self.cfg.model.hidden_dim).to(self.device)\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "    loss, metrics, h, labels = self._closure(batch)\n",
    "    if loss.item() == 0.0 or h is None:\n",
    "        return loss, metrics, batch_mean_anchor\n",
    "    \n",
    "    for i in set(labels.tolist()):\n",
    "        batch_mean_anchor[i] += torch.mean(h[labels==i],dim=0)\n",
    "   \n",
    "    loss.backward()\n",
    "    \n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return loss, metrics, batch_mean_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: DMTL):\n",
    "\n",
    "    epoch_mean_anchor = torch.zeros(self.cfg.data.num_classes, self.cfg.model.hidden_dim).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        epoch_mean_anchor.copy_(self.anchorloss.anchor)\n",
    "\n",
    "    for batch_idx, batch in enumerate(self.train_loader):\n",
    "        batch = self.get_batch(batch)\n",
    "        _, _, batch_mean_anchor = self._run_batch(batch)\n",
    "\n",
    "        for i in self.label_set:\n",
    "            #compute batch mean anchor according to batch label\n",
    "            batch_mean_anchor[i] = batch_mean_anchor[i]/(batch_idx+1)\n",
    "\n",
    "            #compute epoch mean anchor according to batch mean anchor\n",
    "            lambda_momentum = self.cfg.momentum_anchor #pow(2, -(epoch+1))\n",
    "            # epoch_mean_anchor[i] = lambda_momentum * epoch_mean_anchor[i] + (1-lambda_momentum)*batch_mean_anchor[i]\n",
    "            epoch_mean_anchor[i].mul_(lambda_momentum).add_((1 - lambda_momentum) * batch_mean_anchor[i])\n",
    "\n",
    "        \n",
    "\n",
    "    # self.anchorloss.anchor =  torch.nn.Parameter(epoch_mean_anchor, requires_grad=False)\n",
    "    with torch.no_grad():\n",
    "        self.anchorloss.anchor.copy_(epoch_mean_anchor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The communication in DMTL is a matter of sending (saving to the disk) two things, the classification head and a randomly picked data represntation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: DMTL, state_dict):  # noqa: F811\n",
    "    # save the model to self.cfg.save_dir/comm_round/f\"local_output_{id}\"/state.pth\n",
    "    \n",
    "    \n",
    "    state_dict['model'] = self.model.state_dict()\n",
    "    state_dict['optimizer'] = self.optimizer.state_dict()\n",
    "    state_dict['h'] = self.anchorloss.anchor.detach().clone() # (num_classes, hidden_size)\n",
    "    state_dict['label_set'] = self.label_set\n",
    "    \n",
    "    # pick a random data point from the train_ds and save it to the state_dict\n",
    "    # data_point = self.train_ds[np.random.randint(0, len(self.train_ds))]\n",
    "    # data_point = self.get_batch(data_point)\n",
    "    # data = data_point['x']\n",
    "    # batched_data_point = data.view(1, 3, 32, 32) # (B, C, H, W)\n",
    "    # state_dict['h'] = self.model.encoder(batched_data_point)\n",
    "\n",
    "    state_path = os.path.join(self.cfg.save_dir, str(self.t), f\"local_output_{self.id}\", \"state.pth\")\n",
    "    if not os.path.exists(os.path.dirname(state_path)):\n",
    "        os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "    torch.save(state_dict, state_path)\n",
    "\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        save_space(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the server, we are doing the followin steps:\n",
    "- Form the coalitions.\n",
    "  - First, construct the weighted undirected graph $\\mathcal{g}$.\n",
    "  - Second, pass this graph to the louvian algorithm to get the communities.\n",
    "- aggregate based on the equations stated in the next cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, model similrity is compared using the norm of the difference between the two models as follows:\n",
    "$$\\operatorname{Sim}_{\\text {head }}(\\bm{w}_k,\\bm{w}_\\ell)=\\left\\|\\bm{w}_k-\\bm{w}_\\ell\\right\\|,$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def model_similarity(self: DMTL, h1, h2, model1, model2):\n",
    "\n",
    "    model_cls = get_cls(\"fedai.vision.models\", self.cfg.model.name)\n",
    "\n",
    "    m1 = model_cls().classifier\n",
    "    with torch.no_grad():\n",
    "        for k, v in model1.items():\n",
    "            if k in m1.state_dict():\n",
    "                print(\"Copying weights from model1 to m1\")\n",
    "                m1.state_dict()[k].copy_(v)\n",
    "            else:\n",
    "                print(f\"Key {k} not found in model1\")\n",
    "    \n",
    "    m2 = model_cls().classifier\n",
    "    m2.load_state_dict(model2)\n",
    "\n",
    "    m1.to(self.device)\n",
    "    m2.to(self.device)\n",
    "    \n",
    "    avg_sim = 0.0\n",
    "    for h in [h1, h2]:\n",
    "        h = h.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out1 = m1(h)\n",
    "            out2 = m2(h)\n",
    "            sim = F.cosine_similarity(out1, out2, dim=1).mean()\n",
    "            avg_sim += sim.item()\n",
    "\n",
    "    return avg_sim / 2  # correct normalization\n",
    "\n",
    "    # total_l1_norm = 0.0\n",
    "    # total_params = 0\n",
    "\n",
    "    # for key in model1.keys():\n",
    "    #     if key in model2:\n",
    "    #         param1 = model1[key]\n",
    "    #         param2 = model2[key]\n",
    "\n",
    "    #         if param1.shape == param2.shape:\n",
    "    #             diff = param1 - param2\n",
    "    #             total_l1_norm += torch.norm(diff, p=1).item()\n",
    "    #             total_params += diff.numel()\n",
    "    #         else:\n",
    "    #             print(f\"Shape mismatch at {key}: {param1.shape} vs {param2.shape}\")\n",
    "    #     else:\n",
    "    #         print(f\"{key} not found in model2\")\n",
    "\n",
    "    # if total_params == 0:\n",
    "    #     return float('inf')  # or raise an error\n",
    "\n",
    "    # return total_l1_norm / total_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the embedding closeness using cosine similiairy.\n",
    "\n",
    "\n",
    "$$\\operatorname{Sim}_{\\mathrm{repr}}(k, \\ell) = \\cos(\\Omega) = \\frac{\\mathbf{h}_k \\cdot \\mathbf{h}_{\\ell}}{\\|\\mathbf{h}_k\\| \\|\\mathbf{h}_{\\ell}\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def h_similarity(self: DMTL, h1, h2, label_set, label_set2):\n",
    "    h1 = h1.reshape(self.cfg.data.num_classes, self.cfg.model.hidden_dim)\n",
    "    h2 = h2.reshape(self.cfg.data.num_classes, self.cfg.model.hidden_dim)\n",
    "    \n",
    "    h1_norm = F.normalize(h1, p=2, dim=1)  # (3, 512)\n",
    "    h2_norm = F.normalize(h2, p=2, dim=1)  # (3, 512)\n",
    "    \n",
    "    cos_sim_matrix = torch.mm(h1_norm, h2_norm.T)  # (10, 10)\n",
    "    max_similarity = cos_sim_matrix.max(dim=1).values.mean()# This gives a higher similarity score if each class in h1 has at least one good match in h2.\n",
    "    data = cos_sim_matrix.cpu().numpy()\n",
    "\n",
    "    # index only data for the label_set\n",
    "    data = data[label_set][:, label_set2]\n",
    "\n",
    "    cols1 = [self.idx_to_cls[i] for i in label_set]\n",
    "    cols2 = [self.idx_to_cls[i] for i in label_set2]\n",
    "\n",
    "    df = pd.DataFrame(data, columns= cols1, index= cols2)\n",
    "    return df, max_similarity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph laplacian matrix  is a symmetric matrix. Also, the weights(value of the matrix) must be normalized so that they sum to $1$, or at least close to $1$. We can use an approximate method to ensure that those criteria are met. The resultant matrix is called a Doubly stochastic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def sym_nromalization(self: DMTL, A):\n",
    "    \"normalize the adjacency matrix while ensuring symmetry\"\n",
    "\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = (A + A.T) / 2  # Ensure symmetry\n",
    "    # Compute the degree matrix (row sums)\n",
    "    row_sums = A.sum(axis=1)\n",
    "    # Avoid division by zero\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    # Compute D^(-1/2)\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(row_sums))\n",
    "    # Symmetric normalization\n",
    "    A_normalized = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "\n",
    "    return A_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a similarity graph using both the local represntation $h$ and the classification head $w$ where similairty is defined as:\n",
    "$$\\operatorname{Sim} = \\sum_{i \\in S} \\alpha \\cdot \\operatorname{Sim}_{\\text {head }}(C_j)+(1-\\alpha) \\cdot \\operatorname{Sim}_{\\mathrm{repr}}(C_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64680106, 0.02142615, 0.33177279],\n",
       "       [0.36518544, 0.3934235 , 0.24139106],\n",
       "       [0.1084604 , 0.35492474, 0.53661487]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "graph = np.random.rand(3, 3)\n",
    "graph = graph / graph.sum(axis=1)[:, None]\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def build_graph(self: DMTL, lst_active_ids, comm_round):\n",
    "\n",
    "    num_active = len(lst_active_ids)\n",
    "    graph = np.zeros((num_active, num_active))\n",
    "    # graph = graph / graph.sum(axis=1)[:, None]\n",
    "\n",
    "    clients_sim_dict = {}\n",
    "    visited = {}\n",
    "    for i, id in enumerate(lst_active_ids):\n",
    "        state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "        state = torch.load(state_path, weights_only= False)\n",
    "        model1 = state['model']\n",
    "        h1 = state['h']\n",
    "        label_set = state['label_set']\n",
    "\n",
    "        for j, other_id in enumerate(lst_active_ids):\n",
    "            if i == j or (id, other_id) in visited:\n",
    "                continue\n",
    "            other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{other_id}\", \"state.pth\")\n",
    "            other_state = torch.load(other_state_path, weights_only= False)\n",
    "            model2 = other_state['model']\n",
    "            h2 = other_state['h']\n",
    "            label_set2 = other_state['label_set']\n",
    "\n",
    "            w_sim = self.model_similarity(h1, h2, model1, model2)\n",
    "            h_sim_df, h_sim = self.h_similarity(h1, h2, label_set, label_set2)\n",
    "            clients_sim_dict[(id, other_id)] = h_sim_df\n",
    "\n",
    "            # graph[i][j] = - (self.cfg.alpha) * w_sim + (1-self.cfg.alpha) * h_sim\n",
    "            # graph[j][i] = round(graph[i][j], ndigits=3)\n",
    "            # graph[i][j] = graph[j][i]\n",
    "            val = - (self.cfg.alpha) * w_sim + (1 - self.cfg.alpha) * h_sim\n",
    "            print(f\"Client {id} and {other_id} similarity: {val}\")\n",
    "            print(f\"Client {id} and {other_id} h similarity: {h_sim}\")\n",
    "            print(f\"Client {id} and {other_id} w similarity: {w_sim}\")\n",
    "            val = round(val, 3)\n",
    "            val = max(val, 0)  # prevent negative edge weights\n",
    "            graph[i][j] = graph[j][i] = val\n",
    "\n",
    "            visited[(id, other_id)] = True\n",
    "            visited[(other_id, id)] = True\n",
    "\n",
    "    print(\"Before sym:\", graph)\n",
    "    row_sums = graph.sum(axis=1, keepdims=True)\n",
    "    graph = graph / row_sums  \n",
    "    print(\"After sym:\", graph)\n",
    "\n",
    "\n",
    "    edges = []\n",
    "    for i in range(num_active):\n",
    "        for j in range(num_active):\n",
    "            if i != j:\n",
    "                edges.append((i, j, graph[i][j]))\n",
    "                \n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(edges)\n",
    "\n",
    "    for node, label in zip(list(range(num_active)), lst_active_ids):\n",
    "        G.nodes[node]['label'] = label\n",
    "    \n",
    "    df_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"h_sim_df_{str(comm_round)}.pth\")\n",
    "    if not os.path.exists(os.path.dirname(df_path)):\n",
    "        os.makedirs(os.path.dirname(df_path))\n",
    "    torch.save(clients_sim_dict, df_path)\n",
    "\n",
    "    return G, graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a wighted graph, we nedd to form the coalitions (detecting the communities). We do so by using the louvain method for graph partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_coalitions(self: DMTL, G):\n",
    "    correct_clients_indices = nx.get_node_attributes(G, 'label')\n",
    "    partitions = community_louvain.best_partition(G)\n",
    "    communities = defaultdict(list)\n",
    "    for client, community in partitions.items():\n",
    "        communities[community].append(client)\n",
    "    communities = dict(communities)\n",
    "\n",
    "    for community, clients in communities.items():\n",
    "        communities[community] = [correct_clients_indices[client] for client in clients]\n",
    "\n",
    "    return communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [19, 16, 15, 5, 4], 1: [12, 14, 7, 3, 6]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "node_labels = {0: 19, 1: 16, 2: 15, 3: 5, 4: 4, 5: 12, 6: 14, 7: 7, 8: 3, 9: 6}\n",
    "communities = {0:[0, 1, 2, 3, 4], 1:[5, 6, 7, 8, 9]}\n",
    "\n",
    "for community, clients in communities.items():\n",
    "        communities[community] = [node_labels[client] for client in clients]\n",
    "communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapely Value computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We compute the shapely value whcih helps distribute the payoff among the clients in the same coalition. The computation is done as:\n",
    "$$\\zeta_k=\\sum_{S \\subseteq \\mathcal{C} \\backslash\\{i\\}} \\frac{|S|!(|\\mathcal{C}|-|S|-1)!}{|\\mathcal{C}|!}(v(S \\cup\\{k\\})-v(S))$$\n",
    "\n",
    "To ensure that the calculation is fast, we use a monte-carlo-based method to approximate those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def modularity_value(coalition):\n",
    "#     \"\"\" Compute modularity considering the coalition as part of a full partition. \"\"\"\n",
    "#     if not coalition:\n",
    "#         return 0  # Empty coalition has no modularity\n",
    "\n",
    "#     # Construct the partition\n",
    "#     partition = [list(coalition)]  # Coalition as a group\n",
    "#     remaining_nodes = set(G.nodes) - set(coalition)\n",
    "    \n",
    "#     if remaining_nodes:\n",
    "#         partition.append(list(remaining_nodes))  # Add remaining nodes as another community\n",
    "\n",
    "#     return nx.algorithms.community.quality.modularity(G, partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# # @patch\n",
    "# def get_shapley_vals(G, players, num_samples=1000):\n",
    "#     \"\"\" Approximate Shapley values using Monte Carlo for modularity. \"\"\"\n",
    "\n",
    "#     def modularity_value(coalition):\n",
    "#         \"\"\" Compute modularity considering the coalition as part of a full partition. \"\"\"\n",
    "#         if not coalition:\n",
    "#             return 0  # Empty coalition has no modularity\n",
    "\n",
    "#         # Construct the partition\n",
    "#         partition = [list(coalition)]  # Coalition as a group\n",
    "#         remaining_nodes = set(G.nodes) - set(coalition)\n",
    "        \n",
    "#         if remaining_nodes:\n",
    "#             partition.append(list(remaining_nodes))  # Add remaining nodes as another community\n",
    "\n",
    "#         return nx.algorithms.community.quality.modularity(G, partition)\n",
    "\n",
    "#     n = len(players)\n",
    "#     shap_values = {player: 0 for player in players}\n",
    "    \n",
    "#     for _ in range(num_samples):\n",
    "#         perm = list(players)\n",
    "#         random.shuffle(perm)  # Random order\n",
    "#         current_S = set()\n",
    "#         for i in perm:\n",
    "#             v_S = modularity_value(tuple(current_S))\n",
    "#             current_S.add(i)\n",
    "#             v_S_i = modularity_value(tuple(current_S))\n",
    "#             shap_values[i] += (v_S_i - v_S)\n",
    "    \n",
    "#     # Normalize\n",
    "#     for i in shap_values:\n",
    "#         shap_values[i] /= num_samples\n",
    "#     return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Shapley Values: {1: 0.00883928571428572, 2: -0.008420918367346937, 3: 0.00037244897959183457, 4: -0.0007908163265306147}\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# G = nx.Graph()\n",
    "# edges = [(1, 2, 0.8), (1, 3, 0.6), (2, 3, 0.9), (3, 4, 0.5)]\n",
    "# G.add_weighted_edges_from(edges)\n",
    "\n",
    "# # Compute modularity-based coalition structure using Louvain\n",
    "# partition = community.best_partition(G, weight='weight')\n",
    "\n",
    "# players = tuple(G.nodes)\n",
    "\n",
    "# shap_values_approx = get_shapley_vals(G, players, num_samples=1000)\n",
    "# print(\"Approximate Shapley Values:\", shap_values_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to make use of the shapely values is to compute them not just based on similairty but the contribution of each client to the overall colaition.\n",
    "\n",
    "$$a_{k, \\ell}=\\frac{\\zeta_k}{\\sum_{j \\in C_i} \\zeta_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3544696551430991.5,\n",
       " 2: -3376924478001065.5,\n",
       " 3: 149358065370540.1,\n",
       " 4: -317130138800464.6}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# #  normalize the shapley values\n",
    "# shap_values = {k: v / sum(shap_values_approx.values()) for k, v in shap_values_approx.items()}\n",
    "# shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def compute_weighted_akl(self: DMTL, graph):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation rule here is two-folds:\n",
    "- Representations are aggregated as:\n",
    "  $$ h_c = \\sum_{k \\in C_{j}} \\frac{\\zeta_k}{\\sum_{k \\in C_{j}} \\zeta_k}h_k$$\n",
    "  where $\\zeta$ is the shapely value.\n",
    "  \n",
    "The Classification heads are aggregated as:\n",
    "$$  w_k^{(t+1)} = w_{k, R}^{(t)} - \\lambda \\eta_2 \\sum_{\\ell \\in C_{j}} a_{k, \\ell} (w_{k,R}^{(t)} - w_{\\ell, R}^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: DMTL, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    self.graph, self.akl_connection = self.build_graph(lst_active_ids, comm_round)\n",
    "    graph_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"graph_{str(comm_round)}.gpickle\")\n",
    "    with open(graph_path, \"wb\") as f:\n",
    "        pickle.dump(self.graph, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    self.coalitions = self.get_coalitions(self.graph)\n",
    "    coalitions_path = os.path.join(self.cfg.save_dir, str(comm_round), \"coalitions.pth\")\n",
    "    torch.save(self.coalitions, coalitions_path)\n",
    "\n",
    "    global_lr = float(self.cfg.optimizer.lr) * float(self.cfg.local_epochs)\n",
    "    reg_param = self.cfg.lambda_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        coalitions_reprs = {}\n",
    "        for col_ind, lst_clients in self.coalitions.items():\n",
    "\n",
    "            m_t = sum(len_clients_ds[id] for id in lst_clients)\n",
    "            for i, id in enumerate(lst_clients):\n",
    "                if not id in lst_active_ids:\n",
    "                    continue\n",
    "                state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "                state = torch.load(state_path, weights_only= False)\n",
    "                client_h = state['h']\n",
    "\n",
    "                if i == 0:\n",
    "                    col_repr = torch.zeros_like(client_h)\n",
    "\n",
    "                n_k = len_clients_ds[id]\n",
    "                weight =  n_k / m_t \n",
    "\n",
    "                col_repr.add_(weight * client_h)\n",
    "            coalitions_reprs[col_ind] = col_repr\n",
    "            \n",
    "\n",
    "        for col_ind, lst_clients in self.coalitions.items():\n",
    "            for i, id in enumerate(lst_clients):\n",
    "                if not id in lst_active_ids:\n",
    "                    continue\n",
    "                state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "                \n",
    "                state = torch.load(state_path, weights_only= False)\n",
    "                client_model = state['model']\n",
    "\n",
    "                client_diff = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_model.items() if key.startswith(\"fc2\") or key.startswith(\"dropout\")\n",
    "                }\n",
    "\n",
    "                for j, other_id in enumerate(lst_clients):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    other_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{other_id}\", \"state.pth\")\n",
    "                    \n",
    "                    other_state = torch.load(other_state_path, weights_only= False)\n",
    "                    other_client_model = other_state['model']\n",
    "\n",
    "                    a_kl = self.akl_connection[i, j]\n",
    "                    for key in client_model.keys():\n",
    "                        if key.startswith(\"fc2\") or key.startswith(\"dropout\"):\n",
    "                            client_diff[key].add_(a_kl * (client_model[key] - other_client_model[key]))\n",
    "\n",
    "                for key in client_model.keys():\n",
    "                    if key.startswith(\"fc2\") or key.startswith(\"dropout\"):\n",
    "                        client_model[key].sub_(global_lr * reg_param * client_diff[key])\n",
    "\n",
    "                clinet_state = {\n",
    "                    'model': client_model,\n",
    "                    'h': state['h'],\n",
    "                    'h_c': coalitions_reprs[col_ind],\n",
    "                }\n",
    "\n",
    "                agg_client_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"aggregated_model_{id}\", \"state.pth\")\n",
    "                \n",
    "                if not os.path.exists(os.path.dirname(agg_client_state_path)):\n",
    "                    os.makedirs(os.path.dirname(agg_client_state_path))\n",
    "\n",
    "                torch.save(clinet_state, agg_client_state_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to align the represntation of the coalition with the local represntation. This process is done as follows:\n",
    "\n",
    "$$\\min_{\\phi_i} \\left\\|\\bm{\\phi}_i(x)- \\bm{h}_c\\right\\|^2 ; \\hspace{0.3cm} \\forall x \\in D_k$$\n",
    "\n",
    "We prevent the coallpse of reprenstations as follows:\n",
    "\n",
    "$$\\bm{h}_c^{(i+1)} = \\beta \\bm{h}_c^{(i)} + (1-\\beta) \\bm{\\phi}_i(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def extra_computation(self: DMTL, lst_active_ids, comm_round):\n",
    "    \n",
    "    for id in lst_active_ids:\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, comm_round, self.loss_fn, to_read_from= 'aggregated_model_')\n",
    "        \n",
    "        client.model.train()\n",
    "        for param in client.model.classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        client.model = client.model.to(client.device)\n",
    "        client.h_c = client.h_c.to(client.device)\n",
    "\n",
    "        optimizer = get_cls(\"torch.nn\", self.cfg.optimizer2)(client.model.encoder.parameters(), lr=self.cfg.lr2)\n",
    "        \n",
    "        client.train_loader = torch.utils.data.DataLoader(client.train_ds, batch_size=1, shuffle=True)\n",
    "        for i, batch in enumerate(client.train_loader):\n",
    "            batch = client.get_batch(batch)\n",
    "            X = batch['x']\n",
    "            optimizer.zero_grad()\n",
    "            h_prime = client.model.encoder(X)\n",
    "            loss = client.alignment_criterion()(h_prime, client.h_c)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                client.h_c.mul_(self.cfg.beta1).add_(h_prime, alpha=1 - self.cfg.beta1)\n",
    "\n",
    "        \n",
    "        state = {\n",
    "            'model': client.model.state_dict(),\n",
    "            'h_c': client.h_c,\n",
    "            'h': client.h\n",
    "        }\n",
    "\n",
    "        state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_aligned_{id}\", \"state.pth\")\n",
    "        if not os.path.exists(os.path.dirname(state_path)):\n",
    "            os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "        torch.save(state, state_path)\n",
    "\n",
    "        for param in client.model.classifier.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pFedMe  \n",
    "Personalized Federated learning using Morseu-envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class pFedMe(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "        \n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.local_model = deepcopy(self.model)\n",
    "            self.optimizer = pFedMeOptimizer(self.model.to(self.device).parameters(), lr=self.cfg.personal_lr, lambda_=self.cfg.lambda_)\n",
    "            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 s  71.3 ns per loop (mean  std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: pFedMe, state_dict):  # noqa: F811    \n",
    "    state_path = os.path.join(self.cfg.save_dir, str(self.t), f\"local_output_{self.id}\", \"state.pth\")\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(state_path)):\n",
    "        os.makedirs(os.path.dirname(state_path))\n",
    "\n",
    "    self.pers_model = deepcopy(self.model).to(self.device)\n",
    "    with torch.no_grad():\n",
    "        for p_model, p_updated in zip(self.pers_model.parameters(), self.persionalized_model_bar):\n",
    "            p_model.copy_(p_updated)\n",
    "\n",
    "    client_state = {\n",
    "        **state_dict,\n",
    "        \"model\": self.model.state_dict(),\n",
    "        'optimizer': self.optimizer.state_dict(),\n",
    "        'pers_model': self.pers_model.state_dict(),\n",
    "    }\n",
    "    \n",
    "    torch.save(client_state, state_path)\n",
    "\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        save_space(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: pFedMe, another_agent: Agent):  # noqa: F811\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        self.save_state(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: pFedMe, batch: dict) -> tuple:\n",
    "    \n",
    "    # find an approximate theta\n",
    "    for j in range(self.cfg.K):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss, metrics = self._closure(batch)\n",
    "\n",
    "        if loss.item() == 0.0:\n",
    "            return loss, metrics\n",
    "        \n",
    "        loss.backward()\n",
    "        self.persionalized_model_bar, _ = self.optimizer.step(self.local_model)\n",
    "\n",
    "    # update local weight after finding aproximate theta\n",
    "    with torch.no_grad():\n",
    "        for localweight, per_param in zip(self.local_model.parameters(), self.persionalized_model_bar):\n",
    "            localweight.sub_(self.cfg.lambda_* self.cfg.optimizer.lr * (localweight - per_param))\n",
    "\n",
    "\n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "    \n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: pFedMe):\n",
    "\n",
    "    for i, batch in enumerate(self.train_loader):\n",
    "        batch = self.get_batch(batch)\n",
    "        self._run_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def fit(self: pFedMe) -> dict:\n",
    "    \n",
    "    self.model = self.model.to(self.device)\n",
    "    self.local_model = self.local_model.to(self.device)\n",
    "    self.model.train()\n",
    "    self.local_model.train()\n",
    "    for _ in range(self.cfg.local_epochs):\n",
    "        self._run_epoch()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for model_param, local_param in zip(self.model.parameters(), self.local_model.parameters()):\n",
    "            model_param.copy_(local_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train_test_stats(self: pFedMe, batch: dict) -> tuple:\n",
    "    metrics = {k: 0 for k in list(self.cfg.training_metrics)}  # Ensure metrics is always defined\n",
    "\n",
    "    try:\n",
    "        X, y = batch['x'], batch['y']\n",
    "        logits = self.pers_model(X)\n",
    "        loss = self.criterion(logits, y)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        y_pred = probs.argmax(dim=-1)\n",
    "        y_true = batch[self.label_key]\n",
    "\n",
    "        if hasattr(self, \"training_metrics\") and self.cfg.training_metrics:\n",
    "            if hasattr(self, \"tokenizer\"):\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true, tokenizer=self.tokenizer)\n",
    "            else:\n",
    "                metrics = self.training_metrics.compute(y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in _closure Eval_test_stats: {e}\")\n",
    "        return torch.tensor(0.0, dtype=torch.float32, device=self.device), metrics  # Return safe values\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate_local(self: pFedMe, loader= 'train') -> dict:\n",
    "    total_loss = 0\n",
    "    lst_metrics = []\n",
    "\n",
    "    # self.per_model = deepcopy(self.model)\n",
    "    # self.per_model = self.per_model.to(self.device)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for param, per_param in zip(self.per_model.parameters(), self.persionalized_model_bar.to(self.device).parameters()):\n",
    "    #         param.copy_(per_param)\n",
    "    \n",
    "    self.pers_model.eval()\n",
    "    self.pers_model = self.pers_model.to(self.device)\n",
    "\n",
    "    num_eval = 0\n",
    "    data_loader = self.train_loader if loader == 'train' else self.test_loader\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            batch = self.get_batch(batch)\n",
    "            loss, metrics = self.train_test_stats(batch)                 \n",
    "            if not math.isnan(loss.item()):\n",
    "                total_loss += loss.item()  \n",
    "                num_eval += len(batch[self.data_key])  # Ensure num_eval is updated\n",
    "                lst_metrics.append(metrics)           \n",
    "    \n",
    "    avg_loss = total_loss / num_eval if num_eval > 0 else 0.0\n",
    "    logger.info(f\"Average {loader} Loss is : {avg_loss}\")\n",
    "    \n",
    "    if lst_metrics:\n",
    "        total_metrics = {k: sum(m.get(k, 0) for m in lst_metrics) / len(lst_metrics) for k in self.cfg.test_metrics}\n",
    "    else:\n",
    "        total_metrics = {k: 0.0 for k in self.cfg.test_metrics}\n",
    "\n",
    "    return {\"loss\": avg_loss, \"metrics\": total_metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server initiate the evaluation process, which computes the local metrics and average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate(self: pFedMe, t):\n",
    "    self.cfg.agg =\"mtl\"\n",
    "    lst_train_res = []\n",
    "    lst_test_res = []\n",
    "    for id in range(self.cfg.num_clients):\n",
    "\n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)\n",
    "        \n",
    "        res_train = client.evaluate_local(loader= 'train')\n",
    "        lst_train_res.append(res_train)\n",
    "\n",
    "        res_test = client.evaluate_local(loader= 'test')\n",
    "        lst_test_res.append(res_test)\n",
    "        \n",
    "    self.cfg.agg = \"one_model\"\n",
    "    return lst_train_res, lst_test_res    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: pFedMe, lst_active_ids, comm_round, len_clients_ds):\n",
    "\n",
    "    m_t = sum(len_clients_ds.values())\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if comm_round > 1:\n",
    "            prev_server_state_path = os.path.join(self.cfg.save_dir, str(comm_round - 1), \"global_model\", \"state.pth\")\n",
    "            prev_server_state = torch.load(prev_server_state_path, weights_only=False)\n",
    "            prev_global_model = prev_server_state['model']\n",
    "\n",
    "        else:\n",
    "            id = lst_active_ids[0]\n",
    "            prev_server_state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "            prev_server_state = torch.load(prev_server_state_path, weights_only=False)\n",
    "            prev_global_model = prev_server_state['w0'].to(self.device).state_dict()\n",
    "\n",
    "\n",
    "        for i, id in enumerate(lst_active_ids):\n",
    "            state_path = os.path.join(self.cfg.save_dir, str(comm_round), f\"local_output_{id}\", \"state.pth\")\n",
    "            state = torch.load(state_path, weights_only=False)\n",
    "            client_state_dict = state['model']\n",
    "\n",
    "            if i == 0:\n",
    "                global_model = {\n",
    "                    key: torch.zeros_like(value) \n",
    "                    for key, value in client_state_dict.items()\n",
    "                }\n",
    "\n",
    "            n_k = len_clients_ds[id]\n",
    "            weight =  n_k / m_t \n",
    "\n",
    "            for key in client_state_dict.keys():\n",
    "                global_model[key].add_(weight * client_state_dict[key])\n",
    "\n",
    "        for key in global_model.keys():\n",
    "            global_model[key].copy_((1-self.cfg.beta)*global_model[key] + self.cfg.beta*prev_global_model[key])\n",
    "\n",
    "        server_state = {\n",
    "            'model': global_model,\n",
    "        }\n",
    "\n",
    "        server_state_path = os.path.join(self.cfg.save_dir, str(comm_round), \"global_model\", \"state.pth\")\n",
    "        \n",
    "        if not os.path.exists(os.path.dirname(server_state_path)):\n",
    "            os.makedirs(os.path.dirname(server_state_path), exist_ok=True)\n",
    "            \n",
    "        torch.save(server_state, server_state_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-FedAvg\n",
    "\n",
    "Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PerAvgAgent(FLAgent):\n",
    "    def __init__(self, \n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT,\n",
    "                 block= None):\n",
    "        \n",
    "        super().__init__(id, cfg, state, role, block)\n",
    "        \n",
    "        if self.role == AgentRole.CLIENT:\n",
    "            self.local_model = deepcopy(self.model)\n",
    "            self.optimizer = PerFedAvgOpt(self.model.parameters(), lr=self.learning_rate)\n",
    "            self.label_set = list(set(np.array([batch['y'] for batch in self.train_ds])))\n",
    "            self.iter_trainloader = iter(self.train_loader)\n",
    "            self.iter_testloader = iter(self.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_next_batch(self: PerAvgAgent, train= True) -> dict:\n",
    "    \n",
    "    loader_type = self.train_loader if train else self.test_loader\n",
    "    to_iter = self.iter_trainloader if train else self.iter_testloader\n",
    "\n",
    "    if(int(self.cfg.data.batch_size) == 0):\n",
    "        for batch in loader_type:\n",
    "            X = batch['x']\n",
    "            y = batch['y']\n",
    "            return (X.to(self.device), y.to(self.device))\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "\n",
    "            batch = next(to_iter)\n",
    "            X = batch['x']\n",
    "            y = batch['y']\n",
    "            \n",
    "        except StopIteration:\n",
    "\n",
    "            if train:\n",
    "                self.iter_trainloader = iter(self.train_loader)\n",
    "                to_iter = self.iter_trainloader\n",
    "            else:\n",
    "                self.iter_testloader = iter(self.test_loader)\n",
    "                to_iter = self.iter_testloader\n",
    "\n",
    "            batch = next(to_iter)\n",
    "            X = batch['x']\n",
    "            y = batch['y']\n",
    "\n",
    "            \n",
    "        return (X.to(self.device), y.to(self.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: PerAvgAgent) -> tuple:\n",
    "    \n",
    "    X, y = self.get_next_batch()\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(X)\n",
    "    loss = self.loss(output, y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    X, y = self.get_next_batch()\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(X)\n",
    "    loss = self.loss(output, y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step(beta= self.cfg.beta)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for localweight, model_param in zip(self.local_model.parameters(), self.model):\n",
    "            localweight.copy_(model_param)\n",
    "\n",
    "\n",
    "    if self.cfg.model.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.model.grad_norm_clip)\n",
    "    \n",
    "\n",
    "    return loss, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: PerAvgAgent):\n",
    "\n",
    "    for i in range(self.num_minibatch):\n",
    "        self._run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def fit(self: PerAvgAgent) -> None:\n",
    "    \n",
    "    self.num_minibatch = int(len(self.train_ds) / int(self.cfg.data.batch_size))\n",
    "\n",
    "    self.model = self.model.to(self.device)\n",
    "    self.local_model = self.local_model.to(self.device)\n",
    "\n",
    "    self.model.train()\n",
    "    self.local_model.train()\n",
    "\n",
    "    for _ in range(self.cfg.local_epochs):\n",
    "        self._run_epoch()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train_one_step(self: PerAvgAgent) -> None:\n",
    "    self.model.train()\n",
    "    self.model = self.model.to(self.device)\n",
    "\n",
    "    #step 1\n",
    "    X, y = self.get_next_batch(train= False)\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(X)\n",
    "    loss = self.loss(output, y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    \n",
    "    #step 2\n",
    "    X, y = self.get_next_batch(Train= True)\n",
    "    self.optimizer.zero_grad()\n",
    "    output = self.model(X)\n",
    "    loss = self.loss(output, y)\n",
    "    loss.backward()\n",
    "    self.optimizer.step(beta=self.cfg.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def evaluate(self: PerAvgAgent, t):\n",
    "    self.cfg.agg =\"mtl\"\n",
    "    lst_train_res = []\n",
    "    lst_test_res = []\n",
    "    for id in range(self.cfg.num_clients):\n",
    "        \n",
    "        client = self.client_fn(self.client_cls, self.cfg, id, self.latest_round, t, self.loss_fn, state_dir= self.cfg.state_dir)\n",
    "        client.train_one_step()\n",
    "        \n",
    "        res_train = client.evaluate_local(loader= 'train')\n",
    "        lst_train_res.append(res_train)\n",
    "\n",
    "        res_test = client.evaluate_local(loader= 'test')\n",
    "        lst_test_res.append(res_test)\n",
    "        \n",
    "    self.cfg.agg = \"one_model\"\n",
    "    return lst_train_res, lst_test_res    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PeftAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 block,\n",
    "                 id,\n",
    "                 state= None,\n",
    "                 role= \"client\",\n",
    "                 **adapter_settings):\n",
    "        super().__init__(cfg, block, id, state, role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def peftify(self: PeftAgent):\n",
    "    # extract only the adapter's parameters from the model and store them in a dictionary\n",
    "    self.params_dict_old = deepcopy(\n",
    "        OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if\n",
    "                    \"default\" in name))\n",
    "    \n",
    "    self.params_dict_new = deepcopy(self.params_dict_old)\n",
    "    \n",
    "    self.model.state_dict = (\n",
    "        lambda instance, *_, **__: get_peft_model_state_dict(  # noqa: F405\n",
    "            instance, self.params_dict_new, \"default\"\n",
    "        )\n",
    "    ).__get__(self.model, type(self.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch \n",
    "def init_agent(self: PeftAgent):  # noqa: F811\n",
    "    self.peftify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state_(self: PeftAgent, epoch, local_dataset_len_dict, previously_selected_clients_set):  # noqa: F811\n",
    "    # save the new adapter weights to disk\n",
    "    self.save_state(epoch)\n",
    "\n",
    "    local_dataset_len_dict[self.id] = len(self.block)\n",
    "    older_adapter_weight = get_peft_model_state_dict(self.model, self.params_dict_old, \"default\")  # noqa: F405\n",
    "    set_peft_model_state_dict(self.model, older_adapter_weight, \"default\")  # noqa: F405\n",
    "    previously_selected_clients_set = previously_selected_clients_set | set({self.id})\n",
    "    last_client_id = self.id\n",
    "\n",
    "    return self.model, local_dataset_len_dict, previously_selected_clients_set, last_client_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fed-Sophia Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FedSophiaAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None):\n",
    "        super().__init__(id, cfg, state, role, block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train(self: FedSophiaAgent):\n",
    "    trainer = self.trainer(self) \n",
    "    client_history = trainer.fit() \n",
    "    return client_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PADG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class PadgAgent(FLAgent):\n",
    "#     def __init__(self,\n",
    "#                  id, # the id of the agent\n",
    "#                  cfg, # the configuration of the agent.\n",
    "#                  state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "#                  role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "#                  block= None):\n",
    "#         super().__init__(id, cfg, state, role, block)\n",
    "\n",
    "#         if role == AgentRole.SERVER:\n",
    "#             self.connections = torch.from_numpy(generate_graph(self.cfg.num_clients))  # noqa: F405\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def apply_constraints(self: PadgAgent, \n",
    "#                       graph, # (np.ndarray): The input matrix.\n",
    "#                       symmetrize=True, # (bool): If True, makes the matrix symmetric.\n",
    "#                       normalize=True, # (bool): If True, normalizes the matrix symmetrically.\n",
    "#                       threshold= 0, # (float or None): If provided, sets values below this threshold to 0.\n",
    "#                       diag_fill= 0): # (float or None): If provided, fills the diagonal with this value.\n",
    "    \n",
    "\n",
    "#     # Symmetrize the matrix\n",
    "#     if symmetrize:\n",
    "#         graph = (graph + graph.T) / 2\n",
    "\n",
    "#     # Apply threshold to ensure non-negativity\n",
    "#     if threshold is not None:\n",
    "#         graph = torch.where(graph > threshold, graph, 0)\n",
    "\n",
    "#     # Normalize the matrix symmetrically\n",
    "#     if normalize:\n",
    "#         row_sums = graph.sum(axis=1, keepdims=True)\n",
    "#         col_sums = graph.sum(axis=0, keepdims=True)\n",
    "#         norm_factor = torch.sqrt(row_sums @ col_sums)  # Symmetric normalization factor\n",
    "#         graph = torch.divide(graph, norm_factor, where=norm_factor != 0)\n",
    "\n",
    "#     # Fill the diagonal\n",
    "#     if diag_fill is not None:\n",
    "#         torch.fill_diagonal(graph, diag_fill)\n",
    "\n",
    "#     return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def compute_probs(self: PadgAgent,\n",
    "#                   batch_size=32, # batch_size (int): Batch size for evaluation.\n",
    "#                   return_log_probs=True): # return_log_probs (bool): If True, return log-probabilities; otherwise, return probabilities.\n",
    "    \n",
    "#     # Computes probabilities or log-probabilities across the entire dataset for a given model.\n",
    "#     # Ensure model is in evaluation mode\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     self.model.to(device)\n",
    "#     self.model.eval()\n",
    "    \n",
    "#     # Create DataLoader for the dataset\n",
    "#     dataloader = DataLoader(self.train_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     all_probs = []  # To store probabilities or log-probabilities for all batches\n",
    "    \n",
    "#     with torch.no_grad():  # Disable gradient computation\n",
    "#         for batch in dataloader:  # Assuming dataset returns (inputs, labels)\n",
    "#             inputs = batch['x'].to(device)  # Move to model's device\n",
    "            \n",
    "#             logits = self.model(inputs)\n",
    "            \n",
    "#             if return_log_probs:\n",
    "#                 # Convert logits to log-probabilities\n",
    "#                 batch_log_probs = F.log_softmax(logits, dim=-1)\n",
    "#                 all_probs.append(batch_log_probs)\n",
    "#             else:\n",
    "#                 # Convert logits to probabilities\n",
    "#                 batch_probs = F.softmax(logits, dim=-1)\n",
    "#                 all_probs.append(batch_probs)\n",
    "    \n",
    "#     self.model.to('cpu')\n",
    "#     # Concatenate all batch probabilities/log-probabilities\n",
    "#     return torch.cat(all_probs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_grad(self: PadgAgent,\n",
    "#                  ):\n",
    "#     # compute the graident f R(w, A) w.r.t w\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def aggregate(self: PadgAgent, lst_active_ids, comm_round, len_clients_ds, one_model= False):\n",
    "    \n",
    "#     visited = []\n",
    "#     for i, id in enumerate(lst_active_ids):\n",
    "\n",
    "#         neighbour_ids = torch.where(self.connections[id] != float(0))[0]\n",
    "\n",
    "#         model_path = os.path.join(self.cfg.save_dir, \n",
    "#                                    str(comm_round),\n",
    "#                                    f\"local_output_{id}\",\n",
    "#                                    \"pytorch_model.pth\")\n",
    "#         client_state_dict = torch.load(model_path, map_location='cpu', weights_only= False)\n",
    "#         self.model.load_state_dict(client_state_dict)\n",
    "        \n",
    "#         neighbours_sum = {\n",
    "#             key: torch.zeros_like(value) \n",
    "#             for key, value in client_state_dict.items()\n",
    "#         }\n",
    "            \n",
    "#         probs_1 = self.compute_probs(batch_size=32, return_log_probs=True)\n",
    "        \n",
    "#         for other_id in neighbour_ids:\n",
    "\n",
    "#             if (other_id, id) in visited:\n",
    "#                 continue\n",
    "#             other_model_path = os.path.join(self.cfg.save_dir, \n",
    "#                                     str(comm_round),\n",
    "#                                     f\"local_output_{other_id}\",\n",
    "#                                     \"pytorch_model.pth\")\n",
    "            \n",
    "#             other_client_state_dict = torch.load(other_model_path, map_location='cpu', weights_only= False)\n",
    "#             self.model.load_state_dict(other_client_state_dict)\n",
    "            \n",
    "#             probs_2 = self.compute_probs(batch_size=32, return_log_probs=False)\n",
    "\n",
    "#             kl_div = F.kl_div(probs_1, probs_2, reduction= 'batchmean').to('cpu')\n",
    "#             self.connections[id][other_id] -= self.cfg.server_lr * self.cfg.lambda_ * kl_div\n",
    "\n",
    "#             # apply constraints to the KL divergence\n",
    "#             self.connections[id][other_id] = self.apply_constraints(self.connections[id][other_id])\n",
    "#             self.connections[id][other_id] = self.connections[other_id][id]\n",
    "\n",
    "#             visited.append((id, other_id))\n",
    "#             visited.append((other_id, id))\n",
    "            \n",
    "#         for other_id in neighbour_ids:\n",
    "#             other_model_path = os.path.join(self.cfg.save_dir, \n",
    "#                                     str(comm_round),\n",
    "#                                     f\"local_output_{other_id}\",\n",
    "#                                     \"pytorch_model.pth\")\n",
    "#             other_client_state_dict = torch.load(other_model_path, map_location='cpu', weights_only= False)\n",
    "\n",
    "#             weight = self.connections[id][other_id]\n",
    "#             for key in other_client_state_dict.keys():\n",
    "#                 neighbours_sum[key].data += weight * other_client_state_dict[key].data\n",
    "\n",
    "#         # for key in neighbours_sum.keys():\n",
    "#             # neighbours_sum[key].data /= len(neighbour_ids)\n",
    "\n",
    "#         for key in client_state_dict.keys():\n",
    "#             client_state_dict[key].data = self.cfg.beta * client_state_dict[key].data + (1 - self.cfg.beta) * neighbours_sum[key].data\n",
    "\n",
    "    \n",
    "#         # save the updated model to the disk\n",
    "#         self.save_state(client_state_dict, comm_round + 1, id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira clients have more parameters. Since it's a client for LLM in principle, we need to feed the generation dataset (the dataset of text ids at the end layer not the logits). Also, a tokenizer and a collate function that will be used for the generation and the data loader construction processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentMira(FLAgent):\n",
    "    def __init__(self,\n",
    "                 data_dict: dict,\n",
    "                 model: torch.nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 id: int,\n",
    "                 gen_data_dict: dict,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 collat_fn: LLMDataCollator,\n",
    "                 cfg: DictConfig) -> None:\n",
    "            \n",
    "        super().__init__(data_dict, model, criterion, optimizer, id)\n",
    "        \n",
    "        self.train_ds_genr = gen_data_dict['train']\n",
    "        self.test_ds_genr = gen_data_dict['test']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collat_fn = collat_fn\n",
    "        self.cfg = cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to save space, we will replace the original model with only the trainable peft model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mira Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following:\n",
    "- Define a Mira client.\n",
    "- inspect the `init_local_train` and `terminate_local_train` methods and their effect on the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# base_model = deepcopy(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# config = LoraConfig(\n",
    "#     r=8,# arbitrary numbr but usually 8, 16, 32, 64, 128\n",
    "#     target_modules=['c_attn'],\n",
    "#     lora_alpha=8,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "\n",
    "# peft_model = get_peft_model(gpt2, config)\n",
    "# mira  = AgentMira(DataDict, peft_model, criterion, optimizer, 0, train_dataset, test_dataset, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inpect the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
