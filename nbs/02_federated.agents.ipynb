{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients\n",
    "\n",
    "> The core abstraction for different FL Clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp federated.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict,OrderedDict\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "import torch\n",
    "from peft import *\n",
    "from fedai.trainers import *\n",
    "from fedai.utils import *\n",
    "from fedai.data.core import LLMDataCollator\n",
    "from transformers import AutoTokenizer\n",
    "from omegaconf.dictconfig import DictConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentRole(Enum):\n",
    "    SERVER = 1\n",
    "    CLIENT = 2\n",
    "    MARL = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Agent\n",
    "\n",
    "An agent is an entity that has a state and exist in an environment. In the case of Federated learning (FL), the agent's state is defined as its own model, data, criterion, optimizer. FL Focuses on distributed model training across multiple clients (agents), each with its local data. Clients **collaborate** to improve a global or shared model while keeping their data private. Communication is often periodic (e.g., every few training rounds). On the other hand, Multi-agent RL systems (MARL) Involves multiple agents interacting with an environment to learn policies for specific tasks (e.g., navigation, resource allocation). Each agent has a state also, but the state represntation might differ slightly from that of an FL agent. The data is often not preloaded as in FL rather, it's collected from the environemnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 cfg,\n",
    "                 state= None,\n",
    "                 role= AgentRole.CLIENT):\n",
    "        \n",
    "        self.cfg = cfg # contains all the configurations needed for the agent/trainer.\n",
    "        self.state = state # A dictionary containing the state of the agent\n",
    "        self.id = id # each agent has a unique id\n",
    "        self.role = role # either a client or a server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def init_agent(self: Agent):\n",
    "    # Initialize the state of the agent. In FL Agent, this means making any adjustments to the model/optimizer/state_dict/...etc\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, msg):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update_state(self: Agent):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: Agent):\n",
    "    # save the state of the agent to a file on disk (id, model, optimizer, loss_fn).\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: Agent):\n",
    "    self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARLAgent(Agent):\n",
    "    def _sense(self, state):\n",
    "        # sense the environment\n",
    "        self.state = state\n",
    "\n",
    "    def _decide(self):\n",
    "    # Compute the next action(s) based on the current state and observations.\n",
    "        pass\n",
    "\n",
    "    def _act(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MARLAgent\n",
       "\n",
       ">      MARLAgent (cfg, id, state=None, role='client')\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MARLAgent\n",
       "\n",
       ">      MARLAgent (cfg, id, state=None, role='client')\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MARLAgent, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FLAgent(Agent):\n",
    "    # A Federated Learning Agent that can be used to train a model in a federated learning setting\n",
    "    def __init__(self,\n",
    "                 id, # the id of the agent\n",
    "                 cfg, # the configuration of the agent.\n",
    "                 state= None, # the state of the agent (model, optimizer, loss_fn), etc.\n",
    "                 role= AgentRole.CLIENT, # the role of the agent (client or server)\n",
    "                 block= None): # The data block (local data of the FL Agent).\n",
    "                 \n",
    "        super().__init__(id, cfg, state, role)\n",
    "        if block:\n",
    "            self.train_ds, self.test_ds = block[0], block[1]\n",
    "        \n",
    "        if self.state :\n",
    "            for key, value in self.state.items():\n",
    "                setattr(self, key, value)\n",
    "            self.init_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data blocks are already on the disk, and since RL agents don't have a preloaded data blocks, we don't include the data in the FL agent's state. Another ratioanle behind this decision is that, state should contain dynamic objects that change over the interaction of the agents and data blocks are static in the case of FL agents, unless you are doing FL-RL Agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every client abstraction, whether it a base or any other type of federated client, it will initalize the training locally with a set of steps. This might include things like extracting the peft model out of the base model (in the case of LLMs clients). Also, it will terminate the local training with some steps, like saving the model state dictionary and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adjust the string reprsntation of the client abstraction to make it more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __str__(self: FLAgent) -> str:\n",
    "    return f'''FLAgent: {self.__class__.__name__}\n",
    "    Index : {self.id}\n",
    "    Model: {self.model.__class__.__name__}\n",
    "    Criterion: {self.criterion.__class__.__name__}\n",
    "    Optimizer: {self.optimizer.__class__.__name__}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def init_agent(self: FLAgent):  # noqa: F811\n",
    "    self.optimizer = get_class('torch.optim', self.cfg.optimizer.name)(self.model.parameters(),  # noqa: F405\n",
    "                                                                                lr= self.cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: FLAgent):\n",
    "    self.model = None if hasattr(self, 'model') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state(self: FLAgent, state_dict, comm_round):  # noqa: F811\n",
    "    # save the model to self.cfg.save_dir/comm_round/f\"local_output_{id}\"/pytorch_model.bin\n",
    "    \n",
    "    model_path = os.path.join(self.cfg.save_dir, \n",
    "                              str(comm_round),\n",
    "                              f\"local_output_{self.id}\")\n",
    "    \n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(state_dict, \n",
    "               os.path.join(model_path, \n",
    "                            \"pytorch_model.pth\"))\n",
    "    save_space(self)  # noqa: F405\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To do: implement the communication process in **Protobuf**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication\n",
    "\n",
    "Communication refers to the process of downloading and uploading models from the server and to the client. Since we are safeguarding against memory issues, we use sequential client processing and disk checkpointing as our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def communicate(self: Agent, another_agent: Agent, comm_round):  # noqa: F811\n",
    "    if self.role == AgentRole.CLIENT:\n",
    "        self.save_state(self.model.state_dict(), comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: FLAgent, lst_active_ids, comm_round, len_clients_ds):\n",
    "    # load the models of the agents in lst_active_ids and `FedAvg` them. At the end, save the aggregated model to the disk.\n",
    "        \n",
    "    for i, id in enumerate(lst_active_ids):\n",
    "        model_path = os.path.join(self.cfg.save_dir, \n",
    "                                   str(comm_round),\n",
    "                                   f\"local_output_{id}\",\n",
    "                                   \"pytorch_model.pth\")\n",
    "        client_state_dict = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "        if i == 0:\n",
    "            client_avg = {\n",
    "                key: torch.zeros_like(value) \n",
    "                for key, value in client_state_dict.items()\n",
    "            }\n",
    "        \n",
    "        weight = len_clients_ds[i] / sum(len_clients_ds)\n",
    "\n",
    "        for key in client_state_dict.keys():\n",
    "            client_avg[key].data += weight * client_state_dict[key].data\n",
    "\n",
    "    for key in client_avg.keys():\n",
    "        client_avg[key].data /= len(lst_active_ids)\n",
    "\n",
    "    for id in lst_active_ids:\n",
    "        model_path = os.path.join(self.cfg.save_dir, \n",
    "                                  str(comm_round),\n",
    "                                  f\"local_output_{id}\",\n",
    "                                  \"pytorch_model.pth\")\n",
    "        self.save_state(client_avg, comm_round)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "client_avg = defaultdict(lambda: torch.tensor(0.0).to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_avg['1'] = torch.tensor(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {'1': tensor(1.)})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PeftAgent(FLAgent):\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 block,\n",
    "                 id,\n",
    "                 state= None,\n",
    "                 role= \"client\",\n",
    "                 **adapter_params):\n",
    "        super().__init__(cfg, block, id, state, role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def peftify(self: PeftAgent):\n",
    "    # extract only the adapter's parameters from the model and store them in a dictionary\n",
    "    self.params_dict_old = deepcopy(\n",
    "        OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if\n",
    "                    \"default\" in name))\n",
    "    \n",
    "    self.params_dict_new = deepcopy(self.params_dict_old)\n",
    "    \n",
    "    self.model.state_dict = (\n",
    "        lambda instance, *_, **__: get_peft_model_state_dict(  # noqa: F405\n",
    "            instance, self.params_dict_new, \"default\"\n",
    "        )\n",
    "    ).__get__(self.model, type(self.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch \n",
    "def init_agent(self: PeftAgent):  # noqa: F811\n",
    "    self.peftify()\n",
    "    self.state['optimizer'] = get_class('torch.optim', self.cfg.optimizer.name)(self.model.parameters(),\n",
    "                                                                                lr= self.cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_state_(self: PeftAgent, epoch, local_dataset_len_dict, previously_selected_clients_set):  # noqa: F811\n",
    "    # save the new adapter weights to disk\n",
    "    self.save_state(epoch)\n",
    "\n",
    "    local_dataset_len_dict[self.id] = len(self.block)\n",
    "    older_adapter_weight = get_peft_model_state_dict(self.model, self.params_dict_old, \"default\")  # noqa: F405\n",
    "    set_peft_model_state_dict(self.model, older_adapter_weight, \"default\")  # noqa: F405\n",
    "    previously_selected_clients_set = previously_selected_clients_set | set({self.id})\n",
    "    last_client_id = self.id\n",
    "\n",
    "    return self.model, local_dataset_len_dict, previously_selected_clients_set, last_client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def strategy(self: PeftAgent):\n",
    "    # implement the strategy for the agent if it's a server. This is the aggregation strategy.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira clients have more parameters. Since it's a client for LLM in principle, we need to feed the generation dataset (the dataset of text ids at the end layer not the logits). Also, a tokenizer and a collate function that will be used for the generation and the data loader construction processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentMira(FLAgent):\n",
    "    def __init__(self,\n",
    "                 data_dict: dict,\n",
    "                 model: torch.nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 id: int,\n",
    "                 gen_data_dict: dict,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 collat_fn: LLMDataCollator,\n",
    "                 cfg: DictConfig) -> None:\n",
    "            \n",
    "        super().__init__(data_dict, model, criterion, optimizer, id)\n",
    "        \n",
    "        self.train_ds_genr = gen_data_dict['train']\n",
    "        self.test_ds_genr = gen_data_dict['test']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collat_fn = collat_fn\n",
    "        self.cfg = cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to save space, we will replace the original model with only the trainable peft model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mira Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following:\n",
    "- Define a Mira client.\n",
    "- inspect the `init_local_train` and `terminate_local_train` methods and their effect on the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# base_model = deepcopy(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# config = LoraConfig(\n",
    "#     r=8,# arbitrary numbr but usually 8, 16, 32, 64, 128\n",
    "#     target_modules=['c_attn'],\n",
    "#     lora_alpha=8,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "\n",
    "# peft_model = get_peft_model(gpt2, config)\n",
    "# mira  = AgentMira(DataDict, peft_model, criterion, optimizer, 0, train_dataset, test_dataset, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inpect the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to observe the difference of architecture that we get from peft_model vs base_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# mira.init_local_train('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# mira.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the lengths of the keys of the state dictionaries of the two models, you find out the Lora model has fewer keys. In fact, those are the only trainable parameters that e have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 149)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# len(mira.model.state_dict()), len(base_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the keys of the PeftModel are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- base_model.model.transformer.h.0.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.0.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.1.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.1.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.2.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.2.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.3.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.3.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.4.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.4.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.5.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.5.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.6.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.6.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.7.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.7.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.8.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.8.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.9.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.9.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.10.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.10.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.11.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.11.attn.c_attn.lora_B.weight"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #| hide\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# keys_list = \"\\n\".join(f\"- {key}\" for key in mira.model.state_dict().keys())\n",
    "# display(Markdown(keys_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- transformer.wte.weight\n",
       "- transformer.wpe.weight\n",
       "- transformer.h.0.ln_1.weight\n",
       "- transformer.h.0.ln_1.bias\n",
       "- transformer.h.0.attn.c_attn.weight\n",
       "- transformer.h.0.attn.c_attn.bias\n",
       "- transformer.h.0.attn.c_proj.weight\n",
       "- transformer.h.0.attn.c_proj.bias\n",
       "- transformer.h.0.ln_2.weight\n",
       "- transformer.h.0.ln_2.bias\n",
       "- transformer.h.0.mlp.c_fc.weight\n",
       "- transformer.h.0.mlp.c_fc.bias\n",
       "- transformer.h.0.mlp.c_proj.weight\n",
       "- transformer.h.0.mlp.c_proj.bias\n",
       "- transformer.h.1.ln_1.weight\n",
       "- transformer.h.1.ln_1.bias\n",
       "- transformer.h.1.attn.c_attn.weight\n",
       "- transformer.h.1.attn.c_attn.bias\n",
       "- transformer.h.1.attn.c_proj.weight\n",
       "- transformer.h.1.attn.c_proj.bias\n",
       "- transformer.h.1.ln_2.weight\n",
       "- transformer.h.1.ln_2.bias\n",
       "- transformer.h.1.mlp.c_fc.weight\n",
       "- transformer.h.1.mlp.c_fc.bias\n",
       "- transformer.h.1.mlp.c_proj.weight\n",
       "- transformer.h.1.mlp.c_proj.bias\n",
       "- transformer.h.2.ln_1.weight\n",
       "- transformer.h.2.ln_1.bias\n",
       "- transformer.h.2.attn.c_attn.weight\n",
       "- transformer.h.2.attn.c_attn.bias\n",
       "- transformer.h.2.attn.c_proj.weight\n",
       "- transformer.h.2.attn.c_proj.bias\n",
       "- transformer.h.2.ln_2.weight\n",
       "- transformer.h.2.ln_2.bias\n",
       "- transformer.h.2.mlp.c_fc.weight\n",
       "- transformer.h.2.mlp.c_fc.bias\n",
       "- transformer.h.2.mlp.c_proj.weight\n",
       "- transformer.h.2.mlp.c_proj.bias\n",
       "- transformer.h.3.ln_1.weight\n",
       "- transformer.h.3.ln_1.bias\n",
       "- transformer.h.3.attn.c_attn.weight\n",
       "- transformer.h.3.attn.c_attn.bias\n",
       "- transformer.h.3.attn.c_proj.weight\n",
       "- transformer.h.3.attn.c_proj.bias\n",
       "- transformer.h.3.ln_2.weight\n",
       "- transformer.h.3.ln_2.bias\n",
       "- transformer.h.3.mlp.c_fc.weight\n",
       "- transformer.h.3.mlp.c_fc.bias\n",
       "- transformer.h.3.mlp.c_proj.weight\n",
       "- transformer.h.3.mlp.c_proj.bias\n",
       "- transformer.h.4.ln_1.weight\n",
       "- transformer.h.4.ln_1.bias\n",
       "- transformer.h.4.attn.c_attn.weight\n",
       "- transformer.h.4.attn.c_attn.bias\n",
       "- transformer.h.4.attn.c_proj.weight\n",
       "- transformer.h.4.attn.c_proj.bias\n",
       "- transformer.h.4.ln_2.weight\n",
       "- transformer.h.4.ln_2.bias\n",
       "- transformer.h.4.mlp.c_fc.weight\n",
       "- transformer.h.4.mlp.c_fc.bias\n",
       "- transformer.h.4.mlp.c_proj.weight\n",
       "- transformer.h.4.mlp.c_proj.bias\n",
       "- transformer.h.5.ln_1.weight\n",
       "- transformer.h.5.ln_1.bias\n",
       "- transformer.h.5.attn.c_attn.weight\n",
       "- transformer.h.5.attn.c_attn.bias\n",
       "- transformer.h.5.attn.c_proj.weight\n",
       "- transformer.h.5.attn.c_proj.bias\n",
       "- transformer.h.5.ln_2.weight\n",
       "- transformer.h.5.ln_2.bias\n",
       "- transformer.h.5.mlp.c_fc.weight\n",
       "- transformer.h.5.mlp.c_fc.bias\n",
       "- transformer.h.5.mlp.c_proj.weight\n",
       "- transformer.h.5.mlp.c_proj.bias\n",
       "- transformer.h.6.ln_1.weight\n",
       "- transformer.h.6.ln_1.bias\n",
       "- transformer.h.6.attn.c_attn.weight\n",
       "- transformer.h.6.attn.c_attn.bias\n",
       "- transformer.h.6.attn.c_proj.weight\n",
       "- transformer.h.6.attn.c_proj.bias\n",
       "- transformer.h.6.ln_2.weight\n",
       "- transformer.h.6.ln_2.bias\n",
       "- transformer.h.6.mlp.c_fc.weight\n",
       "- transformer.h.6.mlp.c_fc.bias\n",
       "- transformer.h.6.mlp.c_proj.weight\n",
       "- transformer.h.6.mlp.c_proj.bias\n",
       "- transformer.h.7.ln_1.weight\n",
       "- transformer.h.7.ln_1.bias\n",
       "- transformer.h.7.attn.c_attn.weight\n",
       "- transformer.h.7.attn.c_attn.bias\n",
       "- transformer.h.7.attn.c_proj.weight\n",
       "- transformer.h.7.attn.c_proj.bias\n",
       "- transformer.h.7.ln_2.weight\n",
       "- transformer.h.7.ln_2.bias\n",
       "- transformer.h.7.mlp.c_fc.weight\n",
       "- transformer.h.7.mlp.c_fc.bias\n",
       "- transformer.h.7.mlp.c_proj.weight\n",
       "- transformer.h.7.mlp.c_proj.bias\n",
       "- transformer.h.8.ln_1.weight\n",
       "- transformer.h.8.ln_1.bias\n",
       "- transformer.h.8.attn.c_attn.weight\n",
       "- transformer.h.8.attn.c_attn.bias\n",
       "- transformer.h.8.attn.c_proj.weight\n",
       "- transformer.h.8.attn.c_proj.bias\n",
       "- transformer.h.8.ln_2.weight\n",
       "- transformer.h.8.ln_2.bias\n",
       "- transformer.h.8.mlp.c_fc.weight\n",
       "- transformer.h.8.mlp.c_fc.bias\n",
       "- transformer.h.8.mlp.c_proj.weight\n",
       "- transformer.h.8.mlp.c_proj.bias\n",
       "- transformer.h.9.ln_1.weight\n",
       "- transformer.h.9.ln_1.bias\n",
       "- transformer.h.9.attn.c_attn.weight\n",
       "- transformer.h.9.attn.c_attn.bias\n",
       "- transformer.h.9.attn.c_proj.weight\n",
       "- transformer.h.9.attn.c_proj.bias\n",
       "- transformer.h.9.ln_2.weight\n",
       "- transformer.h.9.ln_2.bias\n",
       "- transformer.h.9.mlp.c_fc.weight\n",
       "- transformer.h.9.mlp.c_fc.bias\n",
       "- transformer.h.9.mlp.c_proj.weight\n",
       "- transformer.h.9.mlp.c_proj.bias\n",
       "- transformer.h.10.ln_1.weight\n",
       "- transformer.h.10.ln_1.bias\n",
       "- transformer.h.10.attn.c_attn.weight\n",
       "- transformer.h.10.attn.c_attn.bias\n",
       "- transformer.h.10.attn.c_proj.weight\n",
       "- transformer.h.10.attn.c_proj.bias\n",
       "- transformer.h.10.ln_2.weight\n",
       "- transformer.h.10.ln_2.bias\n",
       "- transformer.h.10.mlp.c_fc.weight\n",
       "- transformer.h.10.mlp.c_fc.bias\n",
       "- transformer.h.10.mlp.c_proj.weight\n",
       "- transformer.h.10.mlp.c_proj.bias\n",
       "- transformer.h.11.ln_1.weight\n",
       "- transformer.h.11.ln_1.bias\n",
       "- transformer.h.11.attn.c_attn.weight\n",
       "- transformer.h.11.attn.c_attn.bias\n",
       "- transformer.h.11.attn.c_proj.weight\n",
       "- transformer.h.11.attn.c_proj.bias\n",
       "- transformer.h.11.ln_2.weight\n",
       "- transformer.h.11.ln_2.bias\n",
       "- transformer.h.11.mlp.c_fc.weight\n",
       "- transformer.h.11.mlp.c_fc.bias\n",
       "- transformer.h.11.mlp.c_proj.weight\n",
       "- transformer.h.11.mlp.c_proj.bias\n",
       "- transformer.ln_f.weight\n",
       "- transformer.ln_f.bias\n",
       "- lm_head.weight"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #| hide\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# keys_list = \"\\n\".join(f\"- {key}\" for key in base_model.state_dict().keys())\n",
    "# display(Markdown(keys_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
