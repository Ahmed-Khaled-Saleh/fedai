{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "from rouge import Rouge\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Metrics:\n",
    "    def __init__(self, names: list):\n",
    "        self.metrics_names = names\n",
    "        def validate_names(names):\n",
    "            if len(names) == 0:\n",
    "                return True\n",
    "            try:\n",
    "                evaluate.combine(names)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return False\n",
    "        is_valid = validate_names(self.metrics_names)\n",
    "        if not is_valid:\n",
    "            raise ValueError(f\"Invalid metric names, the available metrics are {evaluate.list_evaluation_modules('metric')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def compute(self:Metrics, y_true, y_pred, **kwargs):\n",
    "    self.metrics = evaluate.combine(self.metrics_names)\n",
    "    return self.metrics.compute(predictions= y_pred, references= y_true, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics([\"bleu\", \"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'Ä world']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_func = tokenizer.tokenize\n",
    "tokenize_func(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[\"hello th\", \"hello there !\"], [\"foo bar foobar\"]]\n",
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "res = metrics.compute(y_true=references, y_pred=predictions, tokenizer=tokenize_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3976353643835253, 0.7222222222222222)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['bleu'], res['rougeL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def rouge_score(hyp_ids, ref_ids, tokenizer):\n",
    "    rouge = Rouge()\n",
    "    hyps = torch.where(hyp_ids != -100, hyp_ids, tokenizer.pad_token_id)\n",
    "    refs = torch.where(ref_ids != -100, ref_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    hyps = tokenizer.batch_decode(hyps, skip_special_tokens=True)\n",
    "    refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "    \n",
    "    batch_rouge = 0\n",
    "    for i in range(len(hyps)):\n",
    "        if len(hyps[i].strip()) == 0:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            h = hyps[i].strip().lower()\n",
    "            r = refs[i].strip().lower()\n",
    "            try:\n",
    "                item_rouge = rouge.get_scores(h, r)[0]['rouge-l']['f']\n",
    "            except ValueError:\n",
    "                print(\"Error in calculating rouge score\")\n",
    "                item_rouge = 0\n",
    "\n",
    "            batch_rouge += item_rouge\n",
    "\n",
    "    rouge_score = batch_rouge / len(hyps)\n",
    "    \n",
    "    return rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
