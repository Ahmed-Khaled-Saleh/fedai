{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from fastcore.utils import *  # noqa: F403\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from fedai.data import * # noqa: F403\n",
    "from fedai.vision.models import * # noqa: F403\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "def get_class(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_server(cfg, lst_data_dict, model, holdout_ds, **kwargs):\n",
    "    Server = get_class('fedai.servers', f'Server_{cfg.name}')\n",
    "    client_class = get_class('fedai.clients', f'Client_{cfg.name}')\n",
    "    return Server(cfg, lst_data_dict, model, holdout_ds, client_class, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_space(client) -> None:\n",
    "    client.clear_model()\n",
    "    del client.optimizer\n",
    "    del client\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the difference that it takes to prepare the dataset for sinle device vs multi-device training, we make a method that handles this separately. `prepare_dl` prepares the dataloader needed for the trainer's type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_dl(cfg, ds, shuffle=True, collate_fn=None):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size= cfg.data.batch_size,\n",
    "        shuffle= shuffle,\n",
    "        collate_fn= collate_fn     \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_graph(K, # (np.ndarray): The input matrix.\n",
    "                   symmetrize=True, # (bool): If True, makes the matrix symmetric.\n",
    "                   normalize=True, # (bool): If True, normalizes the matrix symmetrically.\n",
    "                   threshold= 0, # (float or None): If provided, sets values below this threshold to 0.\n",
    "                   diag_fill= 0): # (float or None): If provided, fills the diagonal with this value.\n",
    "    \n",
    "    graph = np.random.randn(K, K)\n",
    "\n",
    "    # Symmetrize the matrix\n",
    "    if symmetrize:\n",
    "        graph = (graph + graph.T) / 2\n",
    "\n",
    "    # Apply threshold\n",
    "    if threshold is not None:\n",
    "        graph = np.where(graph > threshold, graph, 0)\n",
    "\n",
    "    # Normalize the matrix symmetrically\n",
    "    if normalize:\n",
    "        row_sums = graph.sum(axis=1, keepdims=True)\n",
    "        col_sums = graph.sum(axis=0, keepdims=True)\n",
    "        norm_factor = np.sqrt(row_sums @ col_sums)  # Symmetric normalization factor\n",
    "        graph = np.divide(graph, norm_factor, where=norm_factor != 0)\n",
    "\n",
    "    # Fill the diagonal\n",
    "    if diag_fill is not None:\n",
    "        np.fill_diagonal(graph, diag_fill)\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def draw_nx_graph(graph):\n",
    "\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add weighted edges\n",
    "    for i in range(graph.shape[0]):\n",
    "        for j in range(graph.shape[1]):\n",
    "            if graph[i, j] > 0:  # Add edge only if weight > 0\n",
    "                G.add_edge(i, j, weight=graph[i, j])\n",
    "\n",
    "    # Get edge weights for coloring\n",
    "    edges = G.edges(data=True)\n",
    "    weights = [d['weight'] for _, _, d in edges]\n",
    "\n",
    "    # Normalize weights for coloring (between 0 and 1)\n",
    "    normalized_weights = (weights - np.min(weights)) / (np.max(weights) - np.min(weights) + 1e-8)\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G)  # Spring layout for positioning\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color=\"indigo\")\n",
    "\n",
    "    # Draw edges with color mapping\n",
    "    edges = nx.draw_networkx_edges(\n",
    "        G, pos, edge_color=normalized_weights, edge_cmap=plt.cm.viridis, width=2\n",
    "    )\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, font_color=\"black\")\n",
    "\n",
    "    # Add a colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=np.min(weights), vmax=np.max(weights)))\n",
    "    sm.set_array([])  # This line fixes the issue\n",
    "    plt.colorbar(sm, label=\"Edge Weight\", ax=plt.gca())\n",
    "\n",
    "    plt.title(\"Graph Connections\", fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def draw_matrix(graph):\n",
    "    # draw adjacency matrix represntation of a graph\n",
    "    K = graph.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(graph, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "    # Add ticks for all nodes\n",
    "    plt.xticks(ticks=np.arange(K), labels=np.arange(K))\n",
    "    plt.yticks(ticks=np.arange(K), labels=np.arange(K))\n",
    "\n",
    "    # Add labels, colorbar, and title\n",
    "    plt.colorbar(label=\"Connection Weight\")\n",
    "    plt.title(\"\", fontsize=16)\n",
    "    plt.xlabel(\"Node Index\")\n",
    "    plt.ylabel(\"Node Index\")\n",
    "\n",
    "    # Annotate the weights in the matrix\n",
    "    for i in range(graph.shape[0]):\n",
    "        for j in range(graph.shape[1]):\n",
    "            if graph[i, j] > 0:  # Show only non-zero weights\n",
    "                plt.text(j, i, f\"{graph[i, j]:.3f}\", ha='center', va='center', color='white')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Lazy initializer \n",
    "\n",
    "To save the memory, we don't need to instantiate all the client's objects at once. We can use `generators` as our tool to **lazily** instanitate them, meaning that they will only be instantiated and created in memory when we access them. This can be achieved by creating a class ad overriding the `__getitem__` method of our defined class. Inside the class, we use not a generator directly, but a cache object (dictionary) to retireve the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LazyList:\n",
    "    def __init__(self, server, client_cls):\n",
    "        self.server = server\n",
    "        self.client_cls = client_cls\n",
    "        self.client_cache = {}  # Cache to store initialized clients\n",
    "\n",
    "    def clear_cache(self):\n",
    "        # Clear the cache to free memory if needed\n",
    "        self.client_cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __getitem__(self: LazyList, idx):\n",
    "    # Check if the client is already instantiated\n",
    "    if idx not in self.client_cache:\n",
    "        # Instantiate the client and store it in the cache\n",
    "        self.client_cache[idx] = self.client_cls(\n",
    "            data_dict= self.server.lst_data_dict[idx],\n",
    "            model= None, #deepcopy(self.server.model),\n",
    "            criterion= self.server.criterion,\n",
    "            optimizer= None, #get_class('torch.optim', self.server.cfg.optimizer)(self.server.model.parameters(), lr= self.server.cfg.lr),\n",
    "            idx= idx,\n",
    "            gen_data_dict= self.server.lst_gen_data_dict[idx],\n",
    "            tokenizer= self.server.tokenizer,\n",
    "            collat_fn= self.server.collat_fn,\n",
    "            cfg= self.server.cfg\n",
    "        )\n",
    "    return self.client_cache[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how can this be used on a real example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
