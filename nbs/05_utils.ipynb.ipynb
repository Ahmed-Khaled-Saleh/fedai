{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from fastcore.utils import *  # noqa: F403\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "from fedai.data import *\n",
    "import numpy as np\n",
    "from fedai.vision.models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "def get_class(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_server(cfg, lst_data_dict, model, holdout_ds, **kwargs):\n",
    "    Server = get_class('fedai.servers', f'Server_{cfg.name}')\n",
    "    client_class = get_class('fedai.clients', f'Client_{cfg.name}')\n",
    "    return Server(cfg, lst_data_dict, model, holdout_ds, client_class, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_space(client) -> None:\n",
    "    client.clear_model()\n",
    "    del client.optimizer\n",
    "    del client\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the difference that it takes to prepare the dataset for sinle device vs multi-device training, we make a method that handles this separately. `prepare_dl` prepares the dataloader needed for the trainer's type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_dl(cfg, ds, shuffle=True, collate_fn=None):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size= cfg.data.batch_size,\n",
    "        shuffle= shuffle,\n",
    "        collate_fn= collate_fn     \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner's utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model(cfg):\n",
    "    model_name = cfg.model.name\n",
    "\n",
    "    # Check if the model name contains \"hf://\"\n",
    "    if model_name.startswith(\"hf://\"):\n",
    "        return get_hf_model(cfg)  # type: ignore # Call your HF model loader function  # noqa: F405\n",
    "\n",
    "    # Define the rest of the model mapping\n",
    "    mapping = {\n",
    "        \"MNISTCNN\": MNISTCNN(num_classes=10),  # noqa: F405 # type: ignore\n",
    "        \"CIFAR10CNN\": CIFAR10CNN(num_classes=10),  # noqa: F405 # type: ignore\n",
    "        \n",
    "        \"MLP\": MLP(\n",
    "            dim_in=cfg.model.dim_in, \n",
    "            dim_hidden=cfg.model.dim_hidden, \n",
    "            dim_out=cfg.model.dim_out\n",
    "            ),\n",
    "\n",
    "        \"CharacterLSTM\": CharacterLSTM(  # noqa: F405 # type: ignore\n",
    "            vocab_size=cfg.model.vocab_size,\n",
    "            embed_size=cfg.model.embed_size,\n",
    "            hidden_size=cfg.model.hidden_size,\n",
    "            num_layers=cfg.model.num_layers\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Look up the model in the mapping\n",
    "    if model_name in mapping:\n",
    "        return mapping[model_name]\n",
    "    \n",
    "    raise ValueError(f\"Model '{model_name}' is not recognized, the available models are: {list(mapping.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_criterion(cfg, customm_fn):\n",
    "    if customm_fn:\n",
    "        return customm_fn\n",
    "    return get_class('fedai.loss', cfg.criterion)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_state_from_disk(cfg, model, id, comm_round):\n",
    "    \n",
    "    model_path = os.path.join(cfg.save_dir,\n",
    "                              str(comm_round),\n",
    "                              f\"local_output_{id}\",\n",
    "                              \"pytorch_model.bin\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "            model.load_state_dict(torch.load(model_path, map_location= model))\n",
    "        else:\n",
    "            set_peft_model_state_dict(model,\n",
    "                                  torch.load(model_path, map_location= model.device), \n",
    "                                  \"default\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Lazy initializer \n",
    "\n",
    "To save the memory, we don't need to instantiate all the client's objects at once. We can use `generators` as our tool to **lazily** instanitate them, meaning that they will only be instantiated and created in memory when we access them. This can be achieved by creating a class ad overriding the `__getitem__` method of our defined class. Inside the class, we use not a generator directly, but a cache object (dictionary) to retireve the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LazyList:\n",
    "    def __init__(self, server, client_cls):\n",
    "        self.server = server\n",
    "        self.client_cls = client_cls\n",
    "        self.client_cache = {}  # Cache to store initialized clients\n",
    "\n",
    "    def clear_cache(self):\n",
    "        # Clear the cache to free memory if needed\n",
    "        self.client_cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __getitem__(self: LazyList, idx):\n",
    "    # Check if the client is already instantiated\n",
    "    if idx not in self.client_cache:\n",
    "        # Instantiate the client and store it in the cache\n",
    "        self.client_cache[idx] = self.client_cls(\n",
    "            data_dict= self.server.lst_data_dict[idx],\n",
    "            model= None, #deepcopy(self.server.model),\n",
    "            criterion= self.server.criterion,\n",
    "            optimizer= None, #get_class('torch.optim', self.server.cfg.optimizer)(self.server.model.parameters(), lr= self.server.cfg.lr),\n",
    "            idx= idx,\n",
    "            gen_data_dict= self.server.lst_gen_data_dict[idx],\n",
    "            tokenizer= self.server.tokenizer,\n",
    "            collat_fn= self.server.collat_fn,\n",
    "            cfg= self.server.cfg\n",
    "        )\n",
    "    return self.client_cache[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how can this be used on a real example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
