{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *  # noqa: F403\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "from fedai.data import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "def get_class(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_ds(cfg, get_datasets):\n",
    "    print(\"Generating the datasets...\")\n",
    "    loss_ds, gener_ds = get_datasets(cfg)\n",
    "    print(\"Datasets generated successfully.\")\n",
    "    return loss_ds, gener_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_server(cfg, lst_data_dict, model, holdout_ds, **kwargs):\n",
    "    Server = get_class('fedai.servers', f'Server_{cfg.name}')\n",
    "    client_class = get_class('fedai.clients', f'Client_{cfg.name}')\n",
    "    return Server(cfg, lst_data_dict, model, holdout_ds, client_class, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_space(server, client) -> None:\n",
    "    client.clear_model()\n",
    "    del client.optimizer\n",
    "    del client\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    server.client_list.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the difference that it takes to prepare the dataset for sinle device vs multi-device training, we make a method that handles this separately. `prepare_dl` prepares the dataloader needed for the trainer's type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_dl(trainer, ds, shuffle=True):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size= trainer.cfg.batch_size,\n",
    "        shuffle= shuffle,\n",
    "        collate_fn= trainer.client.collat_fn        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Lazy initializer \n",
    "\n",
    "To save the memory, we don't need to instantiate all the client's objects at once. We can use `generators` as our tool to **lazily** instanitate them, meaning that they will only be instantiated and created in memory when we access them. This can be achieved by creating a class ad overriding the `__getitem__` method of our defined class. Inside the class, we use not a generator directly, but a cache object (dictionary) to retireve the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LazyList:\n",
    "    def __init__(self, server, client_cls):\n",
    "        self.server = server\n",
    "        self.client_cls = client_cls\n",
    "        self.client_cache = {}  # Cache to store initialized clients\n",
    "\n",
    "    def clear_cache(self):\n",
    "        # Clear the cache to free memory if needed\n",
    "        self.client_cache = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __getitem__(self: LazyList, idx):\n",
    "    # Check if the client is already instantiated\n",
    "    if idx not in self.client_cache:\n",
    "        # Instantiate the client and store it in the cache\n",
    "        self.client_cache[idx] = self.client_cls(\n",
    "            data_dict= self.server.lst_data_dict[idx],\n",
    "            model= None, #deepcopy(self.server.model),\n",
    "            criterion= self.server.criterion,\n",
    "            optimizer= None, #get_class('torch.optim', self.server.cfg.optimizer)(self.server.model.parameters(), lr= self.server.cfg.lr),\n",
    "            idx= idx,\n",
    "            gen_data_dict= self.server.lst_gen_data_dict[idx],\n",
    "            tokenizer= self.server.tokenizer,\n",
    "            collat_fn= self.server.collat_fn,\n",
    "            cfg= self.server.cfg\n",
    "        )\n",
    "    return self.client_cache[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how can this be used on a real example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
