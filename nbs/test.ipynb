{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedai.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from fedai.servers import *\n",
    "from fedai.clients import *\n",
    "from fedai.utils import *\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hydra as our config manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'folder/config/config.yaml'\n",
    "config_path = os.path.dirname(path)\n",
    "config_name = os.path.basename(path)\n",
    "config_name = os.path.splitext(config_name)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--fname', type=str,\n",
    "    help='name of config file to load',\n",
    "    default='configs.yaml')\n",
    "\n",
    "fname = parser.parse_args().fname\n",
    "config_path = os.path.dirname(fname)\n",
    "config_name = os.path.splitext(os.path.basename(config_path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/ahmed/.local/share/jupyter/runtime/kernel-v345b718cf6313858386357bb42dd52bcafd1180c9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "@hydra.main(version_base=None, config_path=config_path, config_name=config_name)\n",
    "def app(cfg: DictConfig) -> None:\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    loss_ds, gener_ds = load_ds(cfg)\n",
    "    list_train_ds, list_eval_ds, tokenizer, datacollator = loss_ds\n",
    "    list_train_ds_genr, list_eval_ds_genr = gener_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedai.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedai.data import get_dolly\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dolly = get_dolly('', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lst_train_ds, lst_eval_set, tokenizer, data_collator), (lst_train_ds_genr, lst_eval_set_genr)  = dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_data_dict = [{'train': tr, 'test': test} for tr, test in zip(lst_train_ds, lst_eval_set)]\n",
    "lst_data_dict_genr = [{'train': tr, 'test': test} for tr, test in zip(lst_train_ds_genr, lst_eval_set_genr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_data_dict) == len(lst_data_dict_genr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(y_pred, y_true= None):\n",
    "    loss = y_pred.loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'lst_gen_data_dict': lst_data_dict_genr,\n",
    "          'tokenizer': tokenizer,\n",
    "          'collat_fn': data_collator,\n",
    "          'criterion': criterion\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'dolly', 'model': 'openai-community/gpt2', 'max_length': 1022, 'num_clients': 80, 'iid': 'dir0.5', 'batch_size': 1, 'dataset_subsample': 1.0, 'eval_metric': 'loss', 'log_root': 'logs', 'save_dir': 'checkpoints', 'lora_alpha': 8, 'lora_dropout': 0.05, 'r': 32, 'target_modules': ['c_attn'], 'lr': 5e-05, 'optimizer': 'Adam', 'name': 'mira', 'device': 0, 'bias_sampling': False, 'num_clients_per_task': 10, 'use_prompts': True, 'rounds': 40, 'm': 0.05}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fedai.core import get_cfg\n",
    "cfg = get_cfg()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.log_dir = os.path.join(cfg.log_root, \n",
    "                            datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        \n",
    "cfg.save_dir = os.path.join(cfg.save_dir, \n",
    "                            datetime.now().strftime(\"%Y%m%d_%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "server = get_server(cfg, lst_data_dict, model= None, holdout_ds= None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fedai.utils.LazyInit>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.client_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.client_list = LazyList(server, Client_mira)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_indices_rounds = get_client_indices_rounds(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(client_indices_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client 31 finished round 1\n",
      "client 71 finished round 1\n",
      "client 44 finished round 1\n",
      "client 20 finished round 1\n",
      "client 28 finished round 2\n",
      "client 37 finished round 2\n",
      "client 24 finished round 2\n",
      "client 30 finished round 2\n",
      "client 73 finished round 3\n",
      "client 58 finished round 3\n",
      "client 29 finished round 3\n",
      "client 1 finished round 3\n",
      "client 8 finished round 4\n",
      "client 76 finished round 4\n",
      "client 57 finished round 4\n",
      "client 47 finished round 4\n",
      "client 28 finished round 5\n",
      "client 39 finished round 5\n",
      "client 37 finished round 5\n",
      "client 63 finished round 5\n",
      "client 65 finished round 6\n",
      "client 70 finished round 6\n",
      "client 8 finished round 6\n",
      "client 16 finished round 6\n",
      "client 51 finished round 7\n",
      "client 32 finished round 7\n",
      "client 38 finished round 7\n",
      "client 47 finished round 7\n",
      "client 30 finished round 8\n",
      "client 37 finished round 8\n",
      "client 51 finished round 8\n",
      "client 78 finished round 8\n",
      "client 38 finished round 9\n",
      "client 51 finished round 9\n",
      "client 18 finished round 9\n",
      "client 57 finished round 9\n",
      "client 29 finished round 10\n",
      "client 11 finished round 10\n",
      "client 20 finished round 10\n",
      "client 61 finished round 10\n",
      "client 16 finished round 11\n",
      "client 22 finished round 11\n",
      "client 78 finished round 11\n",
      "client 10 finished round 11\n",
      "client 50 finished round 12\n",
      "client 13 finished round 12\n",
      "client 39 finished round 12\n",
      "client 51 finished round 12\n",
      "client 19 finished round 13\n",
      "client 79 finished round 13\n",
      "client 74 finished round 13\n",
      "client 77 finished round 13\n",
      "client 44 finished round 14\n",
      "client 36 finished round 14\n",
      "client 47 finished round 14\n",
      "client 10 finished round 14\n",
      "client 3 finished round 15\n",
      "client 41 finished round 15\n",
      "client 14 finished round 15\n",
      "client 0 finished round 15\n",
      "client 12 finished round 16\n",
      "client 46 finished round 16\n",
      "client 3 finished round 16\n",
      "client 57 finished round 16\n",
      "client 16 finished round 17\n",
      "client 61 finished round 17\n",
      "client 66 finished round 17\n",
      "client 42 finished round 17\n",
      "client 66 finished round 18\n",
      "client 49 finished round 18\n",
      "client 34 finished round 18\n",
      "client 75 finished round 18\n",
      "client 61 finished round 19\n",
      "client 27 finished round 19\n",
      "client 10 finished round 19\n",
      "client 79 finished round 19\n",
      "client 79 finished round 20\n",
      "client 11 finished round 20\n",
      "client 60 finished round 20\n",
      "client 42 finished round 20\n",
      "client 38 finished round 21\n",
      "client 32 finished round 21\n",
      "client 18 finished round 21\n",
      "client 6 finished round 21\n",
      "client 4 finished round 22\n",
      "client 68 finished round 22\n",
      "client 33 finished round 22\n",
      "client 15 finished round 22\n",
      "client 66 finished round 23\n",
      "client 71 finished round 23\n",
      "client 31 finished round 23\n",
      "client 42 finished round 23\n",
      "client 31 finished round 24\n",
      "client 65 finished round 24\n",
      "client 39 finished round 24\n",
      "client 1 finished round 24\n",
      "client 79 finished round 25\n",
      "client 26 finished round 25\n",
      "client 18 finished round 25\n",
      "client 25 finished round 25\n",
      "client 46 finished round 26\n",
      "client 20 finished round 26\n",
      "client 71 finished round 26\n",
      "client 62 finished round 26\n",
      "client 58 finished round 27\n",
      "client 38 finished round 27\n",
      "client 2 finished round 27\n",
      "client 39 finished round 27\n",
      "client 12 finished round 28\n",
      "client 78 finished round 28\n",
      "client 7 finished round 28\n",
      "client 79 finished round 28\n",
      "client 70 finished round 29\n",
      "client 33 finished round 29\n",
      "client 78 finished round 29\n",
      "client 18 finished round 29\n",
      "client 76 finished round 30\n",
      "client 53 finished round 30\n",
      "client 50 finished round 30\n",
      "client 4 finished round 30\n",
      "client 64 finished round 31\n",
      "client 58 finished round 31\n",
      "client 8 finished round 31\n",
      "client 33 finished round 31\n",
      "client 56 finished round 32\n",
      "client 7 finished round 32\n",
      "client 60 finished round 32\n",
      "client 31 finished round 32\n",
      "client 53 finished round 33\n",
      "client 65 finished round 33\n",
      "client 55 finished round 33\n",
      "client 47 finished round 33\n",
      "client 47 finished round 34\n",
      "client 15 finished round 34\n",
      "client 34 finished round 34\n",
      "client 59 finished round 34\n",
      "client 37 finished round 35\n",
      "client 45 finished round 35\n",
      "client 1 finished round 35\n",
      "client 15 finished round 35\n",
      "client 50 finished round 36\n",
      "client 65 finished round 36\n",
      "client 22 finished round 36\n",
      "client 10 finished round 36\n",
      "client 41 finished round 37\n",
      "client 63 finished round 37\n",
      "client 36 finished round 37\n",
      "client 21 finished round 37\n",
      "client 15 finished round 38\n",
      "client 56 finished round 38\n",
      "client 62 finished round 38\n",
      "client 7 finished round 38\n",
      "client 1 finished round 39\n",
      "client 23 finished round 39\n",
      "client 78 finished round 39\n",
      "client 32 finished round 39\n",
      "client 64 finished round 40\n",
      "client 54 finished round 40\n",
      "client 24 finished round 40\n",
      "client 46 finished round 40\n"
     ]
    }
   ],
   "source": [
    "# for t in range(1, 41):\n",
    "#     clients_history = []\n",
    "#     local_ds_len = {}\n",
    "#     prev_clients = set()    \n",
    "#     for client in server.get_selected_client(client_indices_rounds[t-1]):\n",
    "#         client.model = server.send_model(client)\n",
    "#         client.init_local_train(\"self.output_dir\")\n",
    "#         client.optimizer = get_class('torch.optim', server.cfg.optimizer)(client.model.parameters(), lr= server.cfg.lr)\n",
    "#         history = [0]# training happens at this step\n",
    "#         client.model, local_ds_len, prev_clients, _ = client.terminate_local_train(t, local_ds_len, prev_clients)\n",
    "#         clients_history.append(history)\n",
    "#         print(f\"client {client.idx} finished round {t}\")\n",
    "#         del client#server.save_space(client)\n",
    "#         server.client_list.clear_cache()\n",
    "#         import gc\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
