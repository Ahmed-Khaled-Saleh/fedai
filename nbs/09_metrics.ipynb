{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "from rouge import Rouge\n",
    "import evaluate\n",
    "from fastcore.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "def get_cls(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Metrics:\n",
    "    def __init__(self, lst_metrics_names):\n",
    "        self.lst_metrics_names = lst_metrics_names\n",
    "        self.metrics = {metric_name: 0 for metric_name in lst_metrics_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def prepare_sequence(self: Metrics, y_true):\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    elif isinstance(y_true, list):\n",
    "        y_true = np.array(y_true)\n",
    "    \n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def compute(self: Metrics, y_true, y_pred):\n",
    "    y_true = self.prepare_sequence(y_true)\n",
    "    y_pred = self.prepare_sequence(y_pred)\n",
    "\n",
    "    for metric_name in self.lst_metrics_names:\n",
    "        metric = get_cls('sklearn.metrics', metric_name)\n",
    "        self.metrics[metric_name] = metric(y_true, y_pred)\n",
    "    return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.75,\n",
       " 'f1_score': 0.6666666666666666,\n",
       " 'precision_score': 1.0,\n",
       " 'recall_score': 0.5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Metrics(['accuracy_score', 'f1_score', 'precision_score', 'recall_score'])\n",
    "y_true = [0, 1, 1, 0]\n",
    "y_pred = [0, 1, 0, 0]\n",
    "m.compute(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class Metrics:\n",
    "#     def __init__(self, names: list):\n",
    "#         self.metrics_names = names\n",
    "#         def validate_names(names):\n",
    "#             if len(names) == 0:\n",
    "#                 return True\n",
    "#             try:\n",
    "#                 evaluate.combine(names)\n",
    "#                 return True\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 return False\n",
    "#         is_valid = validate_names(self.metrics_names)\n",
    "#         if not is_valid:\n",
    "#             raise ValueError(f\"Invalid metric names, the available metrics are {evaluate.list_evaluation_modules('metric')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def prepare_targets_llm(self: Metrics, y_true, y_pred, tokenizer= None):\n",
    "\n",
    "#     if hasattr(tokenizer, \"pad_token_id\"):\n",
    "#         padding_mask = (y_true != tokenizer.pad_token_id)  # Shape: (batch_size, seq_length)\n",
    "#         padding_mask_flat = padding_mask.view(-1)  # Flatten the mask\n",
    "#         # Apply the mask\n",
    "#         y_pred = y_pred[padding_mask_flat]\n",
    "#         y_true = y_true[padding_mask_flat]\n",
    "\n",
    "#     y_true = y_true.cpu() if isinstance(y_true, torch.Tensor) else y_true\n",
    "#     y_pred = y_pred.cpu() if isinstance(y_pred, torch.Tensor) else y_pred\n",
    "#     return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def compute(self: Metrics, y_true, y_pred, tokenizer= None,  **kwargs):\n",
    "#     self.metrics = evaluate.combine(self.metrics_names)\n",
    "#     if tokenizer:\n",
    "#         y_true, y_pred = self.prepare_targets_llm(y_true, y_pred, tokenizer)\n",
    "    \n",
    "#     return self.metrics.compute(predictions= y_pred, references= y_true, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = Metrics([\"bleu\", \"rouge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'Ä world']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize_func = tokenizer.tokenize\n",
    "# tokenize_func(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references = [[\"hello th\", \"hello there !\"], [\"foo bar foobar\"]]\n",
    "# predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "# res = metrics.compute(y_true=references, y_pred=predictions, tokenizer=tokenize_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3976353643835253, 0.7222222222222222)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res['bleu'], res['rougeL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# def rouge_score(hyp_ids, ref_ids, tokenizer):\n",
    "#     rouge = Rouge()\n",
    "#     hyps = torch.where(hyp_ids != -100, hyp_ids, tokenizer.pad_token_id)\n",
    "#     refs = torch.where(ref_ids != -100, ref_ids, tokenizer.pad_token_id)\n",
    "\n",
    "#     hyps = tokenizer.batch_decode(hyps, skip_special_tokens=True)\n",
    "#     refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "    \n",
    "#     batch_rouge = 0\n",
    "#     for i in range(len(hyps)):\n",
    "#         if len(hyps[i].strip()) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         else:\n",
    "#             h = hyps[i].strip().lower()\n",
    "#             r = refs[i].strip().lower()\n",
    "#             try:\n",
    "#                 item_rouge = rouge.get_scores(h, r)[0]['rouge-l']['f']\n",
    "#             except ValueError:\n",
    "#                 print(\"Error in calculating rouge score\")\n",
    "#                 item_rouge = 0\n",
    "\n",
    "#             batch_rouge += item_rouge\n",
    "\n",
    "#     rouge_score = batch_rouge / len(hyps)\n",
    "    \n",
    "#     return rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
