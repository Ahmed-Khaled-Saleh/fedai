{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Servers\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from fastcore.utils import *\n",
    "from peft import *\n",
    "from fedai.models import *\n",
    "from fedai.utils import *\n",
    "from fedai.clients import Client_mira, BaseClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseServer\n",
    "\n",
    "The `BaseServer` class can be seen as an abstract base class (ABC), that enables us to define the core structure of the server and operations carried out by it. The server has the following attributes:\n",
    "- `cfg`: An object that contains the configurations tied to this server (things like learning rate of the clients, optimizer, log directory and so on).\n",
    "- `model`: The base model used for all other child classes. Gets transmitted to clients at the beginning of every round in traditional FL. In MTL-based or personalization settings, a deep copy of the model is distributed only at the first round, and clients iterate over their respective model accordingly after that.\n",
    "- `holdout_ds`: A dataset to evaluate the performance at the server. This is optional and can be passed as **None**.\n",
    "- `lst_data_dict`: A list of dictionaries. Every dictionary belongs to one client and contains two keys `train` and `test`.\n",
    "- `client_list`: An object of type `LazyList`. Instantiate all clients. typically done in lazy way, which means that clients are only instantiated when accessd (via their index). This is the prefered when working with large models. It can be accessed as a regular list. Once accessed, the client will be loaded into memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseServer:\n",
    "\n",
    "    def __init__(self, cfg, lst_data_dict, model, holdout_ds, client_class):\n",
    "        self.cfg = cfg\n",
    "        self.lst_data_dict = lst_data_dict\n",
    "        self.model = model\n",
    "        self.holdout_ds = holdout_ds\n",
    "        self.client_list = LazyList(self, client_class)  # type: ignore # noqa: F405\n",
    "        self.latest_model_iter = dict()\n",
    "        self.__str__ = self.__repr__\n",
    "       \n",
    "    def __str__(self) -> str:\n",
    "        return f'''Server: {self.__class__.__name__}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two methods are common among all servers:\n",
    "- `send`: Send a model from the server to the client.\n",
    "- `aggregate`: The aggregation function, what happens when the server recieves the updates from the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def send(self: BaseServer, client: BaseClient):  # noqa: F811\n",
    "    \n",
    "    if client.idx in self.latest_model_iter:\n",
    "        comm_round = self.latest_model_iter[client.idx]\n",
    "        model_path = os.path.join(self.cfg.output_dir, str(comm_round), \n",
    "                                  \"local_output_{}\".format(client.idx),\n",
    "                                  \"pytorch_model.pth\")\n",
    "    else:\n",
    "        model_path = ''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        client.model = deepcopy(self.model)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        if isinstance(client.model, torch.nn.Module):\n",
    "            client.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        elif isinstance(client.model, PeftModel): # noqa: F405\n",
    "            set_peft_model_state_dict(client.model,  # noqa: F405\n",
    "                                  torch.load(model_path, map_location='cpu'),\n",
    "                                  \"default\")\n",
    "    return client.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `client_selection` : Client selection is done at this function. The bare minimum is a random uniform selection. Returns a list of lists of all the selected indices. Every inner list reprensts the indices of the selected clients at a specefic round.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def client_selection(self: BaseServer):\n",
    "    client_indices_rounds = []\n",
    "    for _ in range(self.cfg.rounds):\n",
    "        client_indices_rounds.append(np.random.choice(a= np.arange(self.cfg.num_clients), \n",
    "                                                      size=int(self.cfg.num_clients * self.cfg.m), \n",
    "                                                      replace=False))\n",
    "        \n",
    "    return client_indices_rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_selected_client`: Access the selected client. takes a list of selected clients at current round and returns a generator which contains the respected clients. Since we are intializing in a lazy manner, we just need to write a function that returns a `generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_selected_client(self: BaseServer,\n",
    "                        client_indices: list) : # a list of current round's selected clients\n",
    "    \n",
    "    for idx in client_indices:\n",
    "        yield self.client_list[idx]  # Lazily access the client and return a generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server MIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira extends the baseserver's capabilities by initializing the model using the `get_model`, which instantiate an LLM from `HuggingFace` library.\n",
    "\n",
    "When working with Large models in the case of MTL (where every client has a unique model), most of the time, you cannot hold more than one or two models in memory an at the same time, you must start client's local training from the latest locally trained model of this particular client. One way to implement such constraint is to **offload** the model between the *memory* and the *disk*. Although this might be slow a little bit, it might be the only possibility in certain cases, especially with very large models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Server_mira(BaseServer):\n",
    "    def __init__(self, cfg, lst_data_dict, model, holdout_ds,client_class, **kwargs):\n",
    "        super().__init__(cfg, lst_data_dict, model, holdout_ds, client_class)\n",
    "        \n",
    "        self.model = get_model(self.cfg)\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several parameters that constitutes to `MIRA`. Since it operates on a graph, there is the laplacian matrix, the regularization parameter (how much weight we give to collaobration versus non-collaboration). The following function `init_sim_matrix` is responsible for intializing the values of those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def init_sim_matrix(self: Server_mira):\n",
    "    N = self.cfg.num_clients\n",
    "    b = np.random.uniform(0,1,size=(N,N))\n",
    "    b_symm = (b + b.T)/2\n",
    "    b_symm[b_symm < 0.25] = 0\n",
    "    self.alk_connection = b_symm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`send`: Sends a model from the server to the given client. It takes the given client as an input and returns the client's model. Uses `set_peft_model_state_dict` to change the `sate_dict` of the client's model to the `state_dict` from either the server (typical FL) or a loaded one from the disk (personalized/multi-task learning). Below is a detailed explaination of how the loading happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offloading models\n",
    "\n",
    "To implement this offloading, you need to keep track of client's model's paths, the directory in which the latest model of the client resides. `latest_model_iter` is a dictionary that does this exactly. it contains keys of clients that participated in the training process along with the lates model's directrory of those clients. The path of the models is chosen as `self.cfg.output_dir/comm_round/local_output_{client.idx}/pytorch_model.pth\"` and the `comm_round` is the value in the dictionary `latest_model_iter`. To give an example of this, the following is a dctionary after the second communication round (assuming we sample two clients per round):\n",
    "\n",
    "```python \n",
    "latest_model_iter = {\n",
    "                        5: 1\n",
    "                        3: 2,\n",
    "                        4: 1,\n",
    "                        6: 2\n",
    "                    }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be interpreted as: in the first round, clients (5, 4) were selected for training and after they trained, the latest model of them resides in the `output_dir/1/local_output_5/pytorch_model.pth` and `output_dir/1/local_output_4/pytorch_model.pth` respectively. On the other hand, in the second round, clients (3, 6) we selected for training, and after finishing training, the latest model of them resides in `output_dir/2/local_output_3/pytorch_model.pth` and `output_dir/2/local_output_6/pytorch_model.pth`. Note that there is no keys for client number `1` or `2`, which indicates that they have not appeared in the training process up until the current round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: In the next version releases, we will refactor this to use the approach [here](https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html). This might be less costy, but we need to test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation at the server is done using the following formula\n",
    "\n",
    "$$\\bm{W}_k^{(t+1)} = \\bm{W}_{k, R}^{(t)}-\\eta \\lambda \\sum_{\\ell \\in \\mathcal{N}_k} a_{k \\ell}\\left( \\bm{W}_{k, R}^{(t)}- \\bm{W}_{\\ell, R}^{(t)}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def aggregate(self: Server_mira, selected_clients_indices, comm_round):\n",
    "    global_lr = float(self.cfg.lr) * float(self.cfg.local_step)\n",
    "\n",
    "    for i, client_id in enumerate(selected_clients_indices):\n",
    "        client_path = os.path.join(self.cfg.output_dir, str(comm_round), f\"local_output_{client_id}\", \"pytorch_model.pth\")\n",
    "        client_state_dict = torch.load(client_path, map_location=self.device)\n",
    "\n",
    "        client_diff = defaultdict(lambda: torch.tensor(0.0).to(self.device))\n",
    "\n",
    "        for key in client_state_dict.keys():\n",
    "            client_diff[key] = torch.zeros_like(client_state_dict[key]).to(self.device)\n",
    "\n",
    "        for j, other_client_id in enumerate(selected_clients_indices):\n",
    "            if i != j:\n",
    "                other_client_path = os.path.join(self.cfg.output_dir, str(comm_round), f\"local_output_{other_client_id}\", \"pytorch_model.pth\")\n",
    "                other_client_state_dict = torch.load(other_client_path, map_location=self.device)\n",
    "\n",
    "                weight = self.alk_connection[int(client_id)][int(other_client_id)]\n",
    "                for key in client_state_dict.keys():\n",
    "                    client_diff[key].data += weight * (client_state_dict[key].data.clone() - other_client_state_dict[key].data.clone())\n",
    "\n",
    "        for key in client_state_dict:\n",
    "            client_state_dict[key].data -=  global_lr * self.cfg.lambda_ * client_diff[key].data\n",
    "\n",
    "        self.update(client_state_dict, comm_round, client_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update(self: Server_mira, client_state_dict: dict, comm_round: int, client_id: int) -> None:\n",
    "    save_dir = os.path.join(self.cfg.output_dir, str(comm_round + 1), f\"local_output_{client_id}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, \"pytorch_model.pth\")\n",
    "    torch.save(client_state_dict, save_path)\n",
    "    set_peft_model_state_dict(self.model, client_state_dict, \"default\")  # noqa: F405\n",
    "    self.model.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the trained model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test` is responsible for the final evaluation for different **text-based** metrics like `rouge` and `BELU`. After all the federated rounds are done, we end up with a model per client (or the global intitial model for clients that has not particpated in the fL training). This function will loop over te clients to wvaluate each client ad then report a dictionary of metrics values per every client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
