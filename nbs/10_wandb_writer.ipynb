{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WandbWriter\n",
    "\n",
    "> A writer to write results to wandb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp wandb_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *  # type: ignore # noqa: F403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from fastcore.utils import *\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WandbWriter:\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = argparse.Namespace(**cfg)\n",
    "        self.exp_name = self.cfg.project_name + self.cfg.now\n",
    "        key = os.getenv(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=key, verify=False)\n",
    "        self.run = wandb.init(project=self.cfg.project_name, name= self.exp_name, config=self.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lst_train_histories` is of the shape `{'loss': 0.2, 'metrics': {\"accuracy\": 0.5, \"f1\": 0.6}}`, and\n",
    "`test_history` is of the same shape.\n",
    "\n",
    "We log Two different things:\n",
    "- Tables of local train and test results\n",
    "  - Note that the local train results table has length equal to the number of participnat clients in the round `t`.\n",
    "- Average train results(average of the average local results) and average test results (for all clients even non-participant ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# @patch\n",
    "# def write(self: WandbWriter, lst_active_ids, lst_train_res, lst_test_res, round):\n",
    "    \n",
    "#     #[{'loss': 0.2, 'metrics': {\"accuracy\": 0.5, \"f1\": 0.6}}, \n",
    "#     # {'loss': 0.4, 'metrics': {\"accuracy\": 0.3, \"f1\": 0.2}}]\n",
    "\n",
    "#     local_train_losses = [r[\"loss\"] for r in lst_train_res] # for participants clients\n",
    "#     local_train_metrics = [r[\"metrics\"] for r in lst_train_res] # for participants clients\n",
    "\n",
    "#     lst_train_histories = [{\"client_id\": client_id, \"loss\": loss, **metrics} for client_id, loss, metrics in zip(lst_active_ids, local_train_losses, local_train_metrics)]\n",
    "#     train_table = wandb.Table(dataframe= pd.DataFrame(lst_train_histories))\n",
    "#     ########################################################################\n",
    "#     local_test_losses = [r[\"loss\"] for r in lst_test_res] # for all cleints\n",
    "#     local_test_metrics = [r[\"metrics\"] for r in lst_test_res] # for all cleints\n",
    "\n",
    "#     lst_test_histories = [{\"client_id\": client_id, \"loss\": loss, **metrics} for client_id, loss, metrics in zip(list(range(self.cfg.num_clients)), local_test_losses, local_test_metrics)]\n",
    "#     test_table = wandb.Table(dataframe= pd.DataFrame(lst_test_histories))\n",
    "#     ########################################################################\n",
    "#     avg_train_losses = np.mean(local_train_losses)\n",
    "#     avg_train_metrics = self.avg_lst_dicts(local_train_metrics)\n",
    "\n",
    "#     avg_test_losses = np.mean(local_test_losses)\n",
    "#     avg_test_metrics = self.avg_lst_dicts(local_test_metrics)\n",
    "\n",
    "#     ########################################################################\n",
    "#     train_metrics = {f\"train_{k}\": v for k, v in avg_train_metrics.items()}\n",
    "#     test_metrics = {f\"test_{k}\": v for k, v in avg_test_metrics.items()}\n",
    "#     ########################################################################\n",
    "\n",
    "#     to_log = {\"train_loss\": avg_train_losses,\n",
    "#               **train_metrics,\n",
    "#               \"avg_test_loss\": avg_test_losses,\n",
    "#               **test_metrics,\n",
    "#               f\"Round {round} Train metrics\": train_table,\n",
    "#               f\"Round {round} test metrics\": test_table}\n",
    "\n",
    "\n",
    "#     self.run.log(to_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def write(self: WandbWriter, lst_active_ids, lst_train_res, lst_test_res, round):\n",
    "\n",
    "    # Training results (for participating clients)\n",
    "    local_train_losses = [r[\"loss\"] for r in lst_train_res] if lst_train_res else []\n",
    "    local_train_metrics = [r[\"metrics\"] for r in lst_train_res] if lst_train_res else []\n",
    "\n",
    "    lst_train_histories = [{\"client_id\": client_id, \"loss\": loss, **metrics} \n",
    "                           for client_id, loss, metrics in zip(lst_active_ids, local_train_losses, local_train_metrics)]\n",
    "    train_table = wandb.Table(dataframe=pd.DataFrame(lst_train_histories))\n",
    "\n",
    "    # Test results (for all clients)\n",
    "    local_test_losses = [r[\"loss\"] for r in lst_test_res] if lst_test_res else []\n",
    "    local_test_metrics = [r[\"metrics\"] for r in lst_test_res] if lst_test_res else []\n",
    "\n",
    "    lst_test_histories = [{\"client_id\": client_id, \"loss\": loss, **metrics} \n",
    "                          for client_id, loss, metrics in zip(range(len(lst_test_res)), local_test_losses, local_test_metrics)]\n",
    "    test_table = wandb.Table(dataframe=pd.DataFrame(lst_test_histories))\n",
    "\n",
    "    # Compute averages safely\n",
    "    avg_train_losses = np.mean(local_train_losses) if local_train_losses else 0.0\n",
    "    avg_train_metrics = self.avg_lst_dicts(local_train_metrics) if local_train_metrics else {}\n",
    "\n",
    "    avg_test_losses = np.mean(local_test_losses) if local_test_losses else 0.0\n",
    "    avg_test_metrics = self.avg_lst_dicts(local_test_metrics) if local_test_metrics else {}\n",
    "\n",
    "    # Prepare logs\n",
    "    train_metrics = {f\"train_{k}\": v for k, v in avg_train_metrics.items()}\n",
    "    test_metrics = {f\"test_{k}\": v for k, v in avg_test_metrics.items()}\n",
    "\n",
    "    to_log = {\"train_loss\": avg_train_losses,\n",
    "              **train_metrics,\n",
    "              \"avg_test_loss\": avg_test_losses,\n",
    "              **test_metrics,\n",
    "              f\"Round {round} Train metrics\": train_table,\n",
    "              f\"Round {round} Test metrics\": test_table}\n",
    "\n",
    "    self.run.log(to_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@patch\n",
    "def avg_lst_dicts(self: WandbWriter, lst_dict):\n",
    "    return {key: sum(d[key] for d in lst_dict) / len(lst_dict) for key in lst_dict[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "@patch\n",
    "def save(self: WandbWriter, res):\n",
    "    df = pd.concat([pd.DataFrame(d1) for d1 in res])\n",
    "    os.makedirs(self.cfg.res_dir, exist_ok=True)\n",
    "    df.to_csv(f\"{self.cfg.res_dir}/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "@patch\n",
    "def finish(self: WandbWriter):\n",
    "    self.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # type: ignore  # noqa: E702\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
