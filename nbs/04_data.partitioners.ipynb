{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.partitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BasePartitioner:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def assign(self, dataset_content, dataset_label, dataidx_map, X, y, statistic):\n",
    "        for client in range(self.cfg.num_clients):\n",
    "            idxs = dataidx_map[client]\n",
    "            X[client] = dataset_content[idxs]\n",
    "            y[client] = dataset_label[idxs]\n",
    "            for i in np.unique(y[client]):\n",
    "                statistic[client].append((int(i), int(sum(y[client]==i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def parition(self: BasePartitioner, dataset_content, dataset_label, least_samples, dataidx_map):\n",
    "    \"The traditional IID partitioning method\"\n",
    "    X = [[] for _ in range(self.cfg.num_clients)]\n",
    "    y = [[] for _ in range(self.cfg.num_clients)]\n",
    "    statistic = [[] for _ in range(self.cfg.num_clients)]\n",
    "    \n",
    "    idxs = np.array(range(len(dataset_label)))\n",
    "    idx_for_each_class = []\n",
    "    for i in range(self.cfg.data.num_classes):\n",
    "        idx_for_each_class.append(idxs[self.cfg.data.dataset_label == i])\n",
    "\n",
    "    class_num_per_client = [self.cfg.data.class_per_client for _ in range(self.cfg.num_clients)]\n",
    "    for i in range(self.cfg.num_classes):\n",
    "        selected_clients = []\n",
    "        for client in range(self.cfg.num_clients):\n",
    "            if class_num_per_client[client] > 0:\n",
    "                selected_clients.append(client)\n",
    "        if len(selected_clients) == 0:\n",
    "            break\n",
    "        selected_clients = selected_clients[:int(np.ceil((self.cfg.num_clients/self.cfg.data.num_classes)*self.cfg.data.class_per_client))]\n",
    "\n",
    "        num_all_samples = len(idx_for_each_class[i])\n",
    "        num_selected_clients = len(selected_clients)\n",
    "        num_per = num_all_samples / num_selected_clients\n",
    "        if self.cfg.balance:\n",
    "            num_samples = [int(num_per) for _ in range(num_selected_clients-1)]\n",
    "        else:\n",
    "            num_samples = np.random.randint(max(num_per/10, least_samples/self.cfg.num_classes), num_per, num_selected_clients-1).tolist()\n",
    "        num_samples.append(num_all_samples-sum(num_samples))\n",
    "\n",
    "        idx = 0\n",
    "        for client, num_sample in zip(selected_clients, num_samples):\n",
    "            if client not in dataidx_map.keys():\n",
    "                dataidx_map[client] = idx_for_each_class[i][idx:idx+num_sample]\n",
    "            else:\n",
    "                dataidx_map[client] = np.append(dataidx_map[client], idx_for_each_class[i][idx:idx+num_sample], axis=0)\n",
    "            idx += num_sample\n",
    "            class_num_per_client[client] -= 1\n",
    "\n",
    "    X, y, statistic = self.assign(dataset_content, dataset_label, dataidx_map, X, y, statistic)\n",
    "\n",
    "    return X, y, statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirPartitioner(BasePartitioner):\n",
    "    # https://github.com/IBM/probabilistic-federated-neural-matching/blob/master/experiment.py\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "    \n",
    "    def parition(self, dataset_content, dataset_label, least_samples, dataidx_map):\n",
    "        X = [[] for _ in range(self.cfg.num_clients)]\n",
    "        y = [[] for _ in range(self.cfg.num_clients)]\n",
    "        statistic = [[] for _ in range(self.cfg.num_clients)]\n",
    "\n",
    "        min_size = 0\n",
    "        K = self.cfg.data.num_classes\n",
    "        N = len(dataset_label)\n",
    "\n",
    "        try_cnt = 1\n",
    "        while min_size < least_samples:\n",
    "            if try_cnt > 1:\n",
    "                print(f'Client data size does not meet the minimum requirement {least_samples}. Try allocating again for the {try_cnt}-th time.')\n",
    "\n",
    "            idx_batch = [[] for _ in range(self.cfg.num_clients)]\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(dataset_label == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(self.cfg.data.alpha, self.cfg.num_clients))\n",
    "                proportions = np.array([p*(len(idx_j)<N/self.cfg.num_clients) for p,idx_j in zip(proportions,idx_batch)])\n",
    "                proportions = proportions/proportions.sum()\n",
    "                proportions = (np.cumsum(proportions)*len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_batch,np.split(idx_k,proportions))]\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "            try_cnt += 1\n",
    "\n",
    "        for j in range(self.cfg.num_clients):\n",
    "            dataidx_map[j] = idx_batch[j]\n",
    "\n",
    "        X, y, statistic = self.assign(dataset_content, dataset_label, dataidx_map, X, y, statistic)\n",
    "\n",
    "        return X, y, statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtPartitioner(BasePartitioner):\n",
    "    # https://arxiv.org/abs/2311.03154\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "    \n",
    "    def parition(self, dataset_content, dataset_label, least_samples, dataidx_map):\n",
    "        X = [[] for _ in range(self.cfg.num_clients)]\n",
    "        y = [[] for _ in range(self.cfg.num_clients)]\n",
    "        statistic = [[] for _ in range(self.cfg.num_clients)]\n",
    "\n",
    "        C = self.cfg.data.class_per_client\n",
    "        \n",
    "        '''The first level: allocate labels to clients\n",
    "        clientidx_map (dict, {label: clientidxclass_per_client}), e.g., C=2, cfg.num_clients=5, cfg.data.num_classes=10\n",
    "            {0: [0, 1], 1: [1, 2], 2: [2, 3], 3: [3, 4], 4: [4, 5], 5: [5, 6], 6: [6, 7], 7: [7, 8], 8: [8, 9], 9: [9, 0]}\n",
    "        '''\n",
    "        min_size_per_label = 0\n",
    "        # You can adjust the `min_require_size_per_label` to meet you requirements\n",
    "        min_require_size_per_label = max(C * self.cfg.num_clients // self.cfg.data.num_classes // 2, 1)\n",
    "        if min_require_size_per_label < 1:\n",
    "            raise ValueError\n",
    "        clientidx_map = {}\n",
    "        while min_size_per_label < min_require_size_per_label:\n",
    "            # initialize\n",
    "            for k in range(self.cfg.data.num_classes):\n",
    "                clientidx_map[k] = []\n",
    "            # allocate\n",
    "            for i in range(self.cfg.num_clients):\n",
    "                labelidx = np.random.choice(range(self.cfg.data.num_classes), C, replace=False)\n",
    "                for k in labelidx:\n",
    "                    clientidx_map[k].append(i)\n",
    "            min_size_per_label = min([len(clientidx_map[k]) for k in range(self.cfg.data.num_classes)])\n",
    "        \n",
    "        '''The second level: allocate data idx'''\n",
    "        dataidx_map = {}\n",
    "        y_train = dataset_label\n",
    "        min_size = 0\n",
    "        min_require_size = 10\n",
    "        K = self.cfg.data.num_classes\n",
    "        N = len(y_train)\n",
    "        print(\"\\n*****clientidx_map*****\")\n",
    "        print(clientidx_map)\n",
    "        print(\"\\n*****Number of clients per label*****\")\n",
    "        print([len(clientidx_map[i]) for i in range(len(clientidx_map))])\n",
    "\n",
    "        # ensure per client' sampling size >= min_require_size (is set to 10 originally in [3])\n",
    "        while min_size < min_require_size:\n",
    "            idx_batch = [[] for _ in range(self.cfg.num_clients)]\n",
    "            # for each class in the dataset\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(y_train == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(self.cfg.data.alpha, self.cfg.num_clients))\n",
    "                # cfg.data.balance\n",
    "                # Case 1 (original case in Dir): cfg.data.balance the number of sample per client\n",
    "                proportions = np.array([p * (len(idx_j) < N / self.cfg.num_clients and j in clientidx_map[k]) for j, (p, idx_j) in enumerate(zip(proportions, idx_batch))])\n",
    "                # Case 2: Don't cfg.data.balance\n",
    "                #proportions = np.array([p * (j in label_netidx_map[k]) for j, (p, idx_j) in enumerate(zip(proportions, idx_batch))])\n",
    "                proportions = proportions / proportions.sum()\n",
    "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "                # process the remainder samples\n",
    "                '''Note: Process the remainder data samples (yipeng, 2023-11-14).\n",
    "                There are some cases that the samples of class k are not allocated completely, i.e., proportions[-1] < len(idx_k)\n",
    "                In these cases, the remainder data samples are assigned to the last client in `clientidx_map[k]`.\n",
    "                '''\n",
    "                if proportions[-1] != len(idx_k):\n",
    "                    for w in range(clientidx_map[k][-1], self.cfg.num_clients-1):\n",
    "                        proportions[w] = len(idx_k)\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))] \n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        for j in range(self.cfg.num_clients):\n",
    "            np.random.shuffle(idx_batch[j])\n",
    "            dataidx_map[j] = idx_batch[j]\n",
    "\n",
    "        X, y, statistic = self.assign(dataset_content, dataset_label, dataidx_map, X, y, statistic)\n",
    "\n",
    "        return X, y, statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ahmed/Ahmed-home/1- Projects/Research/publications/2024/letter 1/code/PFLlib/dataset/Cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, idx, is_train=True):\n",
    "    if is_train:\n",
    "        train_data_dir = os.path.join(path, 'train')\n",
    "\n",
    "        train_file = os.path.join(train_data_dir, str(idx) + '.npz')\n",
    "        with open(train_file, 'rb') as f:\n",
    "            train_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "\n",
    "        return train_data\n",
    "\n",
    "    else:\n",
    "        test_data_dir = os.path.join(path, 'test/')\n",
    "\n",
    "        test_file = test_data_dir + str(idx) + '.npz'\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "    \n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
