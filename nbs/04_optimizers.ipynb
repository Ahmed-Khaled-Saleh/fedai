{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.partitioners\n",
    "\n",
    "> Implement three FL scnarios. IID, Non-IID, and Extended-non-IID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pFedMe Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class pFedMeOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, lambda_=0.1 , mu = 0.001):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr, lambda_=lambda_, mu = mu)\n",
    "        super(pFedMeOptimizer, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, local_weight_updated, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "        for group in self.param_groups:\n",
    "            for p, localweight in zip( group['params'], local_weight_updated.parameters()):\n",
    "                p.data = p.data - group['lr'] * (p.grad.data + group['lambda_'] * (p.data - localweight.data) + group['mu']*p.data)\n",
    "        return  group['params'], loss\n",
    "    \n",
    "    def update_param(self, local_weight_updated, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "        weight_update = local_weight_updated.copy()\n",
    "        for group in self.param_groups:\n",
    "            for p, localweight in zip( group['params'], weight_update):\n",
    "                p.data = localweight.data\n",
    "        #return  p.data\n",
    "        return  group['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-FedAvg Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PerFedAvgOpt(Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        defaults = dict(lr=lr)\n",
    "        super(PerFedAvgOpt, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None, beta = 0):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # print(group)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if(beta != 0):\n",
    "                    p.data.add_(-beta, d_p)\n",
    "                else:     \n",
    "                    p.data.add_(-group['lr'], d_p)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sophia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _single_tensor_sophiag(params: List[Tensor],\n",
    "                         grads: List[Tensor],\n",
    "                         exp_avgs: List[Tensor],\n",
    "                         hessian: List[Tensor],\n",
    "                         state_steps: List[Tensor],\n",
    "                         *,\n",
    "                         bs: int,\n",
    "                         beta1: float,\n",
    "                         beta2: float,\n",
    "                         rho: float,\n",
    "                         lr: float,\n",
    "                         weight_decay: float,\n",
    "                         maximize: bool,\n",
    "                         capturable: bool):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i] if not maximize else -grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        hess = hessian[i]\n",
    "        step_t = state_steps[i]\n",
    "\n",
    "        if capturable:\n",
    "            assert param.is_cuda and step_t.is_cuda and bs.is_cuda \n",
    "            \n",
    "        if torch.is_complex(param):\n",
    "            grad = torch.view_as_real(grad)\n",
    "            exp_avg = torch.view_as_real(exp_avg)\n",
    "            hess = torch.view_as_real(hess)\n",
    "            param = torch.view_as_real(param)\n",
    "\n",
    "        # update step\n",
    "        step_t += 1\n",
    "\n",
    "        # Perform stepweight decay\n",
    "        param.mul_(1 - lr * weight_decay)\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        \n",
    "        if capturable:\n",
    "            step_size = lr \n",
    "            step_size_neg = step_size.neg()\n",
    "\n",
    "            ratio = (exp_avg.abs() / (rho * bs * hess + 1e-15)).clamp(None,1)\n",
    "            param.addcmul_(exp_avg.sign(), ratio, value=step_size_neg)\n",
    "        else:\n",
    "            step_size_neg = - lr \n",
    "            \n",
    "            ratio = (exp_avg.abs() / (rho * bs * hess + 1e-15)).clamp(None,1)\n",
    "            param.addcmul_(exp_avg.sign(), ratio, value=step_size_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sophiag(params: List[Tensor],\n",
    "          grads: List[Tensor],\n",
    "          exp_avgs: List[Tensor],\n",
    "          hessian: List[Tensor],\n",
    "          state_steps: List[Tensor],\n",
    "          capturable: bool = False,\n",
    "          *,\n",
    "          bs: int,\n",
    "          beta1: float,\n",
    "          beta2: float,\n",
    "          rho: float,\n",
    "          lr: float,\n",
    "          weight_decay: float,\n",
    "          maximize: bool):\n",
    "\n",
    "    if not all(isinstance(t, torch.Tensor) for t in state_steps):\n",
    "        raise RuntimeError(\"API has changed, `state_steps` argument must contain a list of singleton tensors\")\n",
    "\n",
    "    \n",
    "    func = _single_tensor_sophiag\n",
    "\n",
    "    func(params,\n",
    "         grads,\n",
    "         exp_avgs,\n",
    "         hessian,\n",
    "         state_steps,\n",
    "         bs=bs,\n",
    "         beta1=beta1,\n",
    "         beta2=beta2,\n",
    "         rho=rho,\n",
    "         lr=lr,\n",
    "         weight_decay=weight_decay,\n",
    "         maximize=maximize,\n",
    "         capturable=capturable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SophiaG(Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.965, 0.99), rho = 0.04,\n",
    "         weight_decay=1e-1, *, maximize: bool = False,\n",
    "         capturable: bool = False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= rho:\n",
    "            raise ValueError(\"Invalid rho parameter at index 1: {}\".format(rho))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, rho=rho, \n",
    "                        weight_decay=weight_decay, \n",
    "                        maximize=maximize, capturable=capturable)\n",
    "        super(SophiaG, self).__init__(params, defaults)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def __setstate__(self: SophiaG, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('maximize', False)\n",
    "            group.setdefault('capturable', False)\n",
    "        state_values = list(self.state.values())\n",
    "        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])\n",
    "        if not step_is_tensor:\n",
    "            for s in state_values:\n",
    "                s['step'] = torch.tensor(float(s['step']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def update_hessian(self: SophiaG):\n",
    "    for group in self.param_groups:\n",
    "        beta1, beta2 = group['betas']\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "\n",
    "            if len(state) == 0:\n",
    "                state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \\\n",
    "                    if self.defaults['capturable'] else torch.tensor(0.)\n",
    "                state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "            \n",
    "            if 'hessian' not in state.keys():\n",
    "                state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "            state['hessian'].mul_(beta2).addcmul_(p.grad, p.grad, value=1 - beta2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def step(self: SophiaG, closure=None, bs=5120):\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "        with torch.enable_grad():\n",
    "            loss = closure()\n",
    "\n",
    "    for group in self.param_groups:\n",
    "        params_with_grad = []\n",
    "        grads = []\n",
    "        exp_avgs = []\n",
    "        state_steps = []\n",
    "        hessian = []\n",
    "        beta1, beta2 = group['betas']\n",
    "\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            params_with_grad.append(p)\n",
    "            \n",
    "            if p.grad.is_sparse:\n",
    "                raise RuntimeError('Hero does not support sparse gradients')\n",
    "            grads.append(p.grad)\n",
    "            state = self.state[p]\n",
    "            # State initialization\n",
    "            if len(state) == 0:\n",
    "                state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \\\n",
    "                    if self.defaults['capturable'] else torch.tensor(0.)\n",
    "                state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "            \n",
    "            if 'hessian' not in state.keys():\n",
    "                state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)                \n",
    "\n",
    "            exp_avgs.append(state['exp_avg'])\n",
    "            state_steps.append(state['step'])\n",
    "            hessian.append(state['hessian'])\n",
    "            \n",
    "            if self.defaults['capturable']:\n",
    "                bs = torch.ones((1,), dtype=torch.float, device=p.device) * bs\n",
    "\n",
    "        sophiag(params_with_grad,\n",
    "                grads,\n",
    "                exp_avgs,\n",
    "                hessian,\n",
    "                state_steps,\n",
    "                bs=bs,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                rho=group['rho'],\n",
    "                lr=group['lr'],\n",
    "                weight_decay=group['weight_decay'],\n",
    "                maximize=group['maximize'],\n",
    "                capturable=group['capturable'])\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
