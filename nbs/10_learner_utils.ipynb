{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner Utils\n",
    "\n",
    "> utils used by the learner class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *  # type: ignore # noqa: F403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from fedai.vision.VisionBlock import VisionBlock\n",
    "from fedai.vision.models import *\n",
    "from fedai.text.models import *\n",
    "from fedai.models import * # noqa: F403\n",
    "from peft import *  # type: ignore # noqa: F403\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "def get_cls(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_block(cfg, id, train=True):\n",
    "    block = VisionBlock if cfg.data.modality == ['Vision'] else None\n",
    "    return block(cfg, id, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model(cfg):\n",
    "    model_name = cfg.model.name\n",
    "\n",
    "    # Set seed before model creation to ensure the same initialization\n",
    "    torch.manual_seed(cfg.random_seed)  \n",
    "    np.random.seed(cfg.random_seed)\n",
    "    random.seed(cfg.random_seed)\n",
    "\n",
    "    # Check if the model name contains \"hf://\"\n",
    "    if model_name.startswith(\"hf://\"):\n",
    "        return get_hf_model(cfg)  # type: ignore # Call your HF model loader function  # noqa: F405\n",
    "\n",
    "    # Define the rest of the model mapping\n",
    "    mapping = {\n",
    "    \"LogisticRegression\": LogisticRegression(  \n",
    "        input_dim=getattr(cfg.model, \"dim_in\", 784),  \n",
    "        output_dim=getattr(cfg.model, \"dim_out\", 10)\n",
    "    ),\n",
    "    \"MNISTCNN\": MNISTCNN(num_classes=10),  \n",
    "    \"CIFAR10CNN\": CIFAR10CNN(num_classes=10),  \n",
    "    \n",
    "    \"MLP\": MLP(  \n",
    "        dim_in=getattr(cfg.model, \"dim_in\", 784),  \n",
    "        dim_hidden=getattr(cfg.model, \"dim_hidden\", 128),  \n",
    "        dim_out=getattr(cfg.model, \"dim_out\", 10)\n",
    "    ),\n",
    "\n",
    "    \"CharacterLSTM\": CharacterLSTM(  \n",
    "        vocab_size=getattr(cfg.model, \"vocab_size\", 50000),  \n",
    "        embed_size=getattr(cfg.model, \"embed_size\", 512),  \n",
    "        hidden_size=getattr(cfg.model, \"hidden_size\", 512),  \n",
    "        num_layers=getattr(cfg.model, \"num_layers\", 8)\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "    # Look up the model in the mapping\n",
    "    if model_name in mapping:\n",
    "        return mapping[model_name]\n",
    "    \n",
    "    raise ValueError(f\"Model '{model_name}' is not recognized, the available models are: {list(mapping.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_criterion(customm_fn):\n",
    "    if customm_fn:\n",
    "        return customm_fn\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the case of one model aggregation, we send the aggregated model back to all clients. On the other hand, Personalized FL, FMTL, ...etc uses one model per client so we need to only work on per client model case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from peft import *  # type: ignore # noqa: F403\n",
    "\n",
    "def load_state_from_disk(cfg, state, latest_round, id, t):\n",
    "    \n",
    "    if cfg.agg == \"one_model\":\n",
    "        global_model_path = os.path.join(cfg.save_dir,\n",
    "                                        str(t-1),\n",
    "                                        \"global_model\",\n",
    "                                        \"state.pth\")\n",
    "\n",
    "        gloabal_model_state = torch.load(global_model_path, weights_only= False)\n",
    "        # print(type(state[\"model\"]))\n",
    "        if isinstance(state[\"model\"], torch.nn.Module):\n",
    "            state[\"model\"].load_state_dict(gloabal_model_state[\"model\"])\n",
    "            print(f\"Loaded Global model state from {global_model_path}\")\n",
    "        else:\n",
    "            set_peft_model_state_dict(state[\"model\"],  # noqa: F405 # type: ignore\n",
    "                                      gloabal_model_state[\"model\"],\n",
    "                                      \"default\")\n",
    "\n",
    "    else:\n",
    "        if id not in latest_round:\n",
    "            return state\n",
    "\n",
    "        latest_comm_round = latest_round[id]\n",
    "        old_state_path = os.path.join(cfg.save_dir,\n",
    "                                       str(latest_comm_round),\n",
    "                                       f\"aggregated_model_{id}\",\n",
    "                                       \"state.pth\")\n",
    "\n",
    "        old_saved_state = torch.load(old_state_path, weights_only= False)\n",
    "\n",
    "        if isinstance(state[\"model\"], nn.Module) or isinstance(state[\"model\"], dict) :\n",
    "            state[\"model\"].load_state_dict(old_saved_state[\"model\"])\n",
    "            print(f\"Loaded client model state from {old_state_path}\")\n",
    "        else:\n",
    "            set_peft_model_state_dict(state[\"model\"],  # noqa: F405 # type: ignore\n",
    "                                      old_saved_state[\"model\"],\n",
    "                                      \"default\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev # type: ignore\n",
    "nbdev.nbdev_export() # type: ignore  # noqa: E702\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
