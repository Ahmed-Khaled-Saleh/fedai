{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *  # type: ignore # noqa: F403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from fedai.vision.VisionBlock import VisionBlock\n",
    "from fedai.vision.models import *\n",
    "from fedai.text.models import *\n",
    "from fedai.models import * # noqa: F403\n",
    "from peft import *  # type: ignore # noqa: F403\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "def get_cls(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_block(cfg, id, train=True):\n",
    "    block = VisionBlock if cfg.data.modality == ['Vision'] else None\n",
    "    return block(cfg, id, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model(cfg):\n",
    "    model_name = cfg.model.name\n",
    "\n",
    "    # Check if the model name contains \"hf://\"\n",
    "    if model_name.startswith(\"hf://\"):\n",
    "        return get_hf_model(cfg)  # type: ignore # Call your HF model loader function  # noqa: F405\n",
    "\n",
    "    # Define the rest of the model mapping\n",
    "    mapping = {\n",
    "    \"LogisticRegression\": LogisticRegression(  \n",
    "        input_dim=getattr(cfg.model, \"dim_in\", 784),  \n",
    "        output_dim=getattr(cfg.model, \"dim_out\", 10)\n",
    "    ),\n",
    "    \"MNISTCNN\": MNISTCNN(num_classes=10),  \n",
    "    \"CIFAR10CNN\": CIFAR10CNN(num_classes=10),  \n",
    "    \n",
    "    \"MLP\": MLP(  \n",
    "        dim_in=getattr(cfg.model, \"dim_in\", 784),  \n",
    "        dim_hidden=getattr(cfg.model, \"dim_hidden\", 128),  \n",
    "        dim_out=getattr(cfg.model, \"dim_out\", 10)\n",
    "    ),\n",
    "\n",
    "    \"CharacterLSTM\": CharacterLSTM(  \n",
    "        vocab_size=getattr(cfg.model, \"vocab_size\", 50000),  \n",
    "        embed_size=getattr(cfg.model, \"embed_size\", 512),  \n",
    "        hidden_size=getattr(cfg.model, \"hidden_size\", 512),  \n",
    "        num_layers=getattr(cfg.model, \"num_layers\", 8)\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "    # Look up the model in the mapping\n",
    "    if model_name in mapping:\n",
    "        return mapping[model_name]\n",
    "    \n",
    "    raise ValueError(f\"Model '{model_name}' is not recognized, the available models are: {list(mapping.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_criterion(customm_fn):\n",
    "    if customm_fn:\n",
    "        return customm_fn\n",
    "    else:\n",
    "        return nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the case of one model aggregation, we send the aggregated model back to all clients. On the other hand, Personalized FL, FMTL, ...etc uses one model per client so we need to only work on per client model case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_state_from_disk(cfg, state, latest_round, id, t):\n",
    "    \n",
    "    if cfg.agg == \"one_model\":\n",
    "        global_model_path = os.path.join(cfg.save_dir,\n",
    "                                        str(t-1),\n",
    "                                        \"global_model\",\n",
    "                                        \"state.pth\")\n",
    "        gloabal_model_state = torch.load(global_model_path)\n",
    "        \n",
    "        if isinstance(state[\"model\"], torch.nn.Module):\n",
    "            state[\"model\"].load_state_dict(gloabal_model_state[\"model\"])\n",
    "        else:\n",
    "            set_peft_model_state_dict(state[\"model\"],  # noqa: F405 # type: ignore\n",
    "                                      gloabal_model_state[\"model\"],\n",
    "                                      \"default\")\n",
    "        \n",
    "    else:\n",
    "        latest_comm_round = latest_round[id]\n",
    "        old_state_path = os.path.join(cfg.save_dir,\n",
    "                                       str(latest_comm_round),\n",
    "                                       f\"local_output_{id}\",\n",
    "                                       \"state.pth\")\n",
    "        \n",
    "        old_saved_state = torch.load(old_state_path)\n",
    "\n",
    "        if isinstance(state[\"model\"], torch.nn.Module):\n",
    "            state[\"model\"].load_state_dict(old_saved_state[\"model\"])\n",
    "        else:\n",
    "            set_peft_model_state_dict(state[\"model\"],  # noqa: F405 # type: ignore\n",
    "                                      old_saved_state[\"model\"],\n",
    "                                      \"default\")    \n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# def client_fn(client_cls, cfg, id, latest_round, t, loss_fn = None, optimizer = None):\n",
    "    \n",
    "#     model = get_model(cfg)\n",
    "#     criterion = get_criterion(loss_fn)\n",
    "\n",
    "#     train_block = get_block(cfg, id)\n",
    "#     test_block = get_block(cfg, id, train=False)    \n",
    "    \n",
    "#     state = {'model': model, 'optimizer': None, 'criterion': criterion, 't': t}\n",
    "    \n",
    "#     if t > 0:\n",
    "#         state = load_state_from_disk(cfg, state, latest_round, id, t)  # noqa: F405\n",
    "#         state['optimizer'] = SophiaG(model.parameters(),\n",
    "#                              lr=2e-4,\n",
    "#                              betas=(0.965, 0.99),\n",
    "#                              rho=0.01,\n",
    "#                              weight_decay=1e-1)\n",
    "        \n",
    "#     return client_cls(id, cfg, state, block= [train_block, test_block])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def client_fn(client_cls, cfg, id, latest_round, loss_fn):\n",
    "#     model = get_model(cfg)\n",
    "#     criterion = get_criterion(loss_fn)\n",
    "#     # get train and test ds\n",
    "#     train_block, test_block = get_block(cfg, id), get_block(cfg, id, train=False)\n",
    "\n",
    "#     state = {'model': model, 'optimizer': None, 'criterion': criterion}\n",
    "\n",
    "#     if id in latest_round:\n",
    "#         comm_round = latest_round[id]\n",
    "#         state['model'] = load_state_from_disk(cfg, model, id, comm_round)  # noqa: F405\n",
    "    \n",
    "#     return client_cls(id, cfg, state, block= [train_block, test_block])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # type: ignore  # noqa: E702\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
