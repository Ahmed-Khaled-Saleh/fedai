{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.core\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ujson\n",
    "import h5py\n",
    "from fastcore.utils import *\n",
    "from fedai.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDownloader:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.config_path = os.path.join(self.cfg.data.dir_path, self.cfg.data.name , \"config.json\")\n",
    "        self.train_path = os.path.join(self.cfg.data.dir_path, self.cfg.data.name , \"train\")\n",
    "        self.test_path = os.path.join(self.cfg.data.dir_path, self.cfg.data.name, \"test\")\n",
    "        print(self.train_path, self.test_path)\n",
    "        self.partitioner_obj = get_class('fedai.data.partitioners', self.cfg.data.partitioner)(self.cfg) # noqa: F405\n",
    "        self.dataidx_map = {}\n",
    "        \n",
    "        self.data = self.load_data()\n",
    "        self.dataset_content, self.dataset_label = self.data if self.data is not None else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def check(self: BaseDownloader):\n",
    "    # check existing dataset\n",
    "    if os.path.exists(self.config_path):\n",
    "        with open(self.config_path, 'r') as f:\n",
    "            config = ujson.load(f)\n",
    "        \n",
    "        if config['num_clients'] == self.cfg.num_clients and \\\n",
    "            config['non_iid'] == self.cfg.data.niid and \\\n",
    "            config['balance'] == self.cfg.data.balance and \\\n",
    "            config['partition'] == self.cfg.data.partitioner and \\\n",
    "            config['alpha'] == self.cfg.data.alpha and \\\n",
    "            config['batch_size'] == self.cfg.data.batch_size:\n",
    "            print(\"\\nDataset already generated.\\n\")\n",
    "            return True\n",
    "\n",
    "    print(f\"\\nDataset not found, Downloading the dataset: {self.cfg.data.name}.\\n\")\n",
    "   \n",
    "    if not os.path.exists(self.train_path):\n",
    "        os.makedirs(self.train_path)\n",
    "\n",
    "    if not os.path.exists(self.test_path):\n",
    "        os.makedirs(self.test_path)\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load_data(self: BaseDownloader):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def split_data(self: BaseDownloader, X, y):\n",
    "    # Split dataset into train and test sets\n",
    "    # make a list of train and test data where each element is a dictionary of x and y\n",
    "    # each element in the list is a client's data\n",
    "    train_data, test_data = [], []\n",
    "    num_samples = {'train':[], 'test':[]}\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X[i], y[i], train_size= self.cfg.data.train_ratio, shuffle= True)\n",
    "\n",
    "        train_data.append({'x': X_train, 'y': y_train})\n",
    "        num_samples['train'].append(len(y_train))\n",
    "        test_data.append({'x': X_test, 'y': y_test})\n",
    "        num_samples['test'].append(len(y_test))\n",
    "\n",
    "    print(\"Total number of samples:\", sum(num_samples['train'] + num_samples['test']))\n",
    "    print(\"The number of train samples:\", num_samples['train'])\n",
    "    print(\"The number of test samples:\", num_samples['test'])\n",
    "    print()\n",
    "    del X, y\n",
    "\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def partition(self: BaseDownloader):\n",
    "    # choose a paritioning method (iid, noniid, ext-nonidd) and split the data into train and test sets for all clients\n",
    "    least_samples = int(min(self.cfg.data.batch_size / (1- self.cfg.data.train_ratio), len(self.dataset_label) / self.cfg.num_clients / 2))\n",
    "    X, y, statistic = self.partitioner_obj.partition(self.dataset_content, self.dataset_label, least_samples, self.dataidx_map)\n",
    "    \n",
    "    for client in range(self.cfg.num_clients):\n",
    "        print(f\"Client {client}\\t Size of data: {len(X[client])}\\t Labels: \", np.unique(y[client]))\n",
    "        print(\"\\t\\t Samples of labels: \", [i for i in statistic[client]])\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    del self.data\n",
    "\n",
    "    train_data, test_data = self.split_data(X, y)\n",
    "    return train_data, test_data, statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_partitions(self: BaseDownloader, train_data, test_data, statistic):\n",
    "    \"\"\"\n",
    "    Save partitions using HDF5 format.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Configuration object.\n",
    "        train_data: List of dictionaries for training data.\n",
    "        test_data: List of dictionaries for test data.\n",
    "        statistic: Statistical information to save.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'num_clients': self.cfg.num_clients,\n",
    "        'num_classes': self.cfg.data.num_classes,\n",
    "        'non_iid': self.cfg.data.niid,\n",
    "        'balance': self.cfg.data.balance,\n",
    "        'partition': self.cfg.data.partitioner,\n",
    "        'Size of samples for labels in clients': statistic,\n",
    "        'alpha': self.cfg.data.alpha,\n",
    "        'batch_size': self.cfg.data.batch_size,\n",
    "    }\n",
    "\n",
    "    print(\"Saving to disk in HDF5 format.\\n\")\n",
    "\n",
    "    # Save training data\n",
    "    with h5py.File(os.path.join(self.train_path, 'train_data.h5'), 'w') as train_h5:\n",
    "        for idx, train_dict in enumerate(train_data):\n",
    "            group = train_h5.create_group(f'client_{idx}')\n",
    "            for key, value in train_dict.items():\n",
    "                group.create_dataset(key, data=value)\n",
    "\n",
    "    # Save test data\n",
    "    with h5py.File(os.path.join(self.test_path, 'test_data.h5'), 'w') as test_h5:\n",
    "        for idx, test_dict in enumerate(test_data):\n",
    "            group = test_h5.create_group(f'client_{idx}')\n",
    "            for key, value in test_dict.items():\n",
    "                group.create_dataset(key, data=value)\n",
    "\n",
    "    # Save configuration as a JSON file\n",
    "    with open(self.config_path, 'w') as f:\n",
    "        ujson.dump(config, f)\n",
    "\n",
    "    self.save_space(train_data, test_data, statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_space(self: BaseDownloader, train_data, test_data, statistic):\n",
    "    import gc\n",
    "    del self.dataset_content\n",
    "    del self.dataset_label\n",
    "    del train_data\n",
    "    del test_data\n",
    "    del statistic\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def save_partitions_np(self: BaseDownloader, cfg, train_data, test_data, statistic):\n",
    "    \n",
    "    config = {\n",
    "        'num_clients': cfg.num_clients, \n",
    "        'num_classes': cfg.data.num_classes, \n",
    "        'non_iid': cfg.data.niid, \n",
    "        'balance': cfg.data.balance, \n",
    "        'partition': cfg.data.partitioner, \n",
    "        'Size of samples for labels in clients': statistic, \n",
    "        'alpha': self.cfg.data.alpha, \n",
    "        'batch_size': self.cfg.data.batch_size, \n",
    "    }\n",
    "\n",
    "    print(\"Saving to disk.\\n\")\n",
    "\n",
    "    for idx, train_dict in enumerate(train_data):\n",
    "        with open(self.train_path + str(idx) + '.npz', 'wb') as f:\n",
    "            np.savez_compressed(f, data=train_dict)\n",
    "\n",
    "    for idx, test_dict in enumerate(test_data):\n",
    "        with open(self.test_path + str(idx) + '.npz', 'wb') as f:\n",
    "            np.savez_compressed(f, data=test_dict)\n",
    "        \n",
    "    with open(self.config_path, 'w') as f:\n",
    "        ujson.dump(config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def tensorify(self: BaseDownloader, data):\n",
    "    X = torch.Tensor(data['x']).type(torch.float32)\n",
    "    y = torch.Tensor(data['y']).type(torch.int64)\n",
    "    return {'x': X, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load_partition(self: BaseDownloader, idx, split= 'train'):\n",
    "    # loads the data for a client indexed by idx. \n",
    "    # By default it loads the training data but can also load the test data when split is set to 'test'\n",
    "    data_dir = os.path.join(self.train_path) if split == 'train' else os.path.join(self.test_path)\n",
    "\n",
    "    train_file = os.path.join(data_dir, str(idx) + '.npz')\n",
    "    with open(train_file, 'rb') as f:\n",
    "        data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "    \n",
    "    data_dict = self.tensorify(data)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load_split(self: BaseDownloader, split= 'train'):\n",
    "    # load the data of all clients. By default, it loads the training data for all clients\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ahmed/Ahmed-home/1- Projects/Research/publications/2024/letter 1/code/PFLlib/dataset/Cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLMDataCollator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ahmed/Ahmed-home/1- Projects/Research/publications/2024/letter 1/code/mira/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
