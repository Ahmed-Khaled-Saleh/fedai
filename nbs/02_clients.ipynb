{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clients\n",
    "\n",
    "> The core abstraction for different FL Clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import os\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from peft import *\n",
    "from fedai.trainers import *\n",
    "from fedai.utils import get_class\n",
    "from fedai.data import LLMDataCollator\n",
    "from transformers import AutoTokenizer\n",
    "from omegaconf.dictconfig import DictConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exportsend, aggregate\n",
    "import torch\n",
    "\n",
    "class BaseClient:\n",
    "    '''A base FL client.\\n\n",
    "        data_dict: A dictionary that contains the train and test data sets. keys: (train, test)\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 data_dict: dict,\n",
    "                 model: torch.nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 idx: int) -> None : \n",
    "        \n",
    "        self.train_ds = data_dict['train']\n",
    "        self.test_ds = data_dict['test']\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.idx = idx\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            setattr(self, key, value)# client now has a data set object for train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adjust the string reprsntation of the client abstraction to make it more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch #allows us to add a method to an existing class\n",
    "def __str__(self: BaseClient) -> str:\n",
    "    return f'''Client: {self.__class__.__name__}\n",
    "    Index : {self.idx}\n",
    "    Model: {self.model.__class__.__name__}\n",
    "    Criterion: {self.criterion.__class__.__name__}\n",
    "    Optimizer: {self.optimizer.__class__.__name__}'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every client abstraction, whether it a base or any other type of federated client, it will initalize the training locally with a set of steps. This might include things like extracting the eft model out of the base model (in the case of LLMs clients). Also, it will terminate the local training with some steps, like saving the model state dictionary and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def init_local_train(self: BaseClient):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@patch\n",
    "def terminate_local_train(self: BaseClient):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@patch\n",
    "def clear_model(self: BaseClient):\n",
    "    self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaseClient\n",
       "\n",
       ">      BaseClient (DataDict:dict, model:torch.nn.modules.module.Module,\n",
       ">                  criterion:int, optimizer:torch.optim.optimizer.Optimizer,\n",
       ">                  idx:int)\n",
       "\n",
       "*A base FL client.\n",
       "\n",
       "DataDict: A dictionary that contains the train and test data sets. keys: (train, test)*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaseClient\n",
       "\n",
       ">      BaseClient (DataDict:dict, model:torch.nn.modules.module.Module,\n",
       ">                  criterion:int, optimizer:torch.optim.optimizer.Optimizer,\n",
       ">                  idx:int)\n",
       "\n",
       "*A base FL client.\n",
       "\n",
       "DataDict: A dictionary that contains the train and test data sets. keys: (train, test)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaseClient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the BaseClient Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "class RandomTwoCaseDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, input_size=10, case_prob=0.5, transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.input_size = input_size\n",
    "        self.case_prob = case_prob\n",
    "        self.transform = transform\n",
    "        self.data, self.labels = self._generate_data()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        \"\"\"Generates random data for two cases.\"\"\"\n",
    "        data = torch.randn(self.num_samples, self.input_size)  # Random data\n",
    "        labels = torch.zeros(self.num_samples, dtype=torch.long)  # Labels (0 or 1)\n",
    "\n",
    "        # Assign case 1 based on case_prob\n",
    "        case_1_indices = torch.rand(self.num_samples) < self.case_prob\n",
    "        labels[case_1_indices] = 1  # Assign case 1 (label=1) to some samples\n",
    "\n",
    "        # Modify data to differ based on the case label\n",
    "        data[labels == 0] *= 1.5  # Modify case 0 samples\n",
    "        data[labels == 1] += 2.0  # Modify case 1 samples\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# Create full dataset\n",
    "full_dataset = RandomTwoCaseDataset(num_samples=2000, input_size=5, case_prob=0.5)\n",
    "\n",
    "# Split into train and test datasets\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "test_size = len(full_dataset) - train_size  # 20% for testing\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "item_shape = train_dataset[0][0].shape[0]\n",
    "item_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from einops import repeat\n",
    "test_item = train_dataset[0][0]\n",
    "test_item =  repeat(test_item, 'l -> b l', b=1) # add a batch dimnsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from torch import nn\n",
    "class SimpleModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model = SimpleModel()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params= model.parameters(), lr= 0.01)\n",
    "DataDict = {\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: BaseClient\n",
      "    Index : 0\n",
      "    Model: SimpleModel\n",
      "    Criterion: CrossEntropyLoss\n",
      "    Optimizer: Adam\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "dummy_client= BaseClient(DataDict, model,criterion, optimizer, 0)\n",
    "print(dummy_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRA Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira clients have more parameters. Since it's a client for LLM in principle, we need to feed the generation dataset (the dataset of text ids at the end layer not the logits). Also, a tokenizer and a collate function that will be used for the generation and the data loader construction processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Client_mira(BaseClient):\n",
    "    def __init__(self,\n",
    "                 data_dict: dict,\n",
    "                 model: torch.nn.Module,\n",
    "                 criterion,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 idx: int,\n",
    "                 gen_data_dict: dict,\n",
    "                 tokenizer: AutoTokenizer,\n",
    "                 collat_fn: LLMDataCollator,\n",
    "                 cfg: DictConfig) -> None:\n",
    "            \n",
    "        super().__init__(data_dict, model, criterion, optimizer, idx)\n",
    "        \n",
    "        self.train_ds_genr = gen_data_dict['train']\n",
    "        self.test_ds_genr = gen_data_dict['test']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.collat_fn = collat_fn\n",
    "        self.cfg = cfg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to save space, we will replace the original model with only the trainable peft model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch \n",
    "def init_local_train(self: Client_mira, out_dir):\n",
    "\n",
    "    self.output_dir = out_dir\n",
    "    self.params_dict_old = deepcopy(\n",
    "        OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if\n",
    "                    \"default\" in name))\n",
    "    \n",
    "    self.params_dict_new = OrderedDict((name, param.detach()) for name, param in self.model.named_parameters() if\n",
    "                                        \"default\" in name)\n",
    "    \n",
    "    self.model.state_dict = (\n",
    "        lambda instance, *_, **__: get_peft_model_state_dict(\n",
    "            instance, self.params_dict_new, \"default\"\n",
    "        )\n",
    "    ).__get__(self.model, type(self.model))\n",
    "\n",
    "    self.optimizer = get_class('torch.optim', self.cfg.optimizer)(self.model.parameters(), lr= self.cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train(self: Client_mira, n_epochs):\n",
    "    self.model.train()\n",
    "    trainer = Trainer(self)\n",
    "    history = trainer.fit(n_epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def clear_model(self: Client_mira):\n",
    "    self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def terminate_local_train(self: Client_mira, epoch, local_dataset_len_dict, previously_selected_clients_set):\n",
    "\n",
    "    local_dataset_len_dict[self.idx] = len(self.train_ds)\n",
    "    new_adapter_weight = self.model.state_dict()\n",
    "    single_output_dir = os.path.join(self.output_dir, str(epoch), \"local_output_{}\".format(self.idx))\n",
    "    os.makedirs(single_output_dir, exist_ok=True)\n",
    "    torch.save(new_adapter_weight, single_output_dir + \"/pytorch_model.bin\")\n",
    "\n",
    "    older_adapter_weight = get_peft_model_state_dict(self.model, self.params_dict_old, \"default\")\n",
    "    set_peft_model_state_dict(self.model, older_adapter_weight, \"default\")\n",
    "    previously_selected_clients_set = previously_selected_clients_set | set({self.idx})\n",
    "    last_client_id = self.idx\n",
    "\n",
    "    return self.model, local_dataset_len_dict, previously_selected_clients_set, last_client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Mira Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following:\n",
    "- Define a Mira client.\n",
    "- inspect the `init_local_train` and `terminate_local_train` methods and their effect on the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# base_model = deepcopy(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/fedai/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "# config = LoraConfig(\n",
    "#     r=8,# arbitrary numbr but usually 8, 16, 32, 64, 128\n",
    "#     target_modules=['c_attn'],\n",
    "#     lora_alpha=8,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "\n",
    "# peft_model = get_peft_model(gpt2, config)\n",
    "# mira  = Client_mira(DataDict, peft_model, criterion, optimizer, 0, train_dataset, test_dataset, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inpect the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to observe the difference of architecture that we get from peft_model vs base_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "# mira.init_local_train('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# mira.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the lengths of the keys of the state dictionaries of the two models, you find out the Lora model has fewer keys. In fact, those are the only trainable parameters that e have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 149)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #| hide\n",
    "# len(mira.model.state_dict()), len(base_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the keys of the PeftModel are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- base_model.model.transformer.h.0.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.0.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.1.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.1.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.2.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.2.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.3.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.3.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.4.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.4.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.5.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.5.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.6.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.6.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.7.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.7.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.8.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.8.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.9.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.9.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.10.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.10.attn.c_attn.lora_B.weight\n",
       "- base_model.model.transformer.h.11.attn.c_attn.lora_A.weight\n",
       "- base_model.model.transformer.h.11.attn.c_attn.lora_B.weight"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #| hide\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# keys_list = \"\\n\".join(f\"- {key}\" for key in mira.model.state_dict().keys())\n",
    "# display(Markdown(keys_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- transformer.wte.weight\n",
       "- transformer.wpe.weight\n",
       "- transformer.h.0.ln_1.weight\n",
       "- transformer.h.0.ln_1.bias\n",
       "- transformer.h.0.attn.c_attn.weight\n",
       "- transformer.h.0.attn.c_attn.bias\n",
       "- transformer.h.0.attn.c_proj.weight\n",
       "- transformer.h.0.attn.c_proj.bias\n",
       "- transformer.h.0.ln_2.weight\n",
       "- transformer.h.0.ln_2.bias\n",
       "- transformer.h.0.mlp.c_fc.weight\n",
       "- transformer.h.0.mlp.c_fc.bias\n",
       "- transformer.h.0.mlp.c_proj.weight\n",
       "- transformer.h.0.mlp.c_proj.bias\n",
       "- transformer.h.1.ln_1.weight\n",
       "- transformer.h.1.ln_1.bias\n",
       "- transformer.h.1.attn.c_attn.weight\n",
       "- transformer.h.1.attn.c_attn.bias\n",
       "- transformer.h.1.attn.c_proj.weight\n",
       "- transformer.h.1.attn.c_proj.bias\n",
       "- transformer.h.1.ln_2.weight\n",
       "- transformer.h.1.ln_2.bias\n",
       "- transformer.h.1.mlp.c_fc.weight\n",
       "- transformer.h.1.mlp.c_fc.bias\n",
       "- transformer.h.1.mlp.c_proj.weight\n",
       "- transformer.h.1.mlp.c_proj.bias\n",
       "- transformer.h.2.ln_1.weight\n",
       "- transformer.h.2.ln_1.bias\n",
       "- transformer.h.2.attn.c_attn.weight\n",
       "- transformer.h.2.attn.c_attn.bias\n",
       "- transformer.h.2.attn.c_proj.weight\n",
       "- transformer.h.2.attn.c_proj.bias\n",
       "- transformer.h.2.ln_2.weight\n",
       "- transformer.h.2.ln_2.bias\n",
       "- transformer.h.2.mlp.c_fc.weight\n",
       "- transformer.h.2.mlp.c_fc.bias\n",
       "- transformer.h.2.mlp.c_proj.weight\n",
       "- transformer.h.2.mlp.c_proj.bias\n",
       "- transformer.h.3.ln_1.weight\n",
       "- transformer.h.3.ln_1.bias\n",
       "- transformer.h.3.attn.c_attn.weight\n",
       "- transformer.h.3.attn.c_attn.bias\n",
       "- transformer.h.3.attn.c_proj.weight\n",
       "- transformer.h.3.attn.c_proj.bias\n",
       "- transformer.h.3.ln_2.weight\n",
       "- transformer.h.3.ln_2.bias\n",
       "- transformer.h.3.mlp.c_fc.weight\n",
       "- transformer.h.3.mlp.c_fc.bias\n",
       "- transformer.h.3.mlp.c_proj.weight\n",
       "- transformer.h.3.mlp.c_proj.bias\n",
       "- transformer.h.4.ln_1.weight\n",
       "- transformer.h.4.ln_1.bias\n",
       "- transformer.h.4.attn.c_attn.weight\n",
       "- transformer.h.4.attn.c_attn.bias\n",
       "- transformer.h.4.attn.c_proj.weight\n",
       "- transformer.h.4.attn.c_proj.bias\n",
       "- transformer.h.4.ln_2.weight\n",
       "- transformer.h.4.ln_2.bias\n",
       "- transformer.h.4.mlp.c_fc.weight\n",
       "- transformer.h.4.mlp.c_fc.bias\n",
       "- transformer.h.4.mlp.c_proj.weight\n",
       "- transformer.h.4.mlp.c_proj.bias\n",
       "- transformer.h.5.ln_1.weight\n",
       "- transformer.h.5.ln_1.bias\n",
       "- transformer.h.5.attn.c_attn.weight\n",
       "- transformer.h.5.attn.c_attn.bias\n",
       "- transformer.h.5.attn.c_proj.weight\n",
       "- transformer.h.5.attn.c_proj.bias\n",
       "- transformer.h.5.ln_2.weight\n",
       "- transformer.h.5.ln_2.bias\n",
       "- transformer.h.5.mlp.c_fc.weight\n",
       "- transformer.h.5.mlp.c_fc.bias\n",
       "- transformer.h.5.mlp.c_proj.weight\n",
       "- transformer.h.5.mlp.c_proj.bias\n",
       "- transformer.h.6.ln_1.weight\n",
       "- transformer.h.6.ln_1.bias\n",
       "- transformer.h.6.attn.c_attn.weight\n",
       "- transformer.h.6.attn.c_attn.bias\n",
       "- transformer.h.6.attn.c_proj.weight\n",
       "- transformer.h.6.attn.c_proj.bias\n",
       "- transformer.h.6.ln_2.weight\n",
       "- transformer.h.6.ln_2.bias\n",
       "- transformer.h.6.mlp.c_fc.weight\n",
       "- transformer.h.6.mlp.c_fc.bias\n",
       "- transformer.h.6.mlp.c_proj.weight\n",
       "- transformer.h.6.mlp.c_proj.bias\n",
       "- transformer.h.7.ln_1.weight\n",
       "- transformer.h.7.ln_1.bias\n",
       "- transformer.h.7.attn.c_attn.weight\n",
       "- transformer.h.7.attn.c_attn.bias\n",
       "- transformer.h.7.attn.c_proj.weight\n",
       "- transformer.h.7.attn.c_proj.bias\n",
       "- transformer.h.7.ln_2.weight\n",
       "- transformer.h.7.ln_2.bias\n",
       "- transformer.h.7.mlp.c_fc.weight\n",
       "- transformer.h.7.mlp.c_fc.bias\n",
       "- transformer.h.7.mlp.c_proj.weight\n",
       "- transformer.h.7.mlp.c_proj.bias\n",
       "- transformer.h.8.ln_1.weight\n",
       "- transformer.h.8.ln_1.bias\n",
       "- transformer.h.8.attn.c_attn.weight\n",
       "- transformer.h.8.attn.c_attn.bias\n",
       "- transformer.h.8.attn.c_proj.weight\n",
       "- transformer.h.8.attn.c_proj.bias\n",
       "- transformer.h.8.ln_2.weight\n",
       "- transformer.h.8.ln_2.bias\n",
       "- transformer.h.8.mlp.c_fc.weight\n",
       "- transformer.h.8.mlp.c_fc.bias\n",
       "- transformer.h.8.mlp.c_proj.weight\n",
       "- transformer.h.8.mlp.c_proj.bias\n",
       "- transformer.h.9.ln_1.weight\n",
       "- transformer.h.9.ln_1.bias\n",
       "- transformer.h.9.attn.c_attn.weight\n",
       "- transformer.h.9.attn.c_attn.bias\n",
       "- transformer.h.9.attn.c_proj.weight\n",
       "- transformer.h.9.attn.c_proj.bias\n",
       "- transformer.h.9.ln_2.weight\n",
       "- transformer.h.9.ln_2.bias\n",
       "- transformer.h.9.mlp.c_fc.weight\n",
       "- transformer.h.9.mlp.c_fc.bias\n",
       "- transformer.h.9.mlp.c_proj.weight\n",
       "- transformer.h.9.mlp.c_proj.bias\n",
       "- transformer.h.10.ln_1.weight\n",
       "- transformer.h.10.ln_1.bias\n",
       "- transformer.h.10.attn.c_attn.weight\n",
       "- transformer.h.10.attn.c_attn.bias\n",
       "- transformer.h.10.attn.c_proj.weight\n",
       "- transformer.h.10.attn.c_proj.bias\n",
       "- transformer.h.10.ln_2.weight\n",
       "- transformer.h.10.ln_2.bias\n",
       "- transformer.h.10.mlp.c_fc.weight\n",
       "- transformer.h.10.mlp.c_fc.bias\n",
       "- transformer.h.10.mlp.c_proj.weight\n",
       "- transformer.h.10.mlp.c_proj.bias\n",
       "- transformer.h.11.ln_1.weight\n",
       "- transformer.h.11.ln_1.bias\n",
       "- transformer.h.11.attn.c_attn.weight\n",
       "- transformer.h.11.attn.c_attn.bias\n",
       "- transformer.h.11.attn.c_proj.weight\n",
       "- transformer.h.11.attn.c_proj.bias\n",
       "- transformer.h.11.ln_2.weight\n",
       "- transformer.h.11.ln_2.bias\n",
       "- transformer.h.11.mlp.c_fc.weight\n",
       "- transformer.h.11.mlp.c_fc.bias\n",
       "- transformer.h.11.mlp.c_proj.weight\n",
       "- transformer.h.11.mlp.c_proj.bias\n",
       "- transformer.ln_f.weight\n",
       "- transformer.ln_f.bias\n",
       "- lm_head.weight"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #| hide\n",
    "# from IPython.display import display, Markdown\n",
    "\n",
    "# keys_list = \"\\n\".join(f\"- {key}\" for key in base_model.state_dict().keys())\n",
    "# display(Markdown(keys_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleint FedIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
