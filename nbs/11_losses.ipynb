{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss functions\n",
    "\n",
    "> ِAny custom losses should be here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *  # type: ignore # noqa: F403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch, os, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastcore.utils import * # type: ignore # noqa: F403\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AnchorLoss(nn.Module):\n",
    "    def __init__(self, num_classes, feature_num, t=1, h_c=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_num = feature_num\n",
    "        self.anchor = nn.Parameter(F.normalize(torch.randn(num_classes, feature_num)), requires_grad=True)\n",
    "        \n",
    "        if t > 1 and h_c is not None:\n",
    "            with torch.no_grad():  # This ensures the operation doesn't track gradients\n",
    "                h_c = h_c.to(self.anchor.device)\n",
    "                self.anchor.copy_(h_c)\n",
    "        # self.anchor = nn.Parameter(h_c, requires_grad= True) if t > 1 and h_c  else self.anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: AnchorLoss, feature, _target, Lambda = 0.1):\n",
    "    assert not torch.isnan(_target).any(), \"Found NaN in _target!\"\n",
    "    # broadcast feature anchors for all inputs\n",
    "    centre = self.anchor.cuda().index_select(dim=0, index=_target.long())\n",
    "    # compute the number of samples in each class\n",
    "    counter = torch.histc(_target.cpu().float(), bins=self.num_classes, min=0, max=self.num_classes-1)\n",
    "    counter = counter.to(_target.device)  # Move back to the same device as _target\n",
    "    count = counter[_target.long()]\n",
    "    centre_dis = feature - centre\t\t\t\t# compute distance between input and anchors\n",
    "    pow_ = torch.pow(centre_dis, 2)\t\t\t\t# squre\n",
    "    sum_1 = torch.sum(pow_, dim=1)\n",
    "    count = count.clamp(min=1)  # Avoid division by zero\n",
    "    dis_ = sum_1 / count.float()\t\t\t\t# sum all distance\n",
    "    # dis_ = torch.div(sum_1, count.float())\t\t# mean by class\n",
    "    sum_2 = torch.sum(dis_)/self.num_classes\t\t\t\t\t\t# mean loss\n",
    "    res = Lambda*sum_2   \t\t\t\t\t\t\t# time hyperparameter lambda \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
