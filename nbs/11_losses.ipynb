{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss functions\n",
    "\n",
    "> ŸêAny custom losses should be here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *  # type: ignore # noqa: F403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch, os, random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastcore.utils import * # type: ignore # noqa: F403\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AnchorLoss(nn.Module):\n",
    "    def __init__(self, num_classes, feature_num):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_num = feature_num\n",
    "        self.anchor = nn.Parameter(F.normalize(torch.randn(num_classes, feature_num)), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: AnchorLoss, feature, _target, Lambda = 0.1):\n",
    "    assert not torch.isnan(_target).any(), \"Found NaN in _target!\"\n",
    "    # broadcast feature anchors for all inputs\n",
    "    centre = self.anchor.cuda().index_select(dim=0, index=_target.long())\n",
    "    print(\"Target:\", _target)\n",
    "    print(\"Target dtype:\", _target.dtype)\n",
    "    print(\"Target shape:\", _target.shape)\n",
    "\n",
    "    # compute the number of samples in each class\n",
    "    counter = torch.histc(_target.cpu().float(), bins=self.num_classes, min=0, max=self.num_classes-1)\n",
    "    counter = counter.to(_target.device)  # Move back to the same device as _target\n",
    "    count = counter[_target.long()]\n",
    "    centre_dis = feature - centre\t\t\t\t# compute distance between input and anchors\n",
    "    pow_ = torch.pow(centre_dis, 2)\t\t\t\t# squre\n",
    "    sum_1 = torch.sum(pow_, dim=1)\n",
    "    count = count.clamp(min=1)  # Avoid division by zero\n",
    "    dis_ = sum_1 / count.float()\t\t\t\t# sum all distance\n",
    "    # dis_ = torch.div(sum_1, count.float())\t\t# mean by class\n",
    "    sum_2 = torch.sum(dis_)/self.num_classes\t\t\t\t\t\t# mean loss\n",
    "    res = Lambda*sum_2   \t\t\t\t\t\t\t# time hyperparameter lambda \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 2, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        2, 0, 1, 2, 0, 0, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "@patch\n",
    "def forward(self: AnchorLoss, feature, _target, Lambda = 0.1):\n",
    "    assert not torch.isnan(_target).any(), \"Found NaN in _target!\"\n",
    "    # broadcast feature anchors for all inputs\n",
    "    centre = self.anchor.cuda().index_select(dim=0, index=_target.long())\n",
    "    print(\"Target:\", _target)\n",
    "    print(\"Target dtype:\", _target.dtype)\n",
    "    print(\"Target shape:\", _target.shape)\n",
    "\n",
    "    # compute the number of samples in each class\n",
    "    counter = torch.histc(_target.cpu().float(), bins=self.num_classes, min=0, max=self.num_classes-1)\n",
    "    counter = counter.to(_target.device)  # Move back to the same device as _target\n",
    "    count = counter[_target.long()]\n",
    "    centre_dis = feature - centre\t\t\t\t# compute distance between input and anchors\n",
    "    pow_ = torch.pow(centre_dis, 2)\t\t\t\t# squre\n",
    "    sum_1 = torch.sum(pow_, dim=1)\n",
    "    count = count.clamp(min=1)  # Avoid division by zero\n",
    "    dis_ = sum_1 / count.float()\t\t\t\t# sum all distance\n",
    "    # dis_ = torch.div(sum_1, count.float())\t\t# mean by class\n",
    "    sum_2 = torch.sum(dis_)/self.num_classes\t\t\t\t\t\t# mean loss\n",
    "    res = Lambda*sum_2   \t\t\t\t\t\t\t# time hyperparameter lambda \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
