{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vision.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils.dataset_utils import check, separate_data, split_data, save_file\n",
    "from fedai.data import *\n",
    "from fedai.utils import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FvisionBlock(FDblock):  # type: ignore # noqa: F405\n",
    "    def __init__(self, cfg, partitioner: str):\n",
    "        super().__init__(cfg, partitioner)\n",
    "\n",
    "        self.transform_norm_mapping = {\n",
    "            \"CIFAR10\": ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            \"CIFAR100\": ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            \"MNIST\": ((0.5,), (0.5,)),\n",
    "            \"FashionMNIST\": ((0.5,), (0.5,)),\n",
    "            \"EMNIST\": ((0.5,), (0.5,)),\n",
    "        }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def load_data(self: FvisionBlock):\n",
    "    if not os.path.exists(self.cfg.data.dir_path):\n",
    "        os.makedirs(self.cfg.data.dir_path)\n",
    "    \n",
    "    ds_class = get_class(torchvision.datasets, self.cfg.data.name)  # noqa: F405\n",
    "    # Setup directory for train/test data\n",
    "\n",
    "    if check(self.config_path, self.train_path, self.test_path, self.cfg.num_clients, self.cfg.data.niid, self.cfg.data.balance, self.cfg.data.partition):\n",
    "        return\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), \n",
    "        transforms.Normalize(self.transform_norm_mapping[self.cfg.data.name][0],\n",
    "                             self.transform_norm_mapping[self.cfg.data.name][1])])\n",
    "\n",
    "    trainset = ds_class(\n",
    "        root=self.cfg.data.dir_path+\"rawdata\", train=True, download=True, transform=transform)\n",
    "    testset = ds_class(\n",
    "        root=self.cfg.data.dir_path+\"rawdata\", train=False, download=True, transform=transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=len(trainset.data), shuffle=False)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=len(testset.data), shuffle=False)\n",
    "\n",
    "    for _, train_data in enumerate(trainloader, 0):\n",
    "        trainset.data, trainset.targets = train_data\n",
    "    for _, test_data in enumerate(testloader, 0):\n",
    "        testset.data, testset.targets = test_data\n",
    "\n",
    "    dataset_image = []\n",
    "    dataset_label = []\n",
    "\n",
    "    dataset_image.extend(trainset.data.cpu().detach().numpy())\n",
    "    dataset_image.extend(testset.data.cpu().detach().numpy())\n",
    "    dataset_label.extend(trainset.targets.cpu().detach().numpy())\n",
    "    dataset_label.extend(testset.targets.cpu().detach().numpy())\n",
    "    dataset_image = np.array(dataset_image)\n",
    "    dataset_label = np.array(dataset_label)\n",
    "\n",
    "    num_classes = len(set(dataset_label))\n",
    "    print(f'Number of classes: {num_classes}')\n",
    "\n",
    "    return dataset_image, dataset_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ahmed/Ahmed-home/1- Projects/Research/publications/2024/letter 1/code/PFLlib/dataset/Cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, idx, is_train=True):\n",
    "    if is_train:\n",
    "        train_data_dir = os.path.join(path, 'train')\n",
    "\n",
    "        train_file = os.path.join(train_data_dir, str(idx) + '.npz')\n",
    "        with open(train_file, 'rb') as f:\n",
    "            train_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "\n",
    "        return train_data\n",
    "\n",
    "    else:\n",
    "        test_data_dir = os.path.join(path, 'test/')\n",
    "\n",
    "        test_file = test_data_dir + str(idx) + '.npz'\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "    \n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/ahmed/Ahmed-home/1- Projects/Research/publications/2024/letter 1/code/mira/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
