{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainers\n",
    "\n",
    "> Implementing the training loop for CPU/GPU training in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from fedai.metrics import *\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trainer class implements the training loop with the `fit()` method. It accepts a client at initialization, which contains all necessary infromation \n",
    "to implement a training loop for a federated learning setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modularity, we implement two trainers, a single device (cpu or gpu), and a distributed trainer (multi-gpu, single node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of the Trainer\n",
    "\n",
    "The basic building blocks for the trainer are : \n",
    "- `_run_batch`: A method that accepts a batch of data as input, and returns the logits and the loss resulting from the forward pass.\n",
    "- `_run_epoch`: A method that iterate over the whole dataset, and calls the `_run_batch` function within its logic.\n",
    "- `fit`: A function that run a loop over a `n_epochs` to excute multiple epochs. It calls the `_run_epoch` method every iteration.\n",
    " \n",
    "  \n",
    "The `fit` method returns a `history` object, which is a `dict` containing the summary statistics of both the train and test datasets performance in the current federated round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Trainer:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.cfg = client.cfg\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.client.train_loader = self.prepare_dataloader(self.client.train_ds, self.cfg.batch_size)\n",
    "        self.client.test_loader = self.prepare_dataloader(self.client.test_ds, self.cfg.batch_size)\n",
    "\n",
    "        self.training_metrics = Metrics(self.cfg.training_metrics)\n",
    "        self.test_metrics = Metrics(self.cfg.test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the difference that it takes to prepare the dataset for sinle device vs multi-device training, we make a method that handles this separately. `prepare_dl` prepares the dataloader needed for the trainer's type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def prepare_dl(self: Trainer, ds):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size= self.cfg.batch_size,\n",
    "        shuffle= True,\n",
    "        collate_fn= self.client.collate_fn        \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since different model have different input shapes that are expicted in their forward method, we define the general case here and sub-class's specfic method in each child class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_input(self: Trainer, batch):\n",
    "    return {k: v.to(self.device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since LLMs usually pass a diciotnary to the `forward` method, we need to make sure to make class-specfic forward. For most cases, the usual `outputs = model(input)` would be sufficent, but for LLMs, we will define a child method that accepts a dicitonary not a regular tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward_pass(self: Trainer, batch):\n",
    "    X, y = batch\n",
    "    outputs = self.client.model(batch)\n",
    "    loss = self.client.criterion(outputs, y)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: Trainer, batch: dict) -> torch.Tensor:\n",
    "    try:\n",
    "        loss, logits = self._forward_pass(batch)\n",
    "        \n",
    "        if self.cfg.training_metrics:\n",
    "            y_pred = torch.nn.functional.softmax(logits, dim=1)\n",
    "            acc = self.training_metrics.compute(y_pred= y_pred, y_true= batch['labels'])['accuracy']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in loss calculation: {e}\")\n",
    "        return torch.tensor(float(0), device=self.device)\n",
    "        \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: Trainer, batch: dict) -> tuple:\n",
    "    loss, acc = self._closure(batch)\n",
    "    self.client.model.zero_grad(set_to_none=True)\n",
    "\n",
    "    if loss.item() == 0:\n",
    "        return loss\n",
    "    \n",
    "    loss.backward()\n",
    "    if self.cfg.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.client.model.parameters(), self.cfg.grad_norm_clip)\n",
    "\n",
    "    self.client.optimizer.step()\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: Trainer):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_trained = 0\n",
    "    progress_bar = tqdm(range(len(self.client.train_loader)))\n",
    "\n",
    "    self.client.model.train()\n",
    "    for i, batch in enumerate(self.client.train_loader):\n",
    "            \n",
    "        batch = self.get_inputs(batch)\n",
    "        \n",
    "        loss, acc = self._run_batch(batch)\n",
    "\n",
    "        if num_trained == 0:\n",
    "            num_trained = 1e-10\n",
    "\n",
    "        print(f'Batch loss is {loss}')\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f'client {self.client.idx} total_loss at step {i}, loss: {total_loss / num_trained if num_trained != 0 else 0.0}')\n",
    "        \n",
    "        if loss.item() != 0:\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            num_trained += len(batch['input_ids'])\n",
    "\n",
    "    return total_loss / num_trained, total_acc / num_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def test(self: Trainer) -> dict:\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    print(\"****************************************\")\n",
    "    print(f'Inside the test () function of client {self.client.idx}')\n",
    "    \n",
    "    self.client.model = self.client.model.to(self.device)\n",
    "    self.client.model.eval()\n",
    "    num_eval = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(self.client.test_loader):\n",
    "            \n",
    "            batch = self.get_inputs(batch)\n",
    "\n",
    "            if num_eval == 0:\n",
    "                num_eval = 1e-10\n",
    "\n",
    "            loss, acc = self._closure(batch)                 \n",
    "\n",
    "            print(f\"Client {self.client.idx}'s Batch loss inside eval() : {loss}\")\n",
    "\n",
    "            if (not torch.isnan(loss)) and (self.client.args.grad_clip <= 0 or loss != 0.0):\n",
    "                total_loss += loss.item()  \n",
    "                total_acc += acc\n",
    "                num_eval += len(batch['input_ids'])            \n",
    "            \n",
    "        print(f'Client {self.client.idx} Eval loss is : {total_loss / num_eval}')\n",
    "        print(\"****************************************\")\n",
    "\n",
    "    return total_loss / num_eval, total_acc / num_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `Keras` approach to return a history object, we make the same approach by invking the `get_gistory` function which averages the local metrics (usually `loss` and `accuraccy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def fit(self: Trainer, n_epochs: int) -> dict:\n",
    "    init_test_loss, init_test_acc = self.test()\n",
    "    test_loss = [init_test_loss]\n",
    "    test_acc = [init_test_acc]\n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for _ in range(n_epochs):\n",
    "        \n",
    "        self.client.model = self.client.model.to(self.device)\n",
    "\n",
    "        avg_train_loss, av_train_acc = self._run_epoch()\n",
    "        train_loss.append(avg_train_loss)\n",
    "        train_acc.append(av_train_acc)\n",
    "        \n",
    "        avg_test_loss, avg_train_acc = self.test()\n",
    "        test_loss.append(avg_test_loss)   \n",
    "        test_acc.append(avg_train_acc)     \n",
    "    \n",
    "    return {\n",
    "        'train_loss': np.mean(train_loss),\n",
    "        'train_acc': np.mean(train_acc),\n",
    "        'test_loss': np.mean(test_loss),\n",
    "        'test_acc': np.mean(test_acc)\n",
    "\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLMTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        client\n",
    "    ) -> None:        \n",
    "        super().__init__(client)\n",
    "        \n",
    "        self.client.train_iterator = iter(self.client.train_loader)\n",
    "        self.client.model.generation_config.pad_token_id = self.client.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_inputs(self: LLMTrainer, batch):\n",
    "    batch = {\n",
    "                'input_ids': batch['input_ids'].to(self.device),\n",
    "                'labels': batch['labels'].to(self.device),\n",
    "                'attention_mask': batch['attention_mask'].to(self.device) \n",
    "            }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward_pass(self: LLMTrainer, batch):\n",
    "    outputs = self.client.model(**batch)\n",
    "    loss = self.client.criterion(outputs)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-specifc text generation test\n",
    "\n",
    "The following two methods computes the text generation capabilities of an LLM. They use the metrics defined in the config file of the expirement. Usually you would use something like `Belu` or `Rouge` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another more accurate and reliable way of assessing the output of an LLM is to use a metric like the Rouge-l, which assses the qquality of the generated text not the quality of the raw model ouptus `logits`. This assessment is only done after the federated training process is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def test_generate(self: LLMTrainer) -> float:\n",
    "    print(\"****************************************\")\n",
    "    print(f'Inside the test_generate () function of client {self.client.idx}')\n",
    "\n",
    "    self.client.model = self.client.model.to(self.device)\n",
    "    self.client.model.eval()\n",
    "    \n",
    "    progress_bar_eval = tqdm(range(len(self.client.test_loader_genr)))\n",
    "    acc_total_eval = 0.0\n",
    "    total_items = len(self.client.test_loader_genr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in self.client.test_loader_genr:\n",
    "\n",
    "            batch = self.get_inputs(batch)\n",
    "\n",
    "            output_ids = self.client.model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_new_tokens=self.cfg.max_new_tokens,\n",
    "                num_beams=self.cfg.num_beams,\n",
    "            )\n",
    "\n",
    "            generated_ids = output_ids[:, len(batch['input_ids'][0]):] \n",
    "            r_score = rouge_score(generated_ids, label_ids, self.client.tokenizer)  # noqa: F405\n",
    "    \n",
    "            acc_total_eval += r_score\n",
    "\n",
    "            print(f\"Client {self.client.idx}'s Batch Rouge is : {r_score}\")\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "    print(f'Client {self.client.idx} Rouge is : {acc_total_eval / total_items}')\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    if total_items == 0:\n",
    "        total_items = 1e-10\n",
    "\n",
    "    return acc_total_eval / total_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
