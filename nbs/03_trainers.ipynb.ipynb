{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainers\n",
    "\n",
    "> Implementing the training loop for CPU/GPU training in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "from fastcore.utils import *\n",
    "from loguru import logger\n",
    "from fedai.utils import *\n",
    "from fedai.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trainer class implements the training loop with the `fit()` method. It accepts a client at initialization, which contains all necessary infromation \n",
    "to implement a training loop for a federated learning setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modularity, we implement two trainers, a single device (cpu or gpu), and a distributed trainer (multi-gpu, single node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of the Trainer\n",
    "\n",
    "The basic building blocks for the trainer are : \n",
    "- `_run_batch`: A method that accepts a batch of data as input, and returns the logits and the loss resulting from the forward pass.\n",
    "- `_run_epoch`: A method that iterate over the whole dataset, and calls the `_run_batch` function within its logic.\n",
    "- `fit`: A function that run a loop over a `n_epochs` to excute multiple epochs. It calls the `_run_epoch` method every iteration.\n",
    " \n",
    "  \n",
    "The `fit` method returns a `history` object, which is a `dict` containing the summary statistics of both the train and test datasets performance in the current federated round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Trainer:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.cfg = client.cfg\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.client.train_loader = prepare_dl(self.cfg, self.client.train_ds)  # noqa: F405\n",
    "        self.client.test_loader = prepare_dl(self.cfg, self.client.test_ds) # noqa: F405\n",
    "\n",
    "        self.training_metrics = Metrics(list(self.cfg.training_metrics))  # noqa: F405\n",
    "        self.test_metrics = Metrics(list(self.cfg.test_metrics))  # noqa: F405\n",
    "\n",
    "        self.data_key, self.label_key = 'x', 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since different model have different input shapes that are expicted in their forward method, we define the general case here and sub-class's specfic method in each child class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_batch(self: Trainer, batch):\n",
    "    return {k: v.to(self.device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since LLMs usually pass a diciotnary to the `forward` method, we need to make sure to make class-specfic forward. For most cases, the usual `outputs = model(input)` would be sufficent, but for LLMs, we will define a child method that accepts a dicitonary not a regular tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: Trainer, batch):\n",
    "    X, y = batch['x'], batch['y']\n",
    "    outputs = self.client.model(X)\n",
    "    loss = self.client.criterion(outputs, y)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _closure(self: Trainer, batch: dict) -> tuple:\n",
    "    try:\n",
    "        loss, logits = self._forward(batch)\n",
    "        probs =  torch.nn.functional.softmax(logits, dim= -1)\n",
    "        y_pred = probs.argmax(dim= -1)#.view(-1)\n",
    "        y_true = batch[self.label_key]#.view(-1)\n",
    "\n",
    "        if self.cfg.training_metrics:\n",
    "\n",
    "            if hasattr(self.client, \"tokenizer\"):\n",
    "                metrcis = self.training_metrics.compute(y_pred= y_pred, y_true= y_true, tokenizer= self.client.tokenizer)\n",
    "            else:\n",
    "                metrcis = self.training_metrics.compute(y_pred= y_pred, y_true= y_true)\n",
    "                \n",
    "        else:\n",
    "            metrcis = {k: 0 for k in self.training_metrics}\n",
    "            \n",
    "    except Exception as e:\n",
    "        # print(f\"Error in loss calculation: {e}\")\n",
    "        metrcis = {k: 0 for k in self.training_metrics}\n",
    "        return torch.tensor(float(0), device=self.device), metrcis\n",
    "        \n",
    "    return loss, metrcis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_batch(self: Trainer, batch: dict) -> tuple:\n",
    "    loss, metrics = self._closure(batch)\n",
    "    self.client.model.zero_grad(set_to_none=True)\n",
    "\n",
    "    if loss.item() == 0:\n",
    "        return loss, metrics\n",
    "    \n",
    "    loss.backward()\n",
    "    if self.cfg.grad_norm_clip:\n",
    "        torch.nn.utils.clip_grad_norm_(self.client.model.parameters(), self.cfg.grad_norm_clip)\n",
    "\n",
    "    self.client.optimizer.step()\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _run_epoch(self: Trainer):\n",
    "    total_loss = 0\n",
    "    lst_metrics= [] \n",
    "    num_trained = 0\n",
    "    progress_bar = tqdm(range(len(self.client.train_loader)))\n",
    "\n",
    "    self.client.model.train()\n",
    "    for i, batch in enumerate(self.client.train_loader):\n",
    "            \n",
    "        batch = self.get_batch(batch)\n",
    "        loss, metrics = self._run_batch(batch)\n",
    "\n",
    "        if num_trained == 0:\n",
    "            num_trained = 1e-10\n",
    "\n",
    "        # print(f'Batch loss is {loss}')\n",
    "        # progress_bar.update(1)\n",
    "        # progress_bar.set_description(f'client {self.client.id} total_loss at step {i}, loss: {total_loss / num_trained if num_trained != 0 else 0.0}')\n",
    "        \n",
    "        if loss.item() != 0:\n",
    "            total_loss += loss.item()\n",
    "            num_trained += len(batch[self.data_key])\n",
    "            lst_metrics.append(metrics)\n",
    "\n",
    "    # average the metrics of the epoch (across batches)\n",
    "    epoch_metrics = {k: sum([m[k] for m in lst_metrics]) / len(lst_metrics) for k in lst_metrics[0].keys()}\n",
    "\n",
    "    return total_loss / num_trained, epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def test(self: Trainer) -> dict:\n",
    "    total_loss = 0\n",
    "    lst_metrics = []\n",
    "    # print(\"****************************************\")\n",
    "    # print(f'Inside the test () function of client {self.client.id}')\n",
    "    \n",
    "    self.client.model = self.client.model.to(self.device)\n",
    "    self.client.model.eval()\n",
    "    num_eval = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(self.client.test_loader):\n",
    "            \n",
    "            batch = self.get_batch(batch)\n",
    "\n",
    "            if num_eval == 0:\n",
    "                num_eval = 1e-10\n",
    "\n",
    "            loss, metrics = self._closure(batch)                 \n",
    "\n",
    "            # print(f\"Client {self.client.id}'s Batch loss inside eval() : {loss}\")\n",
    "\n",
    "            if (not torch.isnan(loss)) and (self.cfg.grad_norm_clip <= 0 or loss != 0.0):\n",
    "                total_loss += loss.item()  \n",
    "                num_eval += len(batch[self.data_key])\n",
    "                lst_metrics.append(metrics)           \n",
    "            \n",
    "        # print(f'Client {self.client.id} Eval loss is : {total_loss / num_eval}')\n",
    "        # print(\"****************************************\")\n",
    "    epoch_metrics = {k: sum([m[k] for m in lst_metrics]) / len(lst_metrics) for k in lst_metrics[0].keys()}\n",
    "\n",
    "    return total_loss / num_eval, epoch_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `Keras` approach to return a history object, we make the same approach by invking the `get_gistory` function which averages the local metrics (usually `loss` and `accuraccy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def train(self: Trainer) -> dict:\n",
    "    init_test_loss, metrics_test = self.test()\n",
    "    test_loss = [init_test_loss]\n",
    "    test_metrics = [metrics_test]\n",
    "    \n",
    "    train_loss = []\n",
    "    train_metrics = []\n",
    "    for _ in range(self.cfg.local_epochs):\n",
    "        \n",
    "        self.client.model = self.client.model.to(self.device)\n",
    "\n",
    "        avg_train_loss, metrics_train = self._run_epoch()\n",
    "        train_loss.append(avg_train_loss)\n",
    "        train_metrics.append(metrics_train)\n",
    "        \n",
    "        avg_test_loss, metrics_test = self.test()\n",
    "        test_loss.append(avg_test_loss)   \n",
    "        test_metrics.append(metrics_test)\n",
    "\n",
    "    # average the metrics across all local rounds\n",
    "    train_metrics = {k: sum([m[k] for m in train_metrics]) / len(train_metrics) for k in train_metrics[0].keys()}\n",
    "    test_metrics = {k: sum([m[k] for m in test_metrics]) / len(test_metrics) for k in test_metrics[0].keys()}\n",
    "\n",
    "    # add train_ to any key in train_metrics\n",
    "    train_metrics = {f'train_{k}': v for k, v in train_metrics.items()}\n",
    "    test_metrics = {f'test_{k}': v for k, v in test_metrics.items()}\n",
    "\n",
    "    all_metrics =  {\n",
    "        'train_loss': np.mean(train_loss),\n",
    "        'test_loss': np.mean(test_loss),\n",
    "    }\n",
    "\n",
    "    # add the train and test metrics to the losses\n",
    "    all_metrics.update(train_metrics)\n",
    "    all_metrics.update(test_metrics)\n",
    "    \n",
    "    return all_metrics\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'1': 1, '2': 2}\n",
    "a.update({'3': 3})\n",
    "a.update({'4': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an LLM-specific trainer. To define the custom LLM trainer, you need to define two methods `get_batch` and `_forward`. Since LLM's specific way of getting the batches are different from the usual pytorch way, you need to define a way to extract the batches out of the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLMTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        client\n",
    "    ) -> None:        \n",
    "        super().__init__(client)\n",
    "        \n",
    "        self.client.train_iterator = iter(self.client.train_loader)\n",
    "        self.client.model.generation_config.pad_token_id = self.client.tokenizer.pad_token_id\n",
    "        self.data_key, self.label_key = 'input_ids', 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def get_batch(self: LLMTrainer, batch):  # noqa: F811\n",
    "    return {\n",
    "                'input_ids': batch['input_ids'].to(self.device),\n",
    "                'labels': batch['labels'].to(self.device),\n",
    "                'attention_mask': batch['attention_mask'].to(self.device) \n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _forward(self: LLMTrainer, batch):\n",
    "    outputs = self.client.model(**batch)\n",
    "    loss = self.client.criterion(outputs)\n",
    "    return loss, outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-specifc text generation test\n",
    "\n",
    "The following two methods computes the text generation capabilities of an LLM. They use the metrics defined in the config file of the expirement. Usually you would use something like `Belu` or `Rouge` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another more accurate and reliable way of assessing the output of an LLM is to use a metric like the Rouge-l, which assses the quality of the generated text not the quality of the raw model ouptus `logits`. This assessment is only done after the federated training process is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def test_generate(self: LLMTrainer) -> float:\n",
    "    # print(\"****************************************\")\n",
    "    # print(f'Inside the test_generate () function of client {self.client.id}')\n",
    "\n",
    "    lst_metrics = []\n",
    "    self.client.model = self.client.model.to(self.device)\n",
    "    self.client.model.eval()\n",
    "    \n",
    "    progress_bar_eval = tqdm(range(len(self.client.test_loader_genr)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in self.client.test_loader_genr:\n",
    "\n",
    "            batch = self.get_batch(batch)\n",
    "\n",
    "            output_ids = self.client.model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_new_tokens=self.cfg.max_new_tokens,\n",
    "                num_beams=self.cfg.num_beams,\n",
    "            )\n",
    "\n",
    "            generated_ids = output_ids[:, len(batch['input_ids'][0]):] \n",
    "            metrics = self.test_metrics.compute(y_pred= generated_ids,\n",
    "                                                y_true= batch['labels'],\n",
    "                                                tokenizer= self.client.tokenizer)\n",
    "    \n",
    "            lst_metrics.append(metrics)\n",
    "\n",
    "            # print(f\"Client {self.client.id}'s Batch test metrics are : {metrics}\")\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "    # print(\"****************************************\")\n",
    "    epoch_metrics = {k: sum([m[k] for m in lst_metrics]) / len(lst_metrics) for k in lst_metrics[0].keys()}\n",
    "\n",
    "    return epoch_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
